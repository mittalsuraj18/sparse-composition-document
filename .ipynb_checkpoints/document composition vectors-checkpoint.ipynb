{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:54:50.126420Z",
     "start_time": "2018-01-16T14:54:50.121312Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:54:51.562553Z",
     "start_time": "2018-01-16T14:54:50.612437Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./resume_data_full_for_suraj_baba.pickle\",\"rb\") as f:\n",
    "    RESUMES = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T12:52:11.818241Z",
     "start_time": "2018-01-16T12:52:11.291443Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T13:37:36.973187Z",
     "start_time": "2018-01-16T13:37:36.698073Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec,TfidfModel,fasttext\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy\n",
    "from scipy.spatial.distance import cosine\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class SCDV(object): \n",
    "\n",
    "    def __init__(self,n_components=100,min_count=5,epochs=5):\n",
    "        self.n_components = n_components\n",
    "        self.min_count = min_count\n",
    "        self.iter = epochs\n",
    "    \n",
    "    \n",
    "    def __get_sentences_tokens(self,documents):\n",
    "        _data = []\n",
    "        for doc in documents:\n",
    "            _doc = doc.lower()\n",
    "            _sentences = sent_tokenize(_doc)\n",
    "            for sent in _sentences:\n",
    "                _data.append(word_tokenize(sent))\n",
    "        return _data\n",
    "    \n",
    "    def fit(self,documents=None):\n",
    "        self.documents = documents\n",
    "        _resumes_words_list = self.__get_sentences_tokens(self.documents)\n",
    "        \n",
    "        \n",
    "        self.model_tfidf = TfidfVectorizer()\n",
    "        self.model_tfidf.fit(self.documents)\n",
    "        \n",
    "        self.model_word2vec = fasttext.FastText(_resumes_words_list,\n",
    "                                                negative=5,\n",
    "                                                workers=4,\n",
    "                                                iter=self.iter,\n",
    "                                                min_count=self.min_count)\n",
    "        self.word_vectors = self.model_word2vec.wv.syn0\n",
    "        \n",
    "        \n",
    "        self.model_cluster = GaussianMixture(n_components=self.n_components)\n",
    "        self.model_cluster.fit(self.word_vectors)\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_document_vector(self,document):\n",
    "        \n",
    "        if type(document) == type(\"str\"):\n",
    "            document = word_tokenize(document.lower())\n",
    "            \n",
    "        doc1 = [document]\n",
    "        \n",
    "        _doc_topic_wv = [self.model_word2vec.wv[i] for i in word_tokenize(doc1[0])]\n",
    "        _doc_topic_probs = [self.model_cluster.predict_proba(i) \n",
    "                            for i in _doc_topic_wv]\n",
    "         \n",
    "        _doc_topic_wv = numpy.array(\n",
    "            [numpy.repeat(\n",
    "                [_doc_topic_wv[i]],\n",
    "                len(_doc_topic_probs[i]),\n",
    "                axis=0\n",
    "            ) \n",
    "             for i in range(len(_doc_topic_wv))\n",
    "            ]\n",
    "        )    \n",
    "        _doc_topic_probs = numpy.array(_doc_topic_probs)\n",
    "        _doc_topic_probs = _doc_topic_probs.reshape(\n",
    "            [\n",
    "                _doc_topic_probs.shape[0],\n",
    "                _doc_topic_probs.shape[1]\n",
    "                ,1\n",
    "            ]\n",
    "        )\n",
    "        _prob_mul_matrix = numpy.multiply(_doc_topic_wv,_doc_topic_probs)\n",
    "        \n",
    "\n",
    "        _indexes_weights_tfidf = self.model_tfidf.transform(doc1)\n",
    "        _indexes_weights_tfidf = _indexes_weights_tfidf.reshape([_indexes_weights_tfidf.shape[1],1])\n",
    "        _indexes_weights_tfidf = numpy.repeat(_indexes_weights_tfidf,repeats=self.n_components,axis=1)\n",
    "        _indexes_weights_tfidf = _indexes_weights_tfidf.reshape(\n",
    "            [_indexes_weights_tfidf.shape[0],\n",
    "             _indexes_weights_tfidf.shape[1],\n",
    "             1\n",
    "            ]\n",
    "        )\n",
    "        _prob_mul_matrix = numpy.multiply(_prob_mul_matrix,_indexes_weights_tfidf)\n",
    "        _prob_mul_matrix = numpy.sum(_prob_mul_matrix,axis=0)\n",
    "        _prob_mul_matrix = _prob_mul_matrix.reshape([-1,1])\n",
    "\n",
    "        # increase the sparcity by reducing the less than 95% values to 0\n",
    "#         _prob_mul_matrix[_prob_mul_matrix < numpy.percentile(_prob_mul_matrix,5)] = 0\n",
    "        return _prob_mul_matrix\n",
    "\n",
    "\n",
    "    def get_ranking(self,documents,query):\n",
    "        docs = []\n",
    "        for i in documents:\n",
    "            docs.append(self.get_document_vector(i))\n",
    "        q = []\n",
    "        for i in query:\n",
    "            q.append(self.get_document_vector(i))\n",
    "        sims = []\n",
    "        for i in q:\n",
    "            for j in docs:\n",
    "                sims.append(cosine(j,i))\n",
    "        return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T13:51:28.352883Z",
     "start_time": "2018-01-16T13:51:28.347650Z"
    }
   },
   "outputs": [],
   "source": [
    "model_scdv = SCDV(n_components=40,min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T13:37:54.543589Z",
     "start_time": "2018-01-16T13:37:54.382678Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_data = [\" \".join(i) for i in RESUMES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:12:18.489177Z",
     "start_time": "2018-01-16T13:51:34.097794Z"
    }
   },
   "outputs": [],
   "source": [
    "model_scdv.fit(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:19:48.855737Z",
     "start_time": "2018-01-16T14:19:48.389666Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:23:47.025003Z",
     "start_time": "2018-01-16T14:23:47.021740Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:24:42.256351Z",
     "start_time": "2018-01-16T14:24:42.244190Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_data = RESUMES[:10]\n",
    "d = Dictionary(temp_data)\n",
    "temp_data = [d.doc2bow(i) for i in temp_data]\n",
    "model_tfidf = TfidfModel(temp_data,id2word=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:00:51.218058Z",
     "start_time": "2018-01-16T15:00:51.215883Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:03:06.378687Z",
     "start_time": "2018-01-16T15:03:06.374435Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_data = [\" \".join(i) for i in RESUMES[1:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:03:11.350448Z",
     "start_time": "2018-01-16T15:03:11.333777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experiencecompany name : doordarshan kendra , nagpur description : attend one week vocational training programme and acquainted with various aspects of television and communication engineering. projectproject title 1 : ask demodulator description : the project was based on developing the kit to study the amplitude modulation and demodulation. project title 2 : office automation description : the project is based on controlling electrical appliances using wireless technology. a_aparticipated in texas instruments innovation challenge , india design contest 2015. participated in technophilia system workshop held at svpcet on 2 - aug - 2014. participated in android app development workshop held at svpcet on 20 - jul - 2014. participated in national conference held at svpcet on 7 - sept - 2013. acted as mentor for technex 2014 - 15 acted as a student co coordinator for technex 2013 - 14. member of iete forum. educationcourse institution university board year of passing percentage ( % ) b.e. svpcet rtmnu 2015 71.85 h.s.s.c. kurveys new model junior college , nagpur maharashtra state board 2011 81.83 s.s.c. hindu dnyanpeeth ( convent ) , nagpur maharashtra state board 2009 75.69 computer literacy : languages : knowledge of c , c++ software known : microsoft office , pcb design , pcb wizard , diptress , matlab , multisim.',\n",
       " 'experienceworking as a rpa developer in ibm india private limited with different accounting projects form october 3rd - present. projectcompleted a project on civil registration management in the seventh semester. currently doing a project on dynamic mr : a dynamic slot utilization optimization framework for hadoop mrv1. participated in ie student chapter best project award. in - plant traning attended in - plant training on the topic of system management organized by titan precession division , hosur co - curricular activities attended a workshop on ethical hacking conducted by mit , chennai. attended a workshop on open source software conducted by adhiyamaan college of engineering , hosur. participated in paper presentation conducted by srm university , chennai. attended a workshop on selenium conducted by ace , hosur. attended a workshop on network implementation and security at adhiyamaan college of engineering organized by iit delhi and network bulls. presented a paper in 8th national conference organized by bannari amman institute of technology , coimbatore. extra - curricular activities jrc certificate holder. participated the cricket match conducted by lions club. won third place in junior science talent expo at st.joseph.matriculation.higher.secondary .school , hosur. participated in nptel online course conducted by iit kanpur. roles and responsibilities active member of iete student chapter in the department. member of quiz committee for crypto2k14. skillslanguages : c and c++ , java , html. database : sql , ms - access. database management systems operating systems cloud computing educationcourse school college board university year of passing percentage cgpa b.e - cse adhiyamaan college of engineering , hosur anna university , chennai 2015 8.09 h.s.c st. joseph matriculation higher secondary school , hosur tamilnadu state board 2011 78.17 % 10th st. joseph matriculation higher secondary school , hosur matriculation 2009 81.2 %',\n",
       " 'experiencecompany : pv web co position : junior web developer duration : may 2016 december 2016 company : pv web co project name : leave management ( for client ) environment : responsive website design ( flat style ) technology : html , css , cakephp role : analysis of the project , front end designing of the project , coding. description : this one we created for our company use , leave management encompasses the processes employees use to request time away from work and supervisors use to grant or deny leave based on organisation policies. complex and manually administered leave management programs are costly and often in errors. project an application for crayon data to conducting meetings and make transactions. the modules are user , sales and admin. user can view meeting details. sales person can create meetings.admin can modify the titles , meetings and users. we developed university application for the medical admissions. by this application students can apply for different universities. each university has to approve as per their policy. super admin of the application is able to track all records. it is complete cms application developed using cakephp. design and implement new features and continuously improving the user experience and performance. title : myths of seclusion in osns technologies : java , jsp , servlets , java script purpose : the main description of this project is to save the records photos , videos etc. in online social network. to that network we are providing a security by using privacy enhancing technologies ( pets ) . skills web technologies : html , php , java script and css frame works : cakephp programming languages : core java. basic knowledge on hadoop , mapreduce , pig , hive , hbase , sqoop. packages : microsoft office and computer fundamentals. a_a participated in the workshop in computer networking in c held by the santhiram engineering college. participated in paper presentation on alarm manager on android platform at g.pullaiha institute of engineering and technology. education completed b tech in information technology from jntu - a in 2014 completed intermediate in m.p.c from sri chaitanya college in 2010 completed s.s.c from keshava reddy public school in 2008',\n",
       " 'skills comprehensive problem tackling and solving abilities. excellent verbal and written communication skills. ability to deal with people professional. believing in team work. believing in smart work more than hard work. knowledge of c++ , c , ms excel , ms word , ms powerpoint , html and internet. a_a participated in many science talent examinations at school level. played cricket tournaments at school and college level. two day workshop national level workshop on \" recent development in electrical engineering \" two day national level techno - cultural fest. 3 days hands on workshop on plc , scada and drives. educationexam degree year name of institute university board percentage b.e [ eee ] 2012 - 2016 sri venkateshwara college of engineering. visvesvaraya technological university 68 % aggregate 12th board [ p.u.c ] 2012 seth rikhabchand parasmal sukhani pre university college karnataka state board 65.66 % 10th board sslc 2010 mother \\' s education trust high school karnataka state board 82.08 % project in engineering title : buck - boost converter fed bldc motor drive for solar pv array based water pumping. work experience at hp technical support engineer : researching , diagnosing , troubleshooting and identifying and resolving the customer \\' s issues. roles : installing and configuring computer operating systems and applications. troubleshooting system and network problems and diagnosing and solving hardware or software faults. replacing parts as required. supporting the roll - out of new applications. working continuously on a task until completion. prioritizing and managing many open cases at one time. rapidly establishing a good working relationship with customers and otherprofessionals. project title internship company hindustan aeronautical limited [ h.a.l ] platform internship in electrical maintenance department. description the internship at h.a.l was an awesome experience that i have gained on how the company works in reality apart from bookish knowledge. it has guided me to handle all the stress and responsibility of the job which i had been assigned. i have practically seen and operated all the machines which i had learned in my engineering course. the working environment and the people were so motivating in training me and making me learn the practical aspects of the work at hal. certified big data hadoop and spark developer ( certified from : simplilearn ) big data ecosystems : hands on experience with hadoop , mapreduce , hdfs , hbase , hive , sqoop , cassandra. programming languages : java , c c++ , scala , scala sql databases : nosql , oracle tools : eclipse , oracle vm , cloudera vm , hue , cloudl abs. platforms : windows( 7 , 8 , 8.1 , 10 ) bigdata hadoop and spark developer project apache spark real time project : marketing analysis : a portuguese banking institution ran a marketing campaign to convince potential customers to invest in bank term deposit. the marketing campaigns were based on phone calls. often , the same customer was contacted more than once through phone , in order to assess if they would want to subscribe to the bank term deposit or not. you have to analyze the data collected through the marketing campaign.',\n",
       " 'summary1.5years experience in r programming language. utilized r programming and shiny app for data manipulation , data visualization , predictive analytics , data analyzing. used matlab and machine learning algorithm to find learning style of a person. experience inoracle , postgresql , mysql , sql server databases. strong knowledge using thecrud , create , read , update and delete methodology. i am an oracle certified professional dba in oracle 10g , having solid practical hands - on with oracle architecture basics. i am well versed in administration , backup - recovery and performance tuning of oracle 10g in large and live production environment. creating and managing databases using sqlplus , security user management and administration including auditing , creating user accounts granting privileges. experiencerole : r developer projectdescription : there is incident management data set of size 1.25mb. using this data set predict the number of counts ticket generated on particular month and particular year. when the user inputs the year and month for which he needs the prediction , it gives the number of counts of tickets on that particular month and year. we have used r and python to reach the goal. role : r developer description : this app takes a time series data set in the form of an excel worksheet , csv file , or text file and creates a forecast. the app has three panels : the first is a plot using dygraphs , the second is information about the forecast model the third is the forecasted data. for this we have used the bench mark dataset which contain benchmark value for various months in the stock market. ui.r : nested r function that assembles an html user interface for an app. server.r : a function with instruction on how to build and rebuild r objects displayed in the ui. call shinyapp( ) function at the end of the code , which combines ui and server into a functioning app wrap with runapp( ) . role : matlab and machine learning project 3 : comparison of machine learning techniques for attention level measurement and learning styles prediction from single - channel eeg data description : first the eeg data is captured using nuero - sky mind - wave mobile via bluetooth. the eeg data is captured from 7 participants on different 8 learning style activities ( active , verbal , reflective , intuitive , sequential , global , visual , sensing ) . and the software used is matlab which is best to apply machine learning techniques. objective : 1. apply ml technique for classification of learning styles. svm ( support vector machine ) , nn ( neural network ) , nave bayes. 2. compare two to three ml techniques for arriving at the best ml technique for learning styles classification ( in terms of performance and accuracy ) . 3. use auto - regression ml for eeg data forecasting train ( like if the attention level is between 1 to 30 low , 31 to 60 moderate and 61 to 100 high ) . role : r developer description : there is vehicles data set of size 22.1mb. analysing the data and predicting the number of vehicles repaired in particular month. role : r developer description : there is solar data set of size 227mb.create training and test dataset on the solar data set. build the model on training data , make actual predicted data frames and find accuracy of predicted data verses actual data. current organization : aroha technologies , bangalore role : data analyst( predictive ) skillslanguages : r programming , matlab , sql , html. tools ide : netbeans ide , git. internet : java script , html , css. databases : oracle , mysql , sql server web application servers : apache2.0 , mongrel. operating systems : linux , unix , solaris , windows xp vista. positive attitude , determined , high energies , hard working and sincere. good decision making and analytical skills. able to handle people in a very efficient way. educationmachine learning from stanford university , grade : 95.3 % machine learning with big data from stanford university , grade : 92.1 % bachelor of engineering ( cse ) from visweswariah technical university belgaum puc from pre - university board bangalore sslc from ( secab english medium school ) sslc board bangalore training : odba( 2months ) from sql star bangalore oracle 10g certified professional ( ocp dba in oracle 10g ) from oracle corporation ( oracle university )',\n",
       " 'summary completed master of computer application ( mca ) in june 2015. completed six month internship in seo from tekblink technology worked as seo executive at tekblink technology. pursued training in big data development from inventateq. 6+ month of experience in big data implementation using apache hadoop and cloudera distribution. having sound knowledge in big data hadoop ecosystems experience in ingestion , storage , querying , processing and analysis of big data. having knowledge of ecosystem components like hadoop hdfs , hive , sqoop , hbase , zookeeper , oozie , scala and pig. rdbms knowledge includes mysql. technology hadoop distributions : apache , cdh big data technologies : apache hadoop mr , hdfs , pig , hive , sqoop , hbase , oozie , zookeeper , spark , java , mysql programming languages : unix shell scripting , python , pl sql , java script , core java mailto : pc.chauhan333@gmail.com databases environments : oracle , rdbms 9i , 10g and 11g , mysql 5.0 , 5.5 operating systems : unix and linux , windows experienceproject title : enterprise data management edm description : data from various sources like click stream , targeted ads is provided in different formats. the data is cleansed and extracted using hiveql and provided to the digital campaign team. the user data is used to send targeted ads on the promotional items of the bank. responsibilities : knowledge in defining job flows. knowledge in managing and reviewing hadoop log files load and transform large sets of structured responsible to manage data coming from different sources involved in loading data from local file system to hdfs. installed and configured hive and pig. involved in creating hive tables , involved loading data using pig and queries which will run internally in map reduce way , loading with data and writing hive queries which will run internally in map reduce way. use mr and hiveql to cleanse and query the data. stored and processes multi - terabyte data in hadoop cluster of multi structured data. writing custom packages and using apis to develop programs according to the business requirements. responsible for the dealing with the problems , bug fixing and troubleshooting. educationmca in computer science bhagwan mahavir college of management bca in computer application nv patel college of arts and science',\n",
       " 'skillsflexible and adapt quickly to new working environment. self confidence , positive thinking and dedication. competitive and perfect work manner with patiently. good experience in team working with good knowledge sharing and have leadership qualities. a_aattended for the workshop conducted on android - app - development in nit , warangal. attended for the workshop conducted by microsoft on big - data. completed a course on introduction to cryptography conducted by nptel. got 1st prize in inter standard in cricket competition. got so many prizes in games at school level. co circular and extra circular activites : worked as code - vita coordinator for bectagon 2k16 in our college. worked as student coordinator during placement drive - 2015. active participation in department magazine spark 2k16 and departments educational club elicit. active participation in social activities like medical camp on behalf of ( sac ) a social welfare organization of our college. create a youtube channel and uploading latest technical related videos https : www.youtube.com results?search_query=techtoppers educationinstitution place university board year of passing cgpa % of marks b. tech branch : it bapatla engineering college , bapatla autonomous 2017 70 intermediate branch : mpc bharathi junior college , chirala board of intermediate 2013 86 s.s.c gayatri english medium school , chirala. board of secondary education 2011 80 techinical skills : programming languages : c , core java web technologies : html database : oracle termpaper : title : detecting malicious applications in facebook using frappe. description : frappefacebooks rigorous application evaluator the first tool focused on detecting malicious apps on facebook.using this we can detect . frappe uses information available on - demand , can identify malicious apps with 99 % accuracy. role : assistant project leader',\n",
       " 'projecttitle : amusement connect ( current project ) the amusement intelligence system is a system to enable a cashless experience in an amusement location and archive appropriate information to assist with the efficiency of operating an amusement space , and enrich the experience for the user. involvements : understanding the business logic and the solution for the same. requirement gathering for the project. attending the day to day meetings and clarifying the requirements with customer. understanding the problem statement of the client. reporting about the development processs progress on a daily basis to the client. presenting the demo of the developed part of the application. personal contribution : worked in development of several modules in kiosk and redemption machines modules. web api integration of both kiosk and redemption machines. integration of the kiosk and redemption machine with devices. manual testing of both kiosk and redemption machine. title : timesheet application timesheet application is an in - house web application used to maintain the employees day to day time sheet entries. the application is developed using asp.net with c# v4.5 and mssql 2012 server for backend and database management. bootstrap v3.3.7 with css3 and html5 for front end. the application allows employees to sign up , login , and enter their time sheet entries on daily basis. admin is also allowed to view and modify specific entries in the application. the application is being deployed in local internal server and users can access the application using same network. personal contribution : contributed in the development of different modules in business layer and data access layer and some part of ui as well. title : creation of brain games the basic purpose of the project is to improve the cognitive abilities of student using brain games. involvement : defining the skill set required. defining the game play. mindset of the game. defining the problem statement and objective of the game. technology used : html5 , css and createjs( a javascript framework ) personal contribution : worked on preload , game play , tutorial , sound modules and manual testing and cross browser testing of the same. hatman ( hadoop trust manager ) augments hadoop name nodes with reputation - based trust management of their slave data nodes. worked as team member for statement module coding using core java , jdbc and java framework. involved in integration and unit testing. loading files to hdfs and writing hive queries to process required data. maintaining and administrating the 4 nodehadoopcluster. twitter data analysis using apache flume. was a member of the team of four people in a web application development called time management system using asp.net and mssql. developed a university management system as internal college project. was a member of the team of four people in an application development called mobile through pc control in vb.net and mysql skillsprogramming languages : c++ , core java , asp.net with c# databases : mssql server scripting languages : html5 , javascript , bootstrap , css3 big data analytical tools : hadoop , map reduce , hdfs , hive , pig , sqoop , flume operating systems : ms windows , linux os ( ubuntu ) general softwares : visual studio 2015 , eclipse technoical activities educationboard university percentage b.e ( computer science engineering ) manipal institute of technology 5.6 10 diploma ( computer science engineering ) dr. t.m.a. pai polytechnic manipal 61.2 % high school s.v.s katapady 60 % company name : - infiquity auto solutions pvt.ltd. duration : - 06th june 2016 to present job role : - software developer company name : - genauth integrated solution pvt. ltd. job role : - software developer intern duration : - four months ( 15th jan15 to 15th may15 ) hadoop developer from blue ocean learning bangalore. diploma in hardware and networking from manipal institute of computer education udupi. co - extra curricular activitie was a member of cricket team in school , diploma and engineering college and won inter college tournaments member of jci , katapadi , udupi district , karnataka. interpersonal skill ability to rapidly build relationship and set up trust. confident , hardworking and determined ability to cope up with different situations.',\n",
       " \"summaryoverall 1 year ( 6 month internship ) competitive experience in it industry as software test engineer working in banking and financial ( bfsi ) domain ability to work in both team environment as well as in individual currently working at fino paytech ltd. ( financial inclusion network and operation paytech ltd. ) as software test engineer. ( fino is an integrated technology platform and delivery channel , enabling sourcing and servicing of world ' s micro customers on a large scale. company provide door - step banking solution to the customer in rural area ) experiencecompany name : fino paytech ltd. ( financial inclusion network and operation ) designation : trainee software test engineer. duration : january 2016 - july 2016 and jan 2017 - till date testing type : manual testing. team size : 3 clients : icici bank and ubi responsibilities : manually perform functional testing , integration testing , regression testing , system testing , retesting testing , database testing. understand the system requirements and technical specifications. involved in requirement analysis. writing test cases as per the system requirements and technical specifications. executing test cases which is written as per the system requirements and technical specifications. handled complete defect management from defect analysis , defect reporting , tracking and closure of the defect. project1 ) loan management system ( lms ) : role : tester responsibility : to do the credit inquiry for the customer using aadhar number. to do enrollment of the customer if he is eligible for loan. to check quality check of the customers. to check the grouping and loan sanctioning of the eligible customers of a particular bank. to check loan disbursement and repayment process through lms. to check the product for which the loan will be given according to customer loan cycle. 2 ) fino unified application - client : icici with team size 2 icici bank financial inclusion business unit decided to unify the payment applications on pot devices. there will be single pot application for doing all payment activities in the field across india. the user logs on once and carrying out various transactions as needed. [ enter text in the clear boxes provided. ] testing of unified application involves - balance inquiry , cash deposit , cash withdrawal transactions via fi , aeps , rupay debit card. reversals for financial transactions. non - financial transactions like ekyc , seeding and insurance. purging data using etl packages. last transaction status ( lts ) 3 ) rupay debit card transaction app - client : icici and ubi pot application to do real time banking transactions using rupay debit card provided by bank to pmjdy customers. pradhan mantri jan - dhan yojana ( pmjdy ) is national mission for financial inclusion to ensure access to financial services , namely , banking savings and deposit accounts , credit , insurance , pension in an affordable manner. testing of rupay debit card transaction application involves - balance inquiry , cash deposit , cash withdrawal transactions , mini statement , rupay card activation , pin generation , pin change . reversals for financial transactions. 4 ) aeps fino micro atm app - client : icici , and ubi pot application to do real time banking transactions using gprs enabled pos by entering aadhaar card number. testing of aeps fino microatm involves - balance inquiry , cash deposit , cash withdrawal and fund transfer transactions. onus and offus transactions. reversals for financial transactions. certification testing with bank. 5 ) fts( fino financial transaction server ) 4.0 - 4.4 : client internal fino operations team team size 2 fts is fino transaction server. it is backend utility ( windows service ) to handle all activities ( transactions ) done from pot. testing of new version of fino transaction server with old and new version of pot application. testing of tdps involves - checking packets sent and received in log files after doing bod and eod ( settlement ) through pot machine. checking log files for different types of bod( flush , discontinued and incremental ) tracking all types of exception that can occur while doing bod eod and validating reason codes in db as well as log files. college project 1. online credit card fraud detection using hmm model description : as credit card becomes the most popular mode of payment for both online as well as regular purchase , cases of fraud associated with it are also rising. in this project , we model the sequence of operations in credit card transaction processing using a hidden markov model ( hmm ) and show how it can be used for the detection of frauds. an hmm is initially trained with the normal behavior of a cardholder. if an incoming credit card transaction is not accepted by the trained hmm with sufficiently high probability , it is considered to be fraudulent. at the same time , we try to ensure that genuine transactions are not rejected. skillsbig data ecosystem : hadoop , mapreduce , yarn , hdfs , hbase , zookeeper , hive , pig , sqoop , oozie and flume languages : java , c++ and .net databases : sql server ide : .net framework and eclipse testing skills : manual , database , web application , mobile testing and pos terminal testing operating system : ms windows , basic knowledge of linux educationhadoop certification from techdata solutions examination university board year class master of computer application( mca ) mumbai university 2016 first bachelor of science in information technology( it ) mumbai university 2013 second h.s.c ( science ) maharashtra board 2010 second s.s.c maharashtra board 2008 first\",\n",
       " 'summaryextensive skills in java , jsp servlets , jdbc and oracle , html. self - confidence , total commitment , patience and positive attitude towards situations in life. ability to set goals and achieve them on a regular basis. has strong ability to learn new technologies in short span and implement independently. experienceworking as a implentation engineer in steelwedge technologies pvt ltd. from past 10 months. skillsoperating systems windowsxp , windows7 programming languages java , jsp servlets , jdbc. databases oracle 11g scripting languages html , css , javascript packages msoffice , d.t.p hadoop : knowledge on rackawarness. knowledge on hive. educationduration institution university percentage division b.tech( it ) sep 2008 to may 2012 avanthi institute of engineering and technology , hyderabad. 75 % first intermediate june 2006 to march 2008 sri aurobindo jr. college , nalgonda. 90.4 % first ssc june 2005 to march 2006 siddartha high school , kondamallepally. 85.16 % first',\n",
       " 'experiencei had worked as intern for muvr technology from 15th november 2015 on application development. projectbig data development : organization : acropolis institute of technology and research , indore description : training on big data hadoop , mapreduce python development : organization : mukul world institute , patna description : i had selected among top 5 candidate for python training. it was course created by mr. purnendu prabhat( patna ) link : http : mukulworld.in old_courses pythons1selected.html android development : organization : acropolis institute of technology and research , indore description : android development ruby development : organization : systematix technocrates pvt. ltd. description : ruby development android projects : description : surasaya( android ) application for teaching about physical objects and imotion to kids . role : frontend( android ) developer , mobile database developer description : created scientific calculator( android ) in 2015 , have 1000+ installs . role : frontend( android ) developer play store link : https : play.google.com store apps details?id=com.scical.calculatorfinal.scical description : created newspaperwala application( popular in maharastra ) role : frontend( android ) developer , mobile database developer on sugar framework play store link : https : play.google.com store apps details?id=com.technotwit.newspaperwala description : i am updating acropolis cdc application( created in 2014 ) for students of acropolis group .it has 50+ installs role : frontend( android ) and backend( parse.com ) developer , mobile database developer using parse.com play store link : https : play.google.com store apps details?id=com.aitr.hello description : i had worked as lead mobile app developer in meraindore app it is currently is development. role : team leader , frontend( android ) and backend( firebase.com ) developer description : i had created app for samaac company of jabalpur . it has 100+ installs role : frontend( android ) play store link : https : play.google.com store apps details?id=com.wsamaac java projects : description : it is car pooling system project role : developer description : it is chatting project in socket programming project role : developer project link : https : groups.google.com forum # ! topic aitr_cs_2011 lohdxcileve description : it is mall system management project role : developer running java projects : organization : acropolis institute of technology and research , indore description : it is facebook clone in jsp , servlet and mongodb role : map reduce jobs developer description : creating book recommendation engine , till now we had calculated average rating by a user for book organization : acropolis institute of technology and research , indore description : it is banking software , i had created . available here project link : https : groups.google.com forum # ! topic aitr_cs_2011 lohdxcileve organization : acropolis institute of technology and research , indore description : it is chess game , i had created . available here project link : https : groups.google.com forum # ! topic aitr_cs_2011 lohdxcileve organization : systematix technocrates pvt. ltd. , indore description : it is meetup application for google developer group project link : http : bit.ly 2ky726s organization : acropolis institute of technology and research , indore description : it is a website project for authors i had created its front - end by html , css , bootstrap , jquery. . it is still running now i had submitted its off line version. skillsc , c++ , java core , android( backend and frontend ) , ruby , python , pygame , mysql , sqlite , big data , parse.com , django , firebase a_a1.awarded with title of vidhya bhusan by amul company in year 2012 - 13. 2.selected as microsoft student partner in the session 2015 - 16. 3. secured 1st prize in inter college chess competition in year 2014. 4. selected as campus ambassador for event voodoo or die contest held by voodoo incorporation in 2015. 1.i had come under top 2 android developer of whole world in duration of 10 july 2016 to 20 july 2016 at stackoverflow.com.i had achieved total of 34 different badges in which 1 golden badge , 6 silver badges and 27 are bronze badge which are very rare for anyone to achieve till 22 02 2017 . 2. secured 1st rank in innovative idea competition in 2015 at tech fest at college level. 3. selected in national talent search examination among top - 100 contestants by nict. 4. secured 1st rank in science exhibition competition in 2013. 5. presented research paper on search engine optimization with page rank algorithm \" at \" jnu delhi \" with reference number esm286 and it is available here : - shuseo 6. presented research paper on introduction to map reduce in ijcam conference at , chameli devi group of institute indore in 2016 which is available here : - shumap 7.i had created 25+ repository on github and my profile is available here : - shubhgitprofile 8. presented research paper on big data analytics using apache spark on iot in national conference on contemporary computing at , chameli devi group of institute indore in 2017 strengths : consistency , dedication . area of improvement : focused on one thing at a time. educationpursuing bachelor of engineering from acropolis institute of technology and research , indore affiliated to rgtu with specialization in computer science and engineering ( 2013 - 2017 ) ( current average - 79.60 % ) senior secondary school certificate ( 10+2 ) from board of secondary education , bhopal m.p. with 83 % in the year 2013 high school certificate ( 10th ) from board of secondary education , bhopal m.p. with 91.3 % in the year 2010 - 11',\n",
       " 'experienceorganization wipro limited , bangalore projectoperational analytics in big data as - a - service description the project has implemented in three parts. first , monitoring of logs from all nodes of hadoop cluster and sending email alerts for particular loglevel. second , monitoring of infrastructure level metrics and hadoop services metrics. third part of project involves anomaly detection on infrastructure and service metrics data using multivariate gaussian distribution algorithm. technology used logstash , elasticsearch , redis , kibana , apache spark , python [ numpy , pandas , scipy and matplotlib ] , rest api. real time data analysis on health related data gathered from wireless sensor network online job portal ( final year project b.e. ) mobile shop management system skillsprogramming language c , c++ , c#.net , java , python , r web development html , css database ms sqlserver , ms access 2007 , oracle 9i and above data analysis tools r - studio , rapidminer , weka big data technologies hadoop ecosystem , spark , elastic search , logstash , kibana , hortonworks data platform [ hdp stack 2.2 and hdp stack 2.3 ] educationajit pandharinath sonawane bangalore , india ajitpsonawane@gmail.com contact : +919962416560 degree institute school board year results m.tech cse with specialization in big data analytics vit university , chennai vit university , india 2016 8.81 b.e. information technology p.e.s college of engineering , maharashtra dr. bamu university , aurangabad 2013 70.00 % hsc sbes college of science , aurangabad maharashtra state board 2007 62.17 % ssc baliram patil high school , aurangabad maharashtra state board 2005 76.13 % attended wor3kshop on big data analytics conducted by 3x3 connect in association with microsoft held in vit university chennai campus in 2014. participated in hackathon on big data analytics conducted by 3x3 connect in association with microsoft held in vit university chennai campus in 2014. training in microsoft azure cloud and participated in mobile app development. attended networking and security in wireless connections workshops conducted by drdo at vit university.',\n",
       " 'experienceetl developer , 09 2015 to present. at procit india , nashik ( subsidiary of procit bv , netherlands ) projectproject title : kws team size : 3 responsibilities : work with project managers , project leads and web development team to achieve business and functional requirements. communicate with mobile webservice to upload data comes in xml file to clients portal. communicate with mobile webservice to upload image encoded in base64 to clients portal. language used : java tool used : talend open studio for esb ( di + esb ) other : ms sql server 2005 , xslt. project title : autodoping project sub - parts : doping2smartflow ( import ) , smartflow2doping ( export ) team size : 3 responsibilities : work with project managers , project leads and web development team to achieve business and functional requirements. import client xml , mapped it in desired format and stores into database using talend open studio. export clients data in desired format using talend open studio. write java code , stored procedures , triggers , sanity scripts to achieve functional requirement. deploy jobs on servers as a service or as a scheduled task. language used : java tool used : talend open studio for esb ( di + esb ) other : ms sql server 2012 , xslt. project title : moba project sub - parts : moba2smartflow ( import ) , smartflow2moba ( export ) team size : 3 responsibilities : work with project managers , project leads and web development team to achieve business and functional requirements. perform bulk insert on client data and stored into database temporarily. fetch data from database and mapped data in desired format and stores into database using talend open studio and generic webservice. write virtual dataset queries to export clients data in desired format using talend open studio and generic webservice. write java code , stored procedures , triggers , sanity scripts to achieve functional requirement. deploy jobs on servers as a service or as a scheduled task. language used : java tool used : talend open studio for esb ( di + esb ) other : ms sql server 2012 , xslt. project title : aboma project sub - parts : aboma2smartflow ( import ) , smartflow2aboma ( export ) team size : 4 responsibilities : work with project managers , project leads and web development team to achieve business and functional requirements. develop a generic import and export job that can work with clients multiple systems with any major changes. communicate with clients web service to import data and then convert it into desired format. upload clients data ( xmls and pdfs ) through clients web service using talend open studio. write java code , stored procedures , triggers , sanity scripts to achieve functional requirement. deploy jobs on servers as a service or as a scheduled task. language used : java tool used : talend open studio for esb ( di + esb ) other : ms sql server 2012 , xslt. project title : fxvrmx project sub - parts : maximo2flex( import ) , flex2maximo( export ) team size : 4 responsibilities : work with project managers , project leads and web development team to achieve business and functional requirements. develop a generic import and export job that can dynamically select database at runtime to import and export data based on clients input data. data sent to server using rest service created in talend open studio. write java code , stored procedures , triggers , sanity scripts to achieve functional requirement. deploy jobs on servers as a service or as a scheduled task. language used : java tool used : talend open studio for esb ( di + esb ) other : ms sql server 2012 , xslt. project title : centric2sharepoint team size : 3 responsibilities : work with project managers , project leads and web development team to achieve business and functional requirements. develop a generic job that can fetch data and sends it to two sharepoint sites. perform transformation on data using xslt to achieve functional requirement. deploy jobs on servers as a service or as a scheduled task. language used : java tool used : talend open studio for esb ( di + esb ) other : oracle 10g , xslt project title : tunnel interface team size : 3 responsibilities : work with project managers , project leads and web development team to achieve business and functional requirements. develop a generic import and export job that can dynamically select database at runtime to import and export data based on clients input data. data sent to server using rest service created in talend open studio. write java code , stored procedures , triggers , sanity scripts to achieve functional requirement. deploy jobs on servers as a service or as a scheduled task. language used : java tool used : talend open studio for esb ( di + esb ) other : ms sql server 2012 , xslt. skillsetl tool : talend open studio for data integration ( v5.6.2 , v5.3.2 ) databases : sql server 2012 , mysql others : hadoop , hive , java , xml , xslt , xpath , tortoisesvn environment : linux , windows ( 7 8 10 ) ability to work in team. ability to communicate effectively. confident and determined. ability to cope up with different situations and tackle them. educationmca ( under engineering ) from gokhale education societys r. h. sapat college of engineering , under savitribai phule pune university and secured 70 % aggregate ( 2015 ) . b.sc. ( information technology ) from n. k. college , malad ( west ) , under mumbai university ( 2011 ) . successfully completed the course , big data and hadoop with grade a+ from mindscripts , karve road , pune ( 2015 ) .',\n",
       " 'experiencecompleted 1 year practical training from spectralogics indore ( m.p ) working in hadoop and bigdata technologies. projecttitle - web visitor log analytics using big data ecosystem. project description : - web visitor log analytics project is initiated to demonstrate your capability on big data processing and analytics.this project involves analyzing log data of the web visitors coming from web server.huge log data is first downloaded and ingested into hdfs - hadoop distributed filesystem using apache flume utility. after ingesting , data will be cleaned and transformed using apache pig hive utilities.cleansed structured data is then transferred to a relational database storage system.where it can be further used for reporting. skillshadoop ecosystem : hdfs , hive , pig , hbase , flume , sqoop , zookeeper. databases : mysql , oracle. operating system : windows xp 7 8 , linux ( ubuntu 14.04 lts , mint ) . productivity applications : ms word , excel , powerpoint. a_aactively participated in various activities for inter school and college. ncc a certificate during 9th to 11th class. mr. model in my collage. educationmoved all crawl data flat files generated from various sources to hdfs for further processing. written apache pig scripts to process the hdfs data. created hive tables to store the processed results in a tabular format. developed sqoop scripts to import export data between hdfs and mysql database. written hive queries to analyze data store in hadoop distributed file system. fetched and store log file data using apache flume in hadoop distributed file system. operation system : ubuntu 14.04 server data ingestion and transfer : apache sqoop and apache flume. data storage : hdfs ( hadoop distributed file system ) , mysql data analysis and transformation : apache pig and apache hive. big data and hadoop development castek academy ( indore ) . bachelors in electronic communication engineering icfai university , dehradun ( u.k ) , 2015 hsc from gov. excellence school , narsinghpur ( m.p ) with 70 % , 2010 ssc from gov. excellence school , narsinghpur ( m.p ) with 80 % , 2008',\n",
       " 'experience attended workshop on information security and cyber forensics conducted by cyber cure solutions. attended workshop on big data analytics and tools conducted by grg school of applied technology. project online banking system java online voting system visual basic tourism website html and java script title : mining challenges with big data. implementation : hadoop framework , map reduce programming. description : to extract the useful information from the large amount of data with in a tolerable elapsed time. to reduce the utilization of memory and produce the accurate statistical result. skillsoperating system : windows , linux. internet tools : html , java script. database management : ms - sql server 2008 2008 r2. languages : c , c++ , java reference : mrs. m. banumathi , m.e , assoc professor cse , m.p.nachimuthu m.jaganathan engineering college , chennimalai , erode dt. phone no : 9942972565 a_apaper presentation presented a paper on bigdata mining in inter department paper presentation. got first prize in the event multimedia and poster design in inter college competition. secured second rank in department for academic year 2010 - 11 and 2011 - 12. educationcompleted courses under : website designing and 3d animation( maya , dreamweaver , flash ) cisco certified network administrator - ccna .net course name of the institution board university year of completion marks % b.e.( cse ) m.p.nachimuthu m.jaganathan engineering college , chennimalai , erode. anna university , chennai 2014 85.7 ( first class with distinction ) hsc kamaraj matriculation higher secondary school , kovilpatti. state board ( english ) 2010 83 sslc everest mariappa nadar higher secondary school , kovilpatti. state board ( english ) 2008 85',\n",
       " 'summary currently working as software engineer and data scientist in yottaasys. designed analytics on ibm bigdata analytics , kibana4 and tableau. coded a mapreduce program to fetch the count of solr log files from apache solr server and coded prediction algorithms using python ( numpy and pandas ) . experienceorganization name : yottaasys consulting pvt ltd , bangalore. profile description : software engineer ( 03oct2014 present ) completed big data university course using text analytics essentials on ibm big infosphere. completed big data university course using bigsheets for analytics on ibm big infosphere. completed data camp course on r. completed java course in aradhya brilliance centre. projectmailto : suresh.kumar@yottaasys.com duration ( 16 02 2015 ) ( 23 04 2015 ) team size : 2 project : internet search application. environment ( with skill versions ) tool : apache solr 4.0 server : apache solr project description the project uses apache solr for its search needs. the entire patent files of the oil and gas category needs to be indexed into apache solr server. desktop search should be intergraded with the solr search. fuzzy sounds like search should be a part of the search interface. autosuggestion from the server side needs to be provided. linguistic search need to be developed. filtering based on product should be made available. contribution as a developer , is responsible for development of the following pieces 1. developing the index profile for indexing the patent files. 2. using the solr search api to retrieve search results. 3. building the jquery code for displaying the server side auto suggestion. 4. using the solr search api for dill down search unsupervised clusters and navigators 5. using the solr search api for similarity search and linguistic search 6. administrate the solr search index server to index required files. 7. creating various collection and search profiles for advance search purpose. designing the search result page according to users preference. client - duration ( 06 05 2015 ) ( 21 05 2015 ) team size : 2 project : analytics visualization environment ( with skill versions ) tool : tableau desktop 8.0 server : tableau server project description this project was to create analytics with the supply - chain data. the customer will be able to analyse the demand and supply for various energy sources in each region in various types of visualization and easily identify as in which part of the city sub - region state has high sales and where is more demand for a particular product etc. as a developer , is responsible for gathering the required data in raw json format. importing it into the tableau desktop and creating visualization according to the needs. hosted the same on the tableau server for easy accessing. 4. machine learning as a data scientist , following are the tasks done : - scrape data from the websites or take up the kaggle competitions. do feature engineering on the available data using various python packages like numpy , pandas. train the model using machine learning algorithms ( gradient boost , randomforest ) predict the target output as per requirements ( regressor classifier ) skillsoperating systems windows , linux languages java , r , python( scikit , pandas , numpy ) databases sql web related html , jsp , javascript , spring , jquery enterprise search tools apache solr , elastic search visualization tools tableau and kibana4 big data ecosystems hadoop , mapreduce , hdfs , hive , sqoop a_a actively participating in kaggle competitions ( id : - https : www.kaggle.com yottoboss ) presented my project in the national level paper presentation held at epcet. participated and won prizes in various theater events such as mock - rock , mad - ads in various inter - college cultural fests. participated and won in various district and state level yoga competitions. educationdegree university college year of passing b.e ( cse ) vtu mvj college of engineering , . bangalore. 07 2014 akash nagar , bangalore - 560016',\n",
       " 'summaryb.tech ( information technology ) from uptu ( formerly mtu ) with 74.2 % . intermediate passed in year 2011 from d.p.s. ghaziabad with 81.8 % . high school passed in year 2009 from d.p.s. ghaziabad with 90 % . experienceproactive and dynamic professional , offering 18+ months of accomplished experience with leading organization in it company , seeking career advancements in a competitive organization fostering an atmosphere that facilitates professionals to balance high - level skills with maximum productivity expertise in multitasking of process and activities with key focus on optimal utilization of resources with timelines and accuracy. working as software engineer - sopra india pvt. ltd , noida july , 2015 till date about company : sopra group is an independent group created in 1968. its a leader in the european consulting , it services and software solutions markets. the company had a turnover of about 3.1 bn with a workforce of over 35000 in 2013. its main sites are at france , benelux , italy , spain , switzerland , united kingdom and india. sopra is a cmmi level 3 company. internet ticketing center , irctc duration : 6 weeks department : internet ticketing center during the internship i worked on 1 project and successfully delivered them in time. meeting management system : this web application is used to schedule meetings for various internal departments of itc , irctc and notify all the invited members about various details of the meetings via e - mail facility. an extra functionality added to the web app is sql injection. technology used : html5 , css , javascript , jquery , php projectthe commercial installed base contains the detailed list of offers ordered owned by the customer. it represents instances of offers subscribed by customers , the place where the offer is in use , as well as the commercial configuration characteristics. it also contains charges ( excluding usages prices ) of an installed offer or a commercial operation applied to the customer ( according to its contracts ) . ordering tool calls cib , to create , update , and remove installed offer at different steps of the order lifecycle. as the order process is specific to each order type , the interactions between ordering tool and cib , depends on the type of order. orange is one of the major clients of sopra. role : as a development engineer , i am involved in : design the requirement and build the technical specification document. do the coding and unit testing including the junits integration testing of the code as well. optimization of the process according to the industry standards. resolve the qa and uat anomalies ( if the anomalies arrive ) . 2. project : customer information systems ( cis ) cis is a database application of orange business services ( france telecom ) . cis gets data from various applications and networks. the data is then consolidated and check rules are applied ; inconsistent data is rejected and validated data is propagated to downstream applications systems , typically reporting , billing and end users. cis programs are mainly batch processes called gateways. the role of gateways is to import , check , validate and export data. orange is one of the major clients of sopra. role : as a development engineer , i am involved in : gathering the requirements and change requests from the clients. do the requirement gathering and provide the quotation to them. design the requirement and build the technical specification document. do the coding and unit testing integration testing of the code as well. resolve the qa and uat anomalies ( if the anomalies arrive ) . the qa and uat are done after every ( quarterly ) delivery at the customers end. resolve the production anomalies as well. provide the support for the production roll out as well. enterprise impact : demonstrated good learning ability on the project and has done the first enhancements on cis project with good quality. role : as a hadoop and java development engineer , i am involved in : developing map - reduce programs in java and python to conduct a poc for yelp. setup 4 node ambari cluster on centos 7 from scratch. enterprise impact : demonstrated good learning ability on the poc and has done the first enhancements on map - reduce programs in yelp poc with good quality. title : emergency response system technologies : html , php , mysql , java , ajax description : online police station will be an android and web app which will avail the public with an emergency response system. it will provide people with an online portal where they can register an fir and there at that moment they will be allotted a police officer who will follow up on their report. they will be able to log on and check the status of their report online which will be submitted by the allotted police officer periodically skillsbig data : hadoop course from ducat noida rdbms : oracle( sql , pl sql ) , mysql operating systems : windows 7 , linux , sun solaris programming languages : java , php , pl sql , spring and hibernate ( learning ) concepts : oops concepts , databases concepts , programming , html a_aapplause certificate to develop an automation tool to automate the patch creation process. two bon travail batch and certificate for quick learning and good quality development. fee waiver scholarship from uptu university. logistics head of the annual techno - cultural fest of the college , genero14. strengths : leadership qualities positive attitude and optimistic ability to take pressures with confidence organizing capabilities',\n",
       " 'projectmini project : title : laser communication system platform : embedded systems. team size : 3 project duration : 45 days major project : title : touch panel based robot control with wireless communication platform : embedded system software used keli. team size : 3 project duration : 45 days. skillslanguages : python , java , scala big data : hadoop , pig , hive , spark linux , ccna , mcse. pc hardware and networking wed designing and developing personal trades : well disciplined. willingness to learn and ability to work effectively and efficiently. quick learner and good team player. good written and oral communication. educationdegree university board college year percentage b.tech ( e.c.e ) j.n.t.university , hyd vaagdevi college of engineering , warangal. 2012 - 15 70 % diploma state board of technical education and training govt polytechnic college( boys ) , warangal 2008 - 11 71 % secondary school certificate board of secondary education little flower high school warangal. 2007 - 08 87 % presentations and participations : presented a paper on the topic voice operated intelligent fire extinguisher vehicle a national level competition in vaagdevi college of engineering , warangal.',\n",
       " \"experiencevaluepoint knowledgeworks. 20 02 2017 - present i am working as a software trainee in valuepoint knowledgeworks pvt ltd. 2 role : - full stack developer. broadband and ip networking in bsnl , govt of india. projectmachine translation of text in english language to hindi language. aim of the project is to decode the meaning of asource text which is in english , by obtaining its part of speech , and analyzing all the features of the text. a data set that requires in - depth knowledge of thegrammar , semantics , syntax , idioms etc is designed , using the data set the source text is re - encoded in the target language. tools : programming language python - 2.7 , python - 3.6 modules sckit - learn , nltk , unicode , pymysql , tkinter databases mysql , sqlite3. northern soft. 01 10 2016 31 12 2016 role : - python developer intern. social media monitoring tool using python and nltk. building a social media monitoring tool that evaluates how many people are influenced by the campaign and one that finds out what people think about the brand. skillsprogramming languages : c , java , python. web development : html5 , css , javascript , bootstrap. tools used : django , cisco packet tracer , ms visual studio , android studio. database : mysql server. operating system : windows , linux and android. a_adesigned and built an autonomous ground vehicle , the project bagged first place in engineering project competition category in srishti 2012 a state level project competition. designing an android app under step ( a college funded platform , to support best projects ) for local farmers , to help them sell products directly to the local retailers. strengths analytical thinking , planning. problem analysis , use of judgment and ability to solve problems efficiently. cooperative , competent and hard working. very flexible and can work hard to complete the assigned work. personel profile name : anand s patil. father ' s name : siddanagouda m patil. sex : male. date of birth : 22 - 04 - 1991. nationality : indian. languages known : kannada , english , hindi. hobbies : doing electronic projects , reading novels , sports etc. educationcollege institutes board university year of passing aggregate b.e in information science basaveshwar engineering college , bagalkot autonomous ( affiliated to vtu , belgaum ) 2016 6.53 cgpa puc basaveshwar independent puc college , bagalkot. karnataka pre - university education board 2009 46.33 % s.s.l.c sri aurbindo vidyamandir , bangalore. karnataka secondary education examination board 2006 64.36 %\",\n",
       " \"summaryhaving 9 months of experience as hadoop associate. experience in technologies like hadoop , hive , sqoop , map - reduce , hbase , pig , core java , jsp , servlet. having basic knowledge on zookeeper , flume , spark , paython , php and hibernet. having good knowledge in run analysis on structured and unstructured data base. experience in importing and exporting data using sqoop from hdfs to relational database system and vice - versa. experience in web development languages like html and javascript. experience in web servers like tomcat7.0 , thrift server and hive server2.0 . experience in ides like eclipse. having good knowledge on databases mysql. proficient in software development life cycle ( sdlc ) , analysis , design , code development. excellent interpersonal and communication skills , creative , research - minded , technically competent and result - oriented with problem solving and leadership skills. ability to function at a high level in a wide variety of settings. good communication skills , self - motivated , quick learner and motivation for the team. experienceemployer starsun technology pvt. ltd , pune designation software engineer 4th july 2016 to present technologies java , jdbc , javaee , html , css , hadoop , map - reduce , hive , sqoop , hbase , pig , yarn tools and software windows7 , jdk - 1.7 , eclipse , tomcat - 7 , mysql , mysql server , ubuntu project worked movers and packers projectproject title movers and packers team size 3 role developer duration from : july 2016 to : till date description this project aims to move all log data and structured data from individual servers to hdfs as the main log storage and management system and then perform analysis on these hdfs data - sets. the data will be processed using hive , pig extract meaningful information out of it and give to the reporting bi tools for showing results. sentimental analysis of the product by taking reviews of product from website , twitter , facebook. traffic and website hit analysis of the website by analysis log data. by sqoop import structured data from rdbms ( mysql ) database to the hive table and analysis these data by hive and giving to reporting tool , statistics tool for showing graph. business benefits aimed were to this analysis is useful take new business decision. minimize hardware cost by using more than one cluster for analyze large datasheets. report generation from unstructured datasheets. extract meaningful information from logs. responsibilities involved in importing whole database , and incremental data from mysql to hdfs and hive using sqoop. analyze data using writing queries with hiveql and pig. sentiment analysis on reviews of the products on the client ' s website , facebook , twitter. exported the resulted sentiment analysis data to bi tool for creating dashboards. involved in working for data retrieval process. implemented flume to import log files to hdfs for analysis. implemented batch jobs for scheduling of hadoop programs. strengths : eager to learn new concepts and technologies. hard working and determined. work efficiently within a team. good and polite communication skills. skillshadoop big data hadoop v2.x , hdfs , yarn , mapreduce , hbase , hive , sqoop , pig. java technology core java , jsp , servlet , jdbc. ide ' s eclipse programming language php , c , c++ , python web technologies html , css educationinstitute name hewlett packard enterprise ( hpe ) certification name bigdata and hadoop 2.x date of issue 27th june 2016\",\n",
       " 'projectm.tech project project title : bidirectional double - boost dc - dc converter for renewable energy applications description : an individual project that explains the working of dc - dc boost converter which has renewable energy applications. duration : 6 months team size : individual semester : 4th semester project title : allophone based speech synthesis description : a team project that provides speech synthesis using a microprocessor and an ic for text to speech conversion duration : 6 months team size : 4 semester : 4th semester project title : aircel technologies : hadoop , mapreduce description : this project is used to determine which tower is being used by the customer. objective of this project is to calculate total data downloaded by each customer using mapreduce program and sort the results according to data downloaded. there is a log file which has subscriber id , tower id and data downloaded. calculate total data downloaded for each customer id using mapreduce program in hadoop and to sort the result from mapreduce program according to data downloaded. as the customer moves to a new location , the subscriber id and tower id changes. the towers are located in different locations. now customer can be tracked using a new tower id and new subscriber id. responsibilities : calculating data downloaded using java program developed in eclipse adding total bytes of data downloaded for each customer using mapreduce running the jar program in vmware project title : kohls click stream analysis technologies : hadoop , pig , hive , hdfs description : the business objective of this initiative is to reach out to customers by email who have abandoned carts ( added items to cart and did not make purchase ) abandoned browse ( browsed items and did not make a purchase ) as per the industry guidance , an effective cart abandon process can recover 18 % of abandon carts and send approximately 17500 emails on an average. responsibilities : loading datasets to hdfs and writing pig scripts. understanding of data node , name node , job tracker , secondary name node , task tracker. loading data to hive tables and writing queries to process resolve issues and risks and fixing the bugs. skillsprogramming language : core java , sql. frame works : hadoop , hive , pig , hbase and sqoop rdbms : my - sql , oracle. web - server : apache - tomcat. tools ide : eclipse. platforms : windows 7 , centos strong knowledge in hadoop and database management systems. strong knowledge of hdfs , mapreduce and hadoop ecosystem components like hive , pig and sqoop strong experience on hadoop distributions like cloudera. having good knowledge in end - to - end in big data. big data ecosystems : hdfs , mapreduce , hbase , hive , pig , sqoop. databases : mysql. tools : eclipse. educationindustrial training on hadoop. niit certified in sql participated in national workshop on scope and challenges in sustainable technologies - a present day scenario. conference paper successfully submitted and accepted in international conference on current trends in engineering and management 2014 presented ieee paper in international conference on advances in electronics , computers and communications. m.tech from dayananda sagar college of engineering , bangalore with aggregate of 76 % - year of passing 2014. b.e from yellamma dasappa institute of technology , bangalore with aggregate of 65.8 % - year of passing 2012. puc from christ college , bangalore with aggregate of 77.5 % - year of passing 2008. s.s.l.c from carmel convent high school , bangalore with aggregate of 90.7 % - year of passing 2006.',\n",
       " 'projectproject : android lost and found applicatoin project duration : 6 months project description : an android app that provide security to the cell phone from theft and lost. it has 6 different features. it works on the sms code that is sent by other phone number which is register on that app. it can switch cell mode from silent to normal , format the cell data , lock the cell phone etc. technologies : java database : sql lite skillslanguages : java , c++ , c# web technologies : asp.net , html rdbms tools : mysql , oracle operating system : win xp win 7 win 8.1 win 10 , linux tools : ms office , ms visual studio , oracle warehouse builder 11g big data ecosystems : hadoop , ( expertise level - mapreduce , hdfs , hive , pig , sqoop ) , ( intermediate level spark , python , hbase , oozie , flume ) a_afirst prize at inter colleges competition in code debugging during 2014 - 2015. educationb.sc it 2016 university of mumbai percentage : 77 class : first hsc( science ) 2013 cbse board percentage : 77.9 class : first ssc 2011 cbse board percentage : 79.8 class : first',\n",
       " 'summary1. 3+ years experience in software industry. 2. more than 9 months of experience in hadoop development. 3. good knowledge of hadoop ecosystem , hdfs ( hadoop file system ) , big data , rdbms. 4. hands on experience in working with hive , pig , sqoop , map reduce and flume. 5. knowledge in using linux commands 6. capturing data from existing databases that provide sql interfaces using sqoop and experience in mysql. 7. experience in database design using stored procedure , functions and strong experience in writing complex queries for sql server. 8. good knowledge on hadoop cluster architecture and monitoring the cluster. 9. excellent problem solving skills , high analytical skills , good communication and interpersonal skills. 10. worked as process coordinator and seo executive for 2.5 years. experienceorganization designation duration cz infotech , bangalore. hadoop trainee from mar 2016 breezegosolutions , technopark , tvm seo executive sep 2013 - feb 2016 business jan 2009 - jun 2013 project1. project title : hatman ( hadoop trust manager ) organization : cz infotech , bangalore. duration : mar 2016 till date skill set : hdfs , mapreduce , hive and eclipse. description : hatman ( hadoop trust manager ) augments hadoop name nodes with reputation - based trust management of their slave data nodes. the trust management system is centralized in the sense that name nodes maintain a small , trusted store of trust and reputation information ; however , all computation is decentralized in that trust matrix computations and user - submitted job code is all dispatched to data nodes. name node computations therefore remain restricted to simple bookkeeping operations related to job dispatch. this keeps the system scalable and maintains high trustworthiness of name nodes by minimizing their attack surfaces. roles and responsibilities : worked as team member for statement module. coding using core java , jdbc and java framework. involved in integration and unit testing. client interaction( interaction with clients for their requirements ) unit testing of components , maintenance of code and components loading files to hdfs and writing hive queries to process required data. maintaining and administrating the 4 nodehadoopcluster. worked on setting uphadoopover multiple nodes , designed and developed java mapreduce jobs. 2. project title : twitter data analysis. organization : cz infotech , banglore. duration : june 2016 till date skill set : hdfs , mapreduce , hive and flume. description : the main aim of this project is to understand the mapreduce execution framework in detail on larger data sets and to tune the mapreduce jobs running on cluster using combiner , configuration tuning parameters like block size , sort factor etc. roles and responsibilities : worked as team member for statement module. involved in integration and unit testing. designed and developed java mapreduce jobs. unit testing of components , maintenance of code and components loading files to hdfs and writing hive queries to process required data. queries using hive and developed map - reduce jobs to analyze data. developed pig latin scripts to extract the data from the web server output files to load into hdfs. maintaining and administrating the 4 nodehadoopcluster. seo experience handled around 30 websites and below list are the top satisfied customers , 1. answerphoneusa.com 2. certifiedsecuritysystems.com 3. certifiedcommercialsecurity.com 4. drjoeldavid.com 5. palmettosecurity.org seo tools used : google analytics , google webmaster tool , google adword , google trends etc skillsbigdata analytical tool hadoop , hdfs , hive , pig , sqoop , flume html , eclipse , maven nosql databases mongodb operating systems windows xp 7 8 10 , linux database mysql others ms office educationexamination duration institution name university b.tech 2004 - 08 university college of [ kerala university ] engineering , tvm plus two 2000 - 02 govt. model h.s.s , tvm [ kerala board ] 10th 2000 sarvodaya vidyalaya [ kerala board ] strength 1. excellent communication and inter - personal skills ( written , oral , presentation ) . 2. adaptable , quick learner and punctual 3. a good listener 4. goal oriented , positive and supportive.',\n",
       " 'summarya programmer and application developer with over 1 years of extensive experience in java application design and development , testing , support using java and j2ee technology. extensive experience in full software life cycle development analysis and requirements gathering , design , development , testing , deployment , support , maintenance and enhancements. experience in configuring and deploying web - based applications using struts framework. experience on web servers like apache tomcat 6.0. strong knowledge on oracle 11g. knowledge on javascript , html , xml. strong analytical , problem solving and debugging skills with excellent understanding of system development metrics , techniques and tools. excellent communication , interpersonal and presentation skills along with the ability to quickly adapt to new environments and learn new technologies. highly organized with ability to manage multiple projects and meet deadlines. ability to work independently with minimum supervision as well as a group member. experiencecurrently working as software engineer in hewlett packard enterprise payroll of tranway technologies pvt ltd from june 2016 to till date. projectproject name : luxottica pos triage client luxottica role software engineer duration from june 2016 to till date team size : 8 members environment software java , sql , linux , spring mvc project description luxottica is a global leader in the design , manufacture and distribution of fashion , luxury and sports eyewear with high technical and stylistic quality. among its core strengths , a strong and well - balancedbrand portfolioincludes iconic proprietary brands such as ray - ban , oakley , vogue eyewear , persol , oliver peoples and alain mikli , as well as highly attractive and prestigious licenses including giorgio armani , burberry , bulgari , chanel , dolce and gabbana , michael kors , prada , ralph lauren , tiffany and co. , versace and valentino. contribution : as a team member , was responsible for understanding the client requirements clearly. develop the technical solution for the scenarios given in the project code fixes , error detection , error correction identifying the clear solutions. involved in designing components , writing the web services , developing features and unit testing. designed and created sql databases , tables and views based on user requirements skillsprogramming languages : core java , j2ee. markup languages : html , css. web application server : apache tomcat. database : mysql5.0 and oracle 10g. os platform : windows xp 7 , ubuntu. educationb.e stream computer science and engineering. university : vtu , college : angadi institute of technology and management belgaum year of passing 2014 puc stream : pcmb pre - university education board , bangalore s.s.l.c karnataka secondary education examination board , bangalore core java and advanced java course completed in jspiders bengaluru.',\n",
       " 'summaryexperience on working with bigdata and hadoop file system. installing various hadoop ecosystems like pig , hive , sqoop , flume mongodb. hands on experience in working with ecosystems like hive , pig , sqoop , mapreduce , flume , spark. hands on experience in analyzing the log files hadoop ecosystem and finding root cause. experience in hdfs data storage and support for running mapreduce jobs. strong knowledge of hive and hive analytical function. capturing data from existing database that proves the sql interface using sqoop. efficient in building pig , hive , mapreduce script. efficient in migrating data from different databases( i.e teredata , mysql ) to hadoop. successfully loaded file to hive , hdfs from mongodb. loaded datasets into hive for etl operation. hands on experience in ide like eclipse. academics : b.e ( industrial and production ) passed in june 2016 from pda college of engineering , gulbarga. experiencesoftware developer : sept 2016 to present install hadoop applications and developing the program for sorting and analyzing the data. import export the data into hdfs , develop udfs using hive , pig latin , java. projectproduct implementation ( current ) client : rbs , tcs organization : vasundhara infotech team size : 06 technology : teredata , sqoop , hive , hdfs , cognos , impala duration : 05th sept 2016 to till date it is a baking domain project , royal bank of scotland is one of the leading bank in the uk. through its world - class networks , it provides a full range services to its customers. project description : in rbs a product level report is required for higher management to take major decisions. several source systems includes in teradata which is having source data in fact tables , using scoop migrating data from teradata system to hadoop layer. once data has been migrated into hadoop layer applying several transformation in hadoop layer and finally generating report into cognos tool were it help us to pull the report. role and responsibilities : involved in start to end process of hadoop installation. executed queries using hive and develop the mapreduce jobs for analyze the data. develop pig latin script to extract data from web server output files to load into hdfs. develops the pig udfs to preprocess the data for analysis. developed hive queries for the analyst. involved in loading data from linux to hdfs. loading data directly into hdfs using flume. importing and exporting data in hdfs and hive using sqoop. extracting file from mongodb using sqoop and placed in hdfs for processing. written hive udfs to extract data from table. involved in creating hive tables , loading with data and writing hive quires which will run internally in mapreduce way. managed hadoop log files. skillshadoop big data : hdfs , pig , hive , sqoop , flume , kafka , mongodb , spark , scala languages : core java , c databases : mysql , mongodbs dev. tools : eclipse platforms : windows( 2000 xp ) , linux scripting : shell scripting education3ri technologies certified bigdata hadoop developer in pune.',\n",
       " 'summaryhaving good knowledge on hadoop big data. extensive knowledge on hadoop ecosystem components like hdfs , map reduce , pig , hive , sqoop , impala , hbase , flume and oozie. having good knowledge on writing map reduce jobs in hive , pig. having good knowledge on importing and exporting data from different systems to hadoop file system using sqoop. using hadoop ecosystem components for storage and processing data , exported data into tableau using live connection. having good knowledge on creating databases , tables and views in hiveql , impala and pig latin. strong knowledge of hadoop and hives analytical functions. implemented proof of concepts on hadoop stack and different bigdata analytic tools , migration from different database ( i.e. mysql ) to hadoop. having good knowledge on using oozie to define and schedule the jobs. having good knowledge on storage and processing in hue covering all hadoop ecosystem components. load and transform large sets of structured , semi - structured and unstructured data using hadoop ecosystem components. efficient in building hive , pig and map - reduce scripts. having good knowledge on using tableau , quickview reporting tools. having knowledge on zookeeper. good knowledge on different data sources like flat files , xml files and databases. having good knowledge on sentiment analysis. having good knowledge on managing hdfs file system. having good knowledge on all flavours of cloudera hadoop. projectproject title : bluetooth server socket api based chat application. no of members in team : three members. duration : 6 months. description : the application uses the bluetooth adapter to discover reachable bluetooth devices and instantiates a secure connection using a known mac address. the application invokes androids bluetooth server socket api to establish communication with other devices. technologies : j2ee , uml. tools : android sdk , eclipse. project title : cloud coumputing - online book sales with mobile sms. no of members in team : four members. duration : 3 months. description : the application is used how a user gets registered into the website where he finds various categories of books which he can purchase. after the successfull submittion of the project a sms is send to his mobile. technologies : uml , j2ee , pure xml. tools : eclipse - rad , db2 strengths : 1 sincere , dedication and hard working. 2 good communication and inter personal skills. 3 confidence in things getting done. achivements : 1 participated in technical events dream spark yatra 2011 , 12 and appfest 2013 conducted by microsoft. 2 attended to infosec 2013 organized by cdac and csi , workshop on android application development organized by coing solutions. skillsbig data technologies : hadoop , hdfs , hive , pig , sqoop , hbase , flume , impala and oozie. olap tools : sql server reporting services , tableau. data modeling tools : tableau. languages : sql server and shell scripting operating system : windows xp windows 07 , 08 and linux office applications : ms office 2007 2010 and 2013 schedulers : oozie educationcourse institution university percentage ( % ) year of passing b. tech( cse ) jayamukhi institute of technological sciences , narasampet , warangal , jntu , hyderbad. 62 2014 intermediate narayana junior college , hyderbad. 86.2 2010 ssc little flower high school , thorrur. 66.6 2008',\n",
       " \"summary1 year of experience in information technology. possess excellent customer facing skills and client relationship skills. has working experience on hadoop , java , hive , sqoop , sqoop2 , flume , oozie , kafka and mysql. s. no. company role period 1 aaum research and analytics , iit madras research park chennai data engineer - data engineering and visualization( big data ) may 2016 to till date skillsexperienced in several it platforms , operating systems , servers , languages and tools. experienced in installing , configuring , and administrating apache hadoop using hortonworks distribution. experienced in windows azure cloud , aws and ibm softlayer for deploying hadoop cluster. experienced in hadoop , java , hive , sqoop , flume , oozie , kafka , spark and mysql. having knowledge about apex , falcon , nifi , mongodb , postgresql , elastic search , kibana dashboards. solid knowledge of core java .. strong analytical , conceptual , and problem - solving abilities. strong people skills for building effective teams and managing offshore team. ability to understand organization ' s goals and objectives. ability to learn new technologies quickly. technology : hadoop , mapreduce , java , hive , sqoop , spark , flume , oozie , kafka , hbase , apex , falcon , nifi , kibana. operating systems : windows , linux ( ubuntu ) . cloud computing : windows azure , aws , ibm softlayer others : eclipse , hortonworks. educations. no. course board university year of passing percentage 1 b.tech ( cse ) jntu - hyderabad 2015 62 % 2 intermediate board of intermediate 2011 83 % 3 s.s.c board of secondary education 2009 60 % attended apache apex technical professional workshop by data torrent co - partner of apache apex.\",\n",
       " 'experienceattended 1 day workshops : cracking the code organized by csi ( student chapter ) in acharya institute of technology karnataka state ethical hacking workshop and competition projectproject : real time extraction of data from social media for sentiment analysis language of implementation : java , hadoop big data( twitter streaming api , flume , hive , hbase , oracle ) operating system : windows 7 , ubuntu description : project using hadoop framework .twitter sentiment analysis is application to calculate the customer sentiments regarding product in question by collecting and analyzing tweets in real time data and batch processing. skillssound knowledge in bigdata hadoop , hdfs , mapreduce , hive , pig , flume , hbase. sound knowledge of languages : java , j2ee , c++ , c sound knowledge in servlets , jsp , spring , struts , hibernate , design pattern. good knowledge in jdbc , software testing , data structure , computer network microsoft office : ms ( word , excel and access ) experience in using test cases to test the application functionality against the requirements manually expertise on sdlc models , test levels , test types and test design techniques ability to generate test scenarios , to write test cases and to collect test data database : sql , dbms , oracle general and graphic application : html 5 , php , css , javascript( js ) , webservices. operating system : windows family , ubuntu ide : eclipse , netbeans. server : weblogic , tomcat . a_a : participated in science olympiad in 2009 and got award for it. participated and won prizes in terri - quiz in 2009 and 2010 actively participated in various co - curricular activities in school and college. worked as a coordinator in steigen forum - a student technical forum interpersonal skill ability to rapidly build relationship and set up trust. confident and determined ability to cope up with different situations and flexible. ability to inspire others and self motivated agreeableness , inspirer , positive thinker and expressive. eager to learn new things. creativity and persistence. educationclass degree institution year b. e. ( information science ) acharya institute of technology 2015 xii ( c.b.s.e. board ) guru nanak higher senior secondary school 2010 x ( i.c.s.e. board ) carmel convent 2008 experiential learning training programmes attended soft skills training in 2nd sem of b.e. learned html in 4th sem of b.e. java application development in 5th sem , winter internship program it',\n",
       " \"projectbig data technologies : analysis of imdb dataset using hadoop map reduce hive pig cassandra mongodb developed application using the concept of chaining of map reduce jobs and map reduce joins to derive statistics from huge data sets. implemented various complex pig latin , hive and cassandra queries to gain insightful analytics of dataset implemented custom made functions for text formatting in hive and pig query languages. implemented recommendation system using mongo db and java adv. computer network : intra and inter domain routing protocols using java udp sockets implemented path vector routing algorithm using udp socket programming in java. programmed routers to compute shortest path to other routers this application also works well with failures in routers. adv. operating systems : mutual exclusion algorithm in distributed system using java multithreading implemented maekawa ' s mutual exclusion algorithm with deadlock handling for distributed operating system in java socket programming. each node communicates through persistent tcp connection to other nodes. concurrent requests from multiple nodes to enter critical section are handled by implementing lamport ' s logical clock. web technologies : online examination system using php , mysql designed online examination system using html , css , php with mysql database to eliminate the need of paper based test in the university. built several forms using jquery and have used javascript for form validations. ajax has been used to generate dynamic content on the html page. computer network : client - server chat application designed the chat application using socket programming in c language. implemented udp connection between client to proxy and tcp connection between proxy to server in a multithreaded environment. achieved smooth communication between 2 clients. skillsprogramming languages : c , c++ , java , python operating system : linux , windows ( all versions ) database : oracle , mysql , ms - access bigdata technologies : hadoop map reduce , hive , pig latin , cassandra , sqoop , mongo db web technologies : html , css , javascript , php , jquery , ajax , xml , xquery tools : php myadmin , adobe dreamweaver cs5 , net beans , eclipse , ms office , altova , ms visio , cisco ucce , borland star team , hp ovsd tool educationmasters in computer science may ' 14 the university of texas at dallas , richardson , tx gpa : 3.66 4.0 bachelors of technology in computer engineering may11 nirma university , india gpa : 7.29 10.0 relevent coursework________________________________________________________________________________________ database design advanced computer network algorithm analysis and design adv. operating system object oriented analysis and design big data analytics and management cloudera certified hadoop developer june14\",\n",
       " 'experience worked as intern in prime move technologies from may 2015 to jun 2016 role : contributed to then ongoing project related to hospital management with asp.net and c# technology. working as software engineer in huawei technologies from jun 2016 to till date role : works in the bigdata team which handles different components. initially worked in the apache hbase component and contributed in developing customer requirements , open source merges and verification test for hadoop cluster. now works in apache carbondata which uses hadoop ecosystem and contributes to the development of requirements , fixing of defects raised by test team and also worked on designing detailed test cases for the requirements given by the customer. project desktop - as - a - service( daas ) description : desktop - as - a - service ( daas ) is a cloud service in which the back - end of a virtual desktop infrastructure( vdi ) is hosted by a cloud service provider. the main objective is to create a customized desktop vdi with specific features. technologies used : microsoft azure , visual studio , c# duration : 3 months role : front end coding team size : 3 log analyzer description : it is a tool developed to search specific value from logs from a large set of log containing giga bytes of data with minimum amount of time. technologies used : eclipse , java duration : 2 weeks role : end to end coding team size : 1 skills hadoop , java a_a won mla award for top scorers in higher secondary examination during the year 2011 - 2012 member , macs , mec association of computer students. attended workshop on basic cloud organized by google developers group at govt. model engineering college. represented govt. model engineering college cricket team in cusat intercollegiate meet during the years 2013 - 2016. education computer science and engineering [ 2016 ] with aggregate of 8.41 from model engineering college class 12 [ 2012 ] with aggregate of 95 from model technical hss class 10 [ 2010 ] with aggregate of 95 from model technical hss',\n",
       " 'experience company project project skills education',\n",
       " 'summary1+ years of exclusive experience in data science , r programming , data analysis with statistics and machine learning , model building , data visualization.hadoop and its components like hdfs , map reduce , pig , hive , sqoop , hbase , flume , kafka , oozie. successfully delivered initiatives ( implementation and development ) on big dataanalytics. capable of developing r models for pre - processed data using rstudio , and applying machine learning algorithms. building models for preprocessed data and predictive models using rapid miner. hands - on experience of machine learning and data mining algorithms such as decision trees , classifiers , clustering , and regression using r. hands on knowledge of r or python for data analysis. visualizing the finalized data using data visualization tool tableau qlik. experience in sql hiveql. experience inpig , hive , mapreduce , scoop , flume , kafka , hbase and other hadoop applications. experience in column - family based databases. ( hbase ) experience in using cloudera and horton works and their eco systems. excellent knowledge on hadoop ( gen - 1 and gen - 2 ) and various components such as hdfs , job tracker , task tracker , name node , data node , resource manager ( yarn ) . involved in writing the scripts to reduce the job execution time. good with unix commands. good communication , interpersonal , written , analytical , problem solving and team building skills. experiencecurrently working as data analyst , sysmedac technologies , chennai since jan 2017. worked as bigdata hadoop devloper , vepabiz consultants pvt ltd since feb 2016 to dec 2016. projectclient : new media group( nmg ) hong kong. project : data analysis and model building. role : hadoop developer and jr.data scientist. environment : hadoop , data science , machine learning , apache pig , hive , rstudio , rapidminer. description : the purpose of the project is to process the data and to create recommender system model using r language and rapidminer tool. the solution is based on the open source big data hadoop and rstudio. model building : by using development tool rstudio clean the data , cluster the processed data and create association rules and store the recommended data for every user. creating same model using rapidminer. visualizing the clustered data using visualization tool tableau 9.0. created dashboards for the processed data. roles and responsibilities : 1. moved all website user masked data to hdfs for processing. 1. written apache pig scripts to process the data. 1. created hive tables to store the processed results in a tabular format. 1. imported data into rstudio server and performed data preprocessing and clustering using r language. 1. model building for the data using machine learning algorithms in rstudio , used apriori algorithm to build recommender system model. 1. creating dashboards using tableau. 1. client : intelliasia soft. 1. project : credit risk scoring. 1. role : hadoop developer and jr.data scientist. 1. environment : hadoop , apache pig , hive , rstudio , rapidminer. description : the purpose of the project is to preprocess the data and calculate credit risk score for the banking data. the solution is based on the open source big data hadoop and rstudio. model building : by using rstudio server import the data from hive table to rstudio using rhive. preprocess the data imported from the hive table using r language. allocating weights to the selected levels of attributes. calculating risk score using scoring techniques. creating prediction models for the risk factor. creating dashboards for the processed data using tableau. roles and responsibilities : 1. moved flat files bank data to hdfs for further processing. 1. written apache pig scripts to process the data. 1. created hive tables to store the processed results in a tabular format. 1. imported data into rstudio server and performed data preprocessing , clustering , allocating weights using r language. 1. visualization of data using tableau. skillsoperating system unix , linux , windows. programming languages core java , sql , r language , python , mapreduce , pig , hive. frameworks hadoop.1.x.x , 2.x.x. ides eclipse. hadoop monitoring tools hortonworks platform , cloudera , ubuntu. database mysql , oracle , hbase , mongodb. development tools rstudio. types of datasets flat , csv , tsv , json , xml , url logs , excel sheets. bi tool tableau. educationpursued bachelor of engineering in computer science with 68.1 % from agm college of engineering and technology , hubli( affiliated to visvesvaraya technological university ) . ihavesuccessfullytrainedinbigdatahadoopdeveloper , jpa solutions , banglore. successfully completed online courses on big data technologies from data camp and big data university ( ibm initiative ) .',\n",
       " 'summary expertise in management professional experience : six months at syntego global it system pvt. ltd. self motivated , dedicated , determined and honest , innovative thinking , good leadership qualities. projectbe project title : smart traffic office system. duration : 1 year description : as vehicle increases , the traffic and also the offences increase. so the traffic police are urgent to require a mobile office system which can be used for checking the driving license , searching and collecting the data information of the related people and vehicles in the working spot. mailto : niksviko@gmail.com roles and responsibilities : software development . web development and testing. skillsskill category skill list programming languages java , c , c++ , sql , html5. dbms mysql , operating system microsoft windows ( xp , vista , windows 7 , 8 , 10 ) , linux ( fedora , ubuntu ) web html , testing. development tools eclipse ide , netbeans a_a state champion of chess. debate competition winner. classical singer. arranged events and worked as anchor. worked as coordinator in state level paper presentation competition( college event ) attended training of zensar mnc company. educationexamination university board year of passing class percentage engg. ( be ) pune 2016 distinction. 73 % h.s.c latur board 2012 distinction. 75.33 % s.s.c. latur board 2010 distinction. 85 % aggregate : 68.89 % discipline : computer engineering',\n",
       " 'experienceworking as software engineer in avasoft , chennai from jan 2016 to till date. projecttitle : baroscopic data analytics using mapreduce in hadoop description : weather sensors collecting data every hour at many locations across the globe gather a large volume of log data for analysis with mapreduce , since it is semi structured and record - oriented. thus by using mapreduce concept , a large set of weather data can be analysed easily with optimum result. title : alumni management system description : mini project is based on the web creation for alumni management system which manages the information about the passed out student of the college. co - curricular activities presented a paper on redaction in the symposium held at sri krishna college of engineering and technology on march 1st 2013. presented a paper on digitaljewellery in the symposium held at ranganathan college of engineering on september 3rd2013. - asset management project - qlik view project skillsobject oriented programming database management systems programming languages : c , c++ , java database : microsoft access packages : ms - office package scripting language : vb - script , html , css bigdata tools : hadoop a_awon second prize in first open house exhibition project titled precautions in nuclear reactor held at p. a. college of engineering and technology. won first prize in skit conducted at p. a. college of engineering and technology. won prizes in drawing competitions at school level. educationcourse name of the institution university board marks in % cgpa year of passing b. tech ( it ) p. a. college of engineering and technology , pollachi anna university , chennai 7.8 2015 hsc nirmalamatha matriculation hr. sec. school , coimbatore matriculation 69.08 2011 sslc nirmalamatha matriculation hr. sec. school , coimbatore matriculation 75.60 2009 certified for attending workshop on basic skills in computers held at government college of technology , coimbatore during 2011 certified for attending workshop on 2d and 3d animation held at p. a. college of engineering and technology during 2013. certified for attending training on bigdata held at duratech solutions during 2015. certified for completed course on java and .net at accord infomatrix , chennai during 2015.',\n",
       " 'experience duration : june 2012 december 2012 role : researcher : i have worked as a researcher in the field of education technology for this company. i used to research and review various online products apps and tools which solve various educational needs. that included the latest trends and predictions about education technology where cloud saas and learning analytics is what i saw were the future. learning analytics in case of big edtech players require big data analytics which attracted me to hadoop and data analytics. research engineer : i have worked as a research engineer for this company. i worked on the companys portal using php , mysql and cms. it includes designing the whole user interface and the development using different methodologies. key deliverables : responsible for creating use cases for usage of hadoop and related technologies in education. developing php based web applications on content management systems like joomla , drupal and wordpress. working on methodologies such as extreme programming and test driven development. participating in management meetings for team assessment and project planning on a regular basis. hadoop big data associate with certified training from bigdatatraining.in and valuable exposure on project and hdfs , mapreduce , apache pig , hive , hbase , zookeeper and other ecosystem tools. b.tech ( information technology ) with 7 months of valuable exposure working with company as researcher and developer. undertaken the following subjects in engineering : java , c , c++ , .net , web development and design , database management ( mysql , access ) , data structures , software engineering and project management. gained valuable exposure in developing applications , conducting testing , interfacing with clients and working under stipulated timeframe. possess sound communication skills with a bent of mind to grasp new concepts easily and quickly. career objectiveto obtain a rewarding position in the field of it in a good company where i can experience and test my skills with highly professional peers. looking for challenging career , always on a lookout for a positive and bigger outlook , sets levels and standards that exceed expectations , a learner for life. software engineer skillsintroductory intermediate hadoop skills : pig , hive , zookeeper , hbase , sqoop hdfs , mapreduce c , c++ , css , php , mysql , javascript , .net java , html tools : netbeans dreamweaver , photoshop operating systems : linux - ubuntu , centos , mac - os windows web servers : apache , tomcat wamp , xamp dbms : n0sql ( hbase ) mysql other systems : drupal , zend framework joomla , wordpress web projectshadoop based : title : sentimental analysis of twitter data duration : february 2012 to present environment : hdfs , mapreduce , pig , nlp , hbase and java description : sentimental analysis on big datasets including data from other sources using hadoop and related technologies. the project aims to determine the attitude of a writer with respect to some topic or the overall contextual polarity of a data. using hadoop ecosystem with natural language processing , the analysis gives the positive negative feedback. php based : title : university erp system duration : may 2011 to july 2011 team size : 3 environment : windows , php , mysql , javascript , css , html and ajax. description : an intranet facility is to be developed for a small and medium organization such that the different departments can share information in an effective manner. this intranet system is easy to use , visually attractive , enterprise intranet portal software with workflow automation and robust reporting. title : search engine development duration : may 2010 to july 2010 team size : 2 environment : windows , php , mysql , lamp stack description : a search engine which is developed for students to search anything related to education. it doesnt have any spam and advertisement. the result shown by the search engine will be strictly related to education. a_asuccessfully maintained smooth flow of operations within the team and ensured accomplishment of a group plan during the absence of the team lead. adhered to strict testing practices to ensure that developed projects were at zero defects and of optimum functionality. instrumental in fostering team spirit among members and mentoring new interns as and when necessary. eduvative technologies llp 7 months researcher engineer educationb.tech ( information technology ) from amity school of engineering and technology , uttar pradesh in 2012. secured 69 % , 6.3 cgpa. 12th grade ( cbse ) in ( 2008 ) . secured 81 % . 10th grade ( cbse ) in ( 2006 ) . secured 82 % .',\n",
       " \"experiencehave attended the workshop on maharaja engineering college ncvcc13 held on 11th april 2013. skillsgood programming skill in c# , bigdata , sql. quick learning ability. operating system : windows. curriculum project project name : human identification through bio - metric technique semester : 6th duration : 90 working days team size : 6 language : asp.net description : this project is very used to improve the security performance. in this project finger print is used as human verification. now a days some persons make the duplicate human verification. but this human finger print is used for identifying human and displays their personal details through internet. so this project improves the security performance and avoids forgery naturally because one human finger print is different from other human. project name : bike sharing demand duration : 45 days language : hadoop , mapreduce , pig , squoop description : this project is about an application which maintains the inventory and information ' s of the bi - cycle of the client. this is mainly developed to monitor and maintain the bi - cycles , which are rented , booked and availability. using this application , we can enter the customer details , who rented the bi - cycle and the information ' s such rented time , and hour calculations with amount. also it automatically calculates the availability of the cycles for pre - booking. this is mainly target for the commitment to the client in faster way. special interest hadoop , map reduce , pig , hive and squoop. a_amake a short film in the national level symposium held on 15th and 16th feb , 2013 at ksr college. participated in seven days nss special camping program on the theme healthy youth for healthy india organized at komma kovil village i have completed my bigdata course in thinkup institued. educationb.tech ( information technology ) name of institution : vellalar college of engineering , erode percentage secured : 63.3 % year of completion : april 2013. diploma name of institution : kongu polytechnic college , perundurai. percentage secured : 87 % year of completion : 2007 - 2010. sslc name of institution : kongu vellalar.mr.hsc.school , chennimalai. percentage secured : 65 % year of completion : 2006 - 2007.\",\n",
       " 'experiencehdfs developer. ( fresher ) june 2016 present. ( fresher ) learned hadoop , hdfs , hive , pig , sqoop , flume from insanalytics. presently working as a senior web developer in integrity web informatics from september 2012 to till date this job involves website development and designing using php and mysql , html , css , jquery and javascript , withcodeigniter( ci ) and cakephp framework . having knowledge on wordpress.also learned and build projects using drupal a widely used php framework. specially experienced on codeigniter and cakephp , serverside programming , works with filezilla and several others ftp clients like putty , winscp etc. having knowledge on bigdata hadoop. worked as a senior web developer in techdevine from 21stnovember 2011 to june 2012. this job involves website development and designing using php and mysql , html , css , jquery and javascript , withcodeigniter( ci ) framework worked as a senior web developer in dweb from 3rd july 2012 to august 2012. this job involves website development and designing using php and mysql , html , css , jquery and javascript , withcodeigniter( ci ) framework portfolio : projecthttp : hostdelicate.com techdemo ( cakephp crm ) username pass - admin@admin.com 123 http : insanalytics.com lms ( using cakephp crm ) username pass admin 12345 http : www.bookurrooms.com ( using ci crm ) username pass admin@thissite.com admin@owner http : www.waptrendz.com ( using codeigniter , video and wallpaper downloadable site with payment gateway only for south african citizen ) http : www.m.waptrendz.com ( mobile version of the previous site ) http : hostdelicate.com cakepos ( it is an application built using cakephp framework ( one of the most advanced framework for php on present time ) . this application basically used to store process sales products , still in development phase but one of the most classical webapp i have ever made. test username password - admin 12345 ) http : www.inwebarticles.com ( article posting website using cakephp ) http : www.jptraders.org ( forex related website with payment gateway and various packages ) http : www.alpha - imobiliare.com ( cakephp property search. we scrap data from another third party site and using them built it ) http : lotsofun.ca , http : lotsofun.ca mobile ( normal php. events arrangement , post , rent relating site .paypal , pdf export implemented ) http : aparttovilla.com , http : candcrent.com ( codeigniter framework , properties buying and renting. various users can register about their property for selling and renting and other users can search and contact them .with payment gateway stripe ) http : alphastock.alphaoils.com ( username password - anirban.in.net@gmail.com 12345678. php framework : codeigniter.customer relationship management ( crm ) project on book store , to buy , store books , sell to customer and students , stock maintenance , return stack , sales reports , exports as pdf all records ) skillshadoop , hdfs , hive , pig , sqoop , flume. core php , mysql , codeigniter , cakephp. have knowledge on wordpress theme integration and custom plugin creation. html , css , ajax , jquery , javascript. payment gateway , pdf export , image classes in php. gd library , scrapping , cron , google api , facebookapi , twitter api , google location data. direct working on servers and have knowledge of server core features. highly proficient at dreamweaver , netbeans , komodo , wamp , xamp , excel , word , powerpoint , have good experience in multimedia software e.g. adobe photoshop , sony sound forge. possess good interpersonal and communication skills and committed to assigned tasks. strong team working and leadership quality. familiar in software life cycle from requirement analysis to successful delivery educationbachelor of technology ( computer science and engineering ) . obtained bachelor of technology in the discipline of computer science and engineering from durgapur institute of advanced technology and management , under west bengal university of technology in the year 2010. academicscore : examination institute board university year of passing percentage b.tech durgapur institute of advanced technology and management w.b.u.t 2010 6.96( cgpa ) 10+2 uttarparagovt.high school ( pure science ) w.b.c.h.s.e 2006 75.6 % 10 uttarparagovt.high school w.b.b.s.e 2004 77.25 % 1 ) achieved big data certificate from big data university ( https : bigdatauniversity.com an ibm community initiative ) 2 ) achieved hadoop certificate from big data university 3 ) achieved hive certification from big data university 4 ) achieved php , mysql certificates from cmc.',\n",
       " 'projectenvironment : hadoop , hdfs , hive , sqoop , flume and hbase. role : hadoop developer. project descript ion : big data analytic framework designed to consume and monitor network traffic and machine exhaust data of a data centre. it is extensible and is designed to work at a massive scale qualif ication( b.e computer science engineering ) : institution : ponjesly college of engineering. , nagercoil , tamil nadu. university : anna university chennai. year of graduation : 2011 aggregate : 61 % p roject : load balancing issue in dht - based peer - to - peer system. client : ponjesly college of engineering. p latform : windows xp professional. language used : java. skills big data ecosystems : map reduce , hdfs , hbase , hive , pig , scoop , oozie. p rogramming languages : java , pig , hive , sql. scripting languages : java script. data base : nosql , mysql. tools : eclipse. p latform : windows , ubuntu , centos. planning and organization. leadership qualities. adaptive to any environment. team work. strength : good team worker. organized and well structured at work. independent , pro - active and self - motivated , able to work under pressure. big data ecosystems : mapreduce , hdfs , hbase , flume , hive , pig , sqoop , oozie. operating systems : ms windows xp , windows 7 , windows 10 , linux , centos. databases know n : my sql , sql 2000 , oracle. languages know n : java , c c++. mirox cyber security jun 2015 - may 2016 bigdata developer mirox cyber security and technology ( p ) ltd is an it security , networking and bigdata solution based company , providingdevelopment and solutionsin security , networkingand bigdatarelated areas. key deliverables : capable of processing large sets of structured , semi - structured and unstructured data and supporting systems application architecture. tested raw data and executed performance scripts. shared responsibility for administration of hadoop , hive , pig , sqoop , oozie. educationhsc institution : st johns hss , trivandrum. branch : science - ( computer science ) board : state year : 2007 aggregate : 55 % class 10 institution : st marys hss , pattom , trivandrum. board : state year : 2005 aggregate : 56 %',\n",
       " 'skillsnetworks data analysis good communication skills with a positive attitude. adaptable to any situations. educationbig data ecosystems hdfs , map reduce , pig , hive , hbase , sqoop , oozie programming languages c , c++ , java database my sql , nosql operating system windows xp , windows 7 , linux web technologies html , xml , javascript',\n",
       " 'project1. title : firewall. description : accessing file from the client device and getting informations of call logs , messages , timing of switch on and off of the device and even gps location. tools : eclipse. platform : android. 2. title : year planner language : c# description : this project is developed to remember the events while using laptop it helps user to better time management. mini project : title : big data analytics for airline data using hadoop description : collecting and aggregating data coming from heterogeneous web servers , aggregating delay time of departure and arrival , the maximum visited places , creating table in hive and providing an interface to end users for analyzing and simple querying. operating system : ubuntu. technology : hadoop. audit course : android certificate course : - duration 2 month.[ june 2016 to july 2016 ] . personal assests : hard working. willingness to take instruction and responsibility. ability to learn new skills accurately. adaptable in any working environment. skillslanguages : c , c++ , java , php , c# , android , hadoop databases : mysql. operating systems : windows , linux educationmailto : vidyaraniuppin14@gmail.com',\n",
       " '',\n",
       " 'summaryworked with biz trans system technology from 2015. customer shri ram groups tenure july15 mar 16 project description business object developer client shri ram value service role bo webi developer responsibilities : working with business users from offshore. co ordinated with technical and functional team. offshore requirement gathering for the project. involved in requirement gathering and determining the scope of the project primary responsible for increasing product quality and maintaining process flow reports were created using webi report utilities module. collaborated with senior level managers in increasing the product quality in reducing the defects involved in quality assurance team to meet the data within specified standards post go - live support. skillslanguages : java database : mysql operating system : windows 7 xp , vista , linux tools : msword , ms excel and ms powerpoint technology : sapbo , bigdata and hadoop , pig , hive , hbase , sqoop , mapreduce educationb.tech( it ) 6.5( cgpa ) , anna university , chennai class xii 75 % , stmarrys , salem class x 75 % , , vaideeshwara , salem one year experience in sap bo java , bigdata and hadoop technology , pig , hive , hbase , sqoop , mapreduce. undergone in - plant training in tvs. entrepreneurship development. industrial visit to kashiv info - tech , bsnl , and nlc',\n",
       " 'summary. around 1 year project experience working with big data and hadoop development completed 6 months of big data and hadoop developer training at simplilearn. capable of processing large sets of structured , semi - structured and unstructured data. completed one project work assigned on big data sound knowledge on programming language java and hadoop. two years of experience in banking domain experiencecompany : axis bank period : march 2014 december 2015 responsibilities : generating leads ( collecting names and contacts of individuals in the market ) maintain customers satisfaction with excellent service for a long term relation introduce new investment products to the clients like mutual funds , life insurance , general insurance , d - mat train and support new colleagues banking software : finacle , ets , branch analytics business skills : clarity in communication commitment to service confidence to achieve diligence in meeting target projectproject name : social media - real world project by simplilearn project description : as part of a recruiting exercise of the biggest social media company , they asked candidates to analyze data set from stack exchange to find out ; - top 10 most commonly used tags in this data set. - average time to answer questions. - number of questions which got answered within 1 hour. - tags of questions which got answered within 1 hour. responsibility : understand the structure of data. load the data in the database ( rdbms ) . sqoop helps to pull data into hdfs. map reduce used to extract the fields. pig is used to do the transformation. hive used to show the results and hbase shows the results on the user interface of a web application. tools used : mapreduce - it is used for processing large data sets over clusters of computers using distributed programming. sqoop - it provides parallel operations and fault tolerance. pig - it can store and analyze any type of data which either structured and unstructured. hive - it can manage vast amount of structured data sets , by using hql language , its similar to sql. hbase - it provide random read and write , can perform thosand of opertion per second on large data set. skillshadoop framework : mapreduce , pig , hive nosql : hadoop hbase hadoop tools : sqoop and flume programming languages : java , visual basic , html scripting : shell scripting ide : eclipse operating systems : linux , windows educationmba : rvs college of engineering and technology ( anna university ) completed in 2014 b com with computer application : dr. ngp arts and science college , coimbatore ( bharathiar university ) completed in 2010',\n",
       " 'summarya competent professional with over 1. years of experience in r and d as software design and development. worked on spring and hibernate framework. possess knowledge of core java , jsp , servlet. possess knowledge of hosting web portals , php , big data hadoop , . excellent communication , problem solving and presentation skill. a comp etent professional with over 1. years of experience in r and d a s software design and development . worked on spring and hibernate framework . possess knowl edge of core java , jsp , servlet . possess knowledge of hosting web portals , php , big data hadoop , . excellent communication , problem solving and presentation skill. total experiencecompany : inditronics pvt.ltd designation : sofware developer. software used : eclipse , netbeans. duration : may2016 - till date. responsibilities understanding of customers project requirement concept technical specification , help and amp ; co - ordinate with design engg. to full fill it . anticipation of problems timely reporting finding solutions with the help of development engg. or management customer. make technical clear project , incorporation of project in system , project execution plan. to make flexible and reliable software as per customer requirement. and help design team in technical and software related problems preparation or validation , approval of specifications , test plans , customized drawings for procurement , all with management. co ordination and amp ; guidance to all design team. status reporting advice to management customer. handling customers resolving queries , obtaining customer clearance for project. finalization of apqp , release of data for approval to manager. interaction with all team , hr reception etc. projects and work experience. 1. efficient deduplication using hadoop in this system , we avoid the deduplication data ( structured and unstructured data ) on both client and server side. the system allows user to choose a file of any type of any size. if the file grater then it will divided into chunks , encrypted , compress and store both at client and server side. if file size is small then it will directly store on server. platform : windows technology : hadoop 2.1.2 , hdfs , core java , hbase 2. dynamic pattern generation using picture data set for secured authentication in cloud. this project mainly developed to provide the security in cloud. in this authentication will provide to by selecting the password from images and also by click on particular part of user define or server provide images. user password is store in the form of images. user will be authenticate after image and click points are matched with database. platform : windows technology : jsp , mysql , html , css 3. event management software managing various types of events of organization is very tedious and time consuming task these days. our product provides easy and reliable solution to this problem. event management is used for managing the birthdays , official parties , anniversary and other events , keeping track of material , keeping track of material , keeping track of vendors and contractors , configuring the details of the company and report generation for each module. , it also provides additional features like invoice generation , reminder to of each event to user. platform : windows technology : mysql 5.0 , core java 4. web portal for ticket booking the system is design to book tickets of films. user register to system and book the no of tickets and do the online payment. in this system user hierarchy is maintain by providing the unique id while registration. we were provided the authority panel to update delete and insert the user information in database. platform : windows technology : mysql 5.0 , spring , hibernate. 5. donor management system the system is user for donors to manage their donations. user register to the system and donate their donation to particular charity. in this system user hierarchy is maintain by providing the unique id while registration. we were also provided authority panel to each system incharge to manage all information related to donations and users in system. platform : windows technology : mysql 5.0 , spring , hibernate. - 1 year : company : inditronics pvt.ltd designation : sofware developer . so ftware used : eclipse , netbeans. duration : may 2016 - till date . responsibilities understanding of customers project requirement concept technical specification , help and amp ; co - ordinate with design engg. to full fill it . anticipation of problems timely reporting finding solutions with the help of development engg. or management customer. make technical clear project , incorporation of project in system , project execution plan. to m ake flexible and reliable software as per customer requirement . and help design team in technical and software related problems preparation or validation , approval of specifications , test plans , customized drawings for procurement , all with management. co - ordination and amp ; guidance to all design team. status reporting advice to management customer. handlin g customers resolving queries , obtaining customer clearance for project. finalization of apqp , release of data for approval to manager. interaction with all team , hr reception etc. project1. deadstock management system. this project deadstock management system mainly developed for any type organization to manage and keep the track of assets in organization. this software application will people to get information about assets , keep record of assets and modified with permission of authorize user , generation of report in excel , sending notification to admin if any record is modified , sending mail from system to particular user. platform : windows technology : jsp , mysql , html , css skillsarea of interest : database , programming. database : mysql , sql. operating system : windows 8 , windows xp , linux. programming language : c , java , hibernate , struts , spring , html and css , android , php , big data and hadoop. educationcourse year institution percentage class obtained be computer engineering 2011 - 15 m.k.s.s.s cummins college of engg. for women. 60.00 % first class hsc 2009 - 11 n. m.v. girls jr. college 74.00 % distinction ssc 2009 n.m.v girls high school . 85.38 % distinction',\n",
       " \"projectuniversity college of engineering , rajasthan technical university , kota , rajasthan , india bachelors of technology in computer science and engineering aug 2012 july 2016 cumulative percentage 79 % and gpa 3.7 4.0( uk fulbright commission ) academically , my interest lies in the area of systems , with a particular interest in software engineering and data analysis. my explorations have led me to be fascinated by software defined networks. i am also interested in security. apart from data churning and analysis , i also like to work on scripting and coding for complex softwares. i like to do photography and photo editing in leisure time. hand gesture pattern recognition ( hgr - v2.0 ) jan 2016 mar 2016 research and development project prof. dr s.c jain and asst. prof. dinesh soni this project is now increased to that level where we can detect the hand gesture using python as programming language implemented on both matlab r2008a and opencv v2.4.12 , implemented the canny edge detect. k - l transform to optimize the euclidian vectors used in hand gesture recognition. weve done skin filtering , palm cropping , edge detection , feature extraction , classification. minor hand gesture pattern recognition ( hgr - v1.0 ) aug 2015 dec 2015 research and development project prof. dr s.c jain and asst. prof. dinesh soni this project is concerned with the problem of recognition of dynamic hand gestures. we have considered gestures which are sequences of distinct hand poses. in these gestures hand poses can undergo motion and discrete changes. however , continuous deformations of the hand shapes are not permitted. we have developed a recognition engine which can reliably recognize these gestures despite individual variations. the system is fairly robust to background clutter and uses skin color for static shape recognition and tracking. a real time implementation on standard hardware is developed. experimental results establish the equations to track the gestures. vikrant sharma file : g : d4 cv main sharma.vikrant.rk@gmail.com file : c : users shrik appdata roaming microsoft word www.vikrantsharma.in http : www.linkedin.com in rkbvikrant http : www.vikrantsharma.in 2 research internships hadoop optimizations ( bigdata ) hewlett and packard ( hp ) , bangalore , india may 2015 jul 2015 ceo and mgr. billa venkata siva prasad ( abbey soft. pvt. ) proposed optimizations for hadoops distributed file system. analyzed with major components and modified the source code of hdfs to find the performance bottlenecks and features using java with map reduce machine. apache sqoop : the tool designed for efficiently transferring bulk data between apache hadoop and structured data stores such as rdbms. apache flume : studied the distributed , reliable and available service for effective collecting , aggregating and large amounts of log data. apache pig : platform for analyzing large datasets , coupled with infrastructure for evaluating these programs. apache hive : data warehouse infrastructure built on top of hadoop for proving data summarization , query and analysis. which may vary from version to version. university 2012 - 2016 awarded special merit for designing logo of uni. alumni association and ngo. awarded as branch topper in 1 , 2 , 4 , 8 semesters in 2012 - 2016. awarded as state university rank 2 in first semester 2012. high school 2009 - 2011 secured all india rank 2051 in nstse 2011 out of 90 , 000 students. secured all india rank 1975 in nstse 2010 out of 85 , 000 students. secured all india rank 431 in national science proficiency test 2009 conducted by career point foundation. secured nice positions in top olympiads like nsep 2011 organized by iapt and be in top 600 teams in technothlon 2013 organized by techniche ( indian institute of technology , guwahati ) . online student grading portal autumn 2014 asst. prof. naveen dabley developed an online web portal using the scripting languages like php , html , ajax and many other an interactive gui with java - scripts showing total grades , online test , download tutorials , print information , and many more. efficient design of entities in the database and the relationship between them allow for fast processing on the server side. 3 seminars and hackathons inventory management system spring 2014 sr.lecturer inderpreet kaur developed an advanced level inventory management system using php , ajax , css3 interactive gui is provided through java - scripts showing total deals , print information , and many more. efficient design of entities in the database and the relationship between them allow for fast processing on the server side. course registration system summer 2014 asst. prof. anurag mishra computer aided software eng. project where a software requirement specification report and working project is made by using visual basic ( vb ) as frontend language. it includes student registration , grading , scheduling etc. ded in such a way that it provides a new gui to student for selecting a course. whatsapp tech - seed summer 2015 a small python seed to create a whatsapp bot , with regex - callback routes ( just like a web framework ) .implementation based on yowsup which is a practical extension of this bot.in this mini - project ive implemented the basic messaging with media download , group administration , google text - to - speech. captcha2text winter 2015 i used captcha2text to decode some basic captcha images for my page scraping applications. this does not use any advanced image processing algorithms , or any other ocr techniques. this php based system requires you to build a hash list of known letters after which any re - occurance of that letter is automatically detected. but , please be sure to ask the site owner ' s permissions before using this. gcoe parallel computing with gpu ' s ( national workshop ) summer 2016 the gnu centre of excellence ( gcoe ) , iit - bombay has conducted one - day workshop on : parallel computing on gpu ' s and covered parallel computing with matlab gpu computing with cuda embedded super computing with jetson tk1 board tequip ii ( national conference ) winter 2015 the teqip ii sponsored national conference on computational mathematics in engineering and 26th annual conference of rajasthan ganita parishad. study about latest scientific researches on comp. mathematics. robo species ( robo workshop ) spring 2013 attended the workshop under ( robo club ) . learn path tracker , object tracker mini - robots. made the line tracker robot instantly with using given hardware and software. https : github.com akarthik10 captcha2text 4 positions held skyfi lab and roboversity ( robo workshop ) autumn 2013 attended the workshop under ( robo club ) . learn bluetooth operated robots , sensor guided robots , arm architecture , pcb manufacturing and do study on them synchronizing. asd cyber security and consultant ( ethical hacking ) spring 2012 practice the sql - injection and other penetration techniques to find the vulnerabilities of various servers and websites. organizational : white - hat ethical hacking workshop ( university workshop ) summer 2016 organising and coordination committee - the whehw is sponsored by asd cyber security and its based on following modules live on kali 2.0 linux : penetration testing with kali linux( pwk ) offensive security wireless attacks( wifu ) advance web attacks and windows exploitation. organizer teqip iii ( national conference ) summer 2016 general arrangement coordination committee - the teqip ii sponsored national conference on matlab 2016a and latex. it is a faculty development program study about latest scientific researches on latex and matlab. organizer teqip ii ( national conference ) winter 2015 coordination committee - the teqip ii sponsored national conference on computational mathematics in engineering and 26 th annual conference of rajasthan ganita parishad ( rgp ) . study about latest scientific researches on computational mathematics. organizer ( anukriti15 ) summer 2015 design and decoration committee - designed fest - logo , theme - posters ( adobe photoshop cs3 ) for various events organized by the design committee. helped in organizing various events by event group by registering people , accompanying and arranging for respected personalities and publicizing events and was in - charge for music - band performance night. organizer ( thar15 ) summer 2015 design and decoration committee - designed fest - logo , theme - posters ( using corel - draw x7 ) for various events organized by the design committee. helped in organizing various events by registering people , accompanying and arranging for respected personalities and publicizing events. organizer ( anukriti14 ) spring 2014 print and media committee - coordinator for various events organized. helped in coordinating the printing committee for events by registering people , guiding then , arranging the various events related with press and advertising. 5 educationinterest research honors and groups ( 2012 - 16 ) core member in ( student activity cell ) computer and programming group. active member in akshaya patra ( non - government organisation division - iskcon bangalore ) . ex - core member in eloquence club( language club ) . active member in programming club( pc ) . presentation and symposium : secured 1st position in extempore , smart - university ideas in college fest cosint ' 16. secured 2nd position in linux - trouble shooting in college fest cosint ' 16. secured 1st position in android - test , techno - trivia in college fest anukriti15. secured 1st position in logo , brain challenges in college fest anukriti14. secured 1 st position in physics master , technical writing in anukriti14 secured 2nd position in documentary ( inspire india ) in benchmark13. secured 1st position in symposium , paper presentation in college febenchmark13 , anukriti13. marathon : participated in green india , clean india campaigns. participated in marathon for walk to save girl child. non - government organization participation hk.\",\n",
       " 'experiencecurrently working in bigdata and java projects at aegis global limited( itpl , bangalore ) . doj : 16th december 2016 at aegis global limited( itpl , bangalore ) projectdata analysis( sentimental text analysis ) using bigdata skillsenvironment : bigdata , hadoop , hdfs , spark , pyspark team size : 10 role : using spark create rdd , data cleaning using python , sentimental text analysis , visualization time period : 2nd january 2017 to present aiges dialer : environment : jdk 8 , html , css , java script , jsp , jdbc , mysql , eclips ide team size : 6 role : front end using html , jvascript , storing datas from frontend to backend database using jsp time period : 18th december 2016 to present online internet banking : environment : jdk 7 , html , css , java script , jsp , jdbc , mysql , eclips ide team size : 2 role : frontend and backend both time period : 1st march 2016 1st august 2016 educationdata analytics : bigdata framework : hadoop , hdfs , mapreduce , spark , hive tools : cloudera api : pyspark operating systems : windows , linux programming languages : java , python web technologies : j2ee database softwares : mysql jse technologies : jdbc jee technologies : servlets , jsp servers : tomcat , glassfish ides : netbeans , myeclipse core java at cmc ltd j2ee at igcti autocad at jadavpur university degree name of institution board university year of passing aggregate ( % ) bachelor of technology ( b.tech in eee ) pailan college of management and technology westbengal university of technology ( wbut ) 2016 75 10th+2( intermediate , math - physics - chemistery ) south suburban school( main ) wbchse 2012 63 10th( school ) barisha hgh school wbbse 2010 60 achivements : finalist in technical quiz college techfest in the year 2015.',\n",
       " \"experiencecompany : datawitches technologies pvt. ltd. designation : software developer ( sept 2016 to present ) skill set used : java , python. projectgenetic algorithm for ad - campaign optimization deep character - level neural machine translation by learning morphology ( nlp ) name : detection of primary user emulation attack in cognitive radio network using rss - based detection. description : this project is based on wireless networking technology. in this project we shown the simulation of frequency bands using network simulator 2. the proposed system focus on detection of attacker from licensed ( primary ) user frequency band and assign the free or unused frequency band to requested user. so our aim to avoid the wastage of frequency band and detection from attacker. project for diploma : name : digital electronic notice board. description in this project , we developed a system in which notice directly send to the digital notice board from authorized person computer system. because of that it reduce the wastage of paper while circulating the notice. skillslanguages c , c++ , java , advanced java , html , android , jsp , php , python operating systems windows xp , windows 2007 , windows 2008 , ubuntu database oracle , sql server 2010 , mysql , mongodb , hadoop designing tools rational rose , staruml , axure ide eclipse. a_aparticipated in edp development program by udyogwardhini. participated in csi organized hadoop workshop. working as blogger for departmental e - magazine in 2013 - 14. attended android development workshop by mr. atul plandurkar. attended zensar ' s esd training 2015 - 16. paper published at icstsd conference , nashik , maharashtra. strengths positive attitude multitasking educationinstitute university year percentage division be sandip institute of training and research center , nashik. savitribai phule pune university 2016 61.44 % first class diploma ( it ) sandip foundations , sandip polytechnic , nashik. m.s.b.t.e 2013 74.44 % first class s.s.c. lvh high school. maharashtra state board 2009 73.69 % first class\",\n",
       " 'experiencea result oriented professional with 7 years of experience as bi developer. presently associated with aon , gurgaon. currently involved in creating required bi solutions using hadoop and its various components like sqoop , pig , hive , beeline , impala etc. , creating new dashboards using tableau , having exposure in gathering and analysing requirements , creating design and development and performance improvement of cognos and tableau reports. since mar10 : aonhewitt , gurgaon software engineer he aon hewitt provides the services on global human resources ( hr ) outsourcing and consulting firm delivering a complete range of integrated services to help companies manage their total hr and employee costs , enhance hr services , and improve their workforces. a assignments and responsibilities : assignment 1 : recipient of following awards : premium team award at team level for outstanding achievements in 2011. champion award for efforts put in project development and deployment , performance across the tenure in 2012 gold award for efforts put in development and deployment of project for cognos upgrade to 10.1 in 2013. benefits it award for outstanding contribution in platform rationalization ( migration from informix to hadoop ) in 2015 gold award for unmatched teams for displaying exemplary performance to deliver business results with excellence. significant in putting extra efforts by working for extra hours to ensure project completion and timely deliveries. actively involved in cognos , tableau , hadoop upgrades , performance enhancements and various project deployments. projectperiod : mar10 till date client : all us based tableau offline reporting , upgrades , automation of hive scripts duration from feb 2016 to aug 2016 project abstract : project is related to monitor and improve the tableau server health , creating offline solution for tableau reports , tableau dashboards performance improvement , automation of hadoop hive scripts etc. the major task involved to create tableau server health monitoring dashboards , create offline solution for some tableau reports to offload the server and improve reports performance. roles responsibilities as a team member following roles were performed by me : involved in creating multiple health monitoring dashboards for daily monitoring of tableau server created complete offline solution using hadoop and tableau reader to offload tableau server by 30 - 35 % implemented security and provided reports for 1200+ managers with tableau reader. involved in many adhoc tasks like automation of tableau reports migration , tableau reports improvement etc. worked with java team for the automation of hive scripts. an user interface was created similar to etl tools like informatica that will generate the hive scripts. worked in requirement , design and testing phase to cover all the functionalities for hive scripts. technology tableau , hadoop ( hive , impala , beeline ) assignment 2 : ewat and pace reporting duration from jan 2015 to feb 2016 project abstract : project is related to provide the bi reporting solution for the colleagues working on wat tool ( work allocation tool ) used for operational teams to get the productivity and efficiency of colleagues the major task involves to gather business requirements , create complete solution in hadoop and to provide tableau reports for leadership and inline managers. roles responsibilities as a team member following roles were performed by me : involved in connecting with business teams to gather requirements created scripts to pull the data from different kind of data sources ( wat , leave and attendance , swipe , shift data ) etc. created scripts to process the raw data using pig from different source systems. created warehouse , setup flow of data using hadoop components sqoop , hive , pig , impala etc. created tableau reports , dashboards using tableau desktop for visualization and analysis purpose. setup users and security on tableau server as per requirements. technology big data ( sqoop , hive , pig , impala , beeline ) , tableau as reporting tool assignment 3 : migration from informix to hadoop duration from nov 2013 to dec 2014 project abstract : project is related to move the complete setup that exists on informix to hadoop , the major task was to create complete etl solutions using big data with in limited time frame. roles responsibilities as a team member following roles were performed by me : created pig scripts to process the data in the same way it is done in informix created warehouse , setup flow of data using hadoop components pig , hive , beeline , impala etc. involved in development , fixing testing defects and deployment of scripts to production. technology big data ( sqoop , hive , pig , impala , beeline ) , tableau as reporting tool assignment 4 : ysa ( your spending account ) duration from july 2013 to dec 2013 project abstract : aon hewitt provides and end - to - end administrative solution for flexible spending and other tax favored plans by your spending account ( ysa ) the major task was to create new reports for analysis purpose for clients which are directly tied with aon hewitt and non hewitt clients. reports needs to be created individually for each client users along with cross client reports for enterprise. roles responsibilities as a team member following roles were performed by me : gathered the requirement of the project via discussions with business analysts and source team. created the complete fm model for reports. analysis of requirements and creation of new reports along with team. applied column , row level security in reports. created a wrapper on cognos connection using cognos portlets. performed the unit testing of the code with the team. technology cognos report studio 10.1.1 , cognos framework manager 10.1.1 , db2. assignment 5 : performance improvement of reports , cognos upgrade from 8.2 to 8.4 and then to 10.1 and designing a new user interface for reports duration from may 2011 to jul 13 project abstract : this project is an enhanced version of the existing psp , prior to this project the reports were in cognos 8.4 version and there was a need to improve performance , upgrade the reports to the new cognos10.1.1 version , and also enhance the user experience by upgrading the user interface. the major task was to improve the performance of reports , make changes in the design of the reports so as to easy the navigation process roles responsibilities as a team member following roles were performed by me : involved in complete deployment process for the migration of reports for 300+ clients to 8.4 and then to 10.1 involved in ldap migration to edirectory implemented changes in model and reports required for performance improvement. implementation of idaa ( ibm db2 analysis and accelerator ) along with db2 to improve performance of reports the only team member involved in initial level analysis of all cognos 10 new features and tools like bw , bwa , active reports etc. design of the new user interface structure both for reports and cognos portlets involved in customization of cognos interface by changing cognos configuration files. get the project rolled out to client with zero open defect. technology cognos report studio 8.4 and 10.1.1 , cognos framework manager 10.1.1 , db2 , idaa assignment 6 : plan sponsor portal ( reporting development and client customization ) duration from feb 2010 to apr 2011 project abstract : this project is an enhanced version of the existing pss and aims at reducing the data latency from 1 month to near time with a new reporting experience on cognos series 8. with the set of standard reports users will also have a utility for creating their own reports. the project includes developing around 25 reports in deferent services viz. common , health and measures , direct benefits and direct savings using cognos 8.2 as reporting tool and db2 as database roles responsibilities as a developer team member following roles are defined : developed reports in cognos 8.2 report studio as per business requirements poc on existing data model for performance enhancements developed design specification document for reports as per client requirements. verified the correctness of report data with the data from the backend. upgraded reports from cognos 8.2 to cognos 8.4. involved in testing and peer reviews. technology cognos report studio 8.2 , db2. skillsfunctional : 7 years of exhaustive experience in bi tools like cognos , big data , tableau command on data warehousing and business intelligence concepts. developed business reports in report studio , created metadata model using framework manager. designed and developed multiple dashboards using tableau. upgraded reports from cognos 8.2 to cognos 8.4 and 10.1 worked on tools like idaa ( ibm db2 analytics accelerator ) , google analytics. working as bigdata developer created scripts using sqoop , pig , hive along with other big data tools a strong contributor towards knowledge management activities including project documentation , user manuals and other deployment activities. technical : reporting tools : cognos report studio , query studio , framework manager , tableau etl tool : hadoop ( sqoop , pig , beeline , hive , impala ) languages : sql tools and utilities advanced query tool , ms office , hpqc rdbms : db2 , idaa : hr domain educationbachelors in technology - computer science and engineering in 2009 n.c. college of engineering and technology , kurukshetra university with 81 % higher secondary certificate in 2005 s.d. vidya mandir school , panipat ( cbse board ) with 83 % senior secondary certificate in 2003 s.d. vidya mandir school , panipat ( cbse board ) with 83 %',\n",
       " 'experience1.6 years. date of availability : 2 months. target locations : any. oct 2006 apr 2009 niit technologies ltd. kolkata july 2009 - feb 2015 siemens information system ltd. kolkata feb 2015 till now cerner india health services ltd. kolkata software skillsets : bigdata utilities : hive , pig , sqoop , zookeeper bigdata scheduler : oozie distributed file system : apache hdfs distributed programming : apache map reduce nosql database : apache hbase languages : core java , j2ee , pl sql web technologies : jsp , servlet , jdbc framework : struts 1.1 orm : hibernate rdbms dbms : oracle9i operating system : dos , windows 2000 xp , unix version control tool : rational clearcse6.0 , vss application server : was6.0 , tomcat6.0 current role : senior systems engineer soarian is a product which is more than revenue cycle management software. it handles receivables as well as claims processing. healthcare industries use this software to manage their financial processes which is not only manages transactions and data , but also automated most of the processes used in healthcare. more than just a traditional healthcare information system , soarian is designed with a workflow orientation that helps you provide optimum care while supporting cooperation across the healthcare enterprise. currently our team executed one poc to generate one dashboard using hadoop eco system components and got huge performance improvement. some portion of the applications were migrated to hadoop and improving the entire application life cycle performance by more than 50 % . the applications were implemented in 4 layers : - data ingestion : - were implemented mainly using sqoop for oracle db sources. for non - db sources ftp script were used. data integration : - mostly implemented using pig script for consolidating data from various different sources. data analysis : - implemented using hive queries for implementing the complex sql query on top of consolidated data prepared in integration layer data visualization layer : - implemented in tableau. used the final hive tables as source for few data cases where data were stored in hbase tables as well as exposing them hive for implementing the scd scenarios. responsibility : implementing the module for online sales data sources writing sqoop jobs for transferring data from oracle database to hdfs writing pig and hive script for data integration and analysis writing java code for writing udf in hive performance tuning hive and pig script designing and implementing the logic for scd scenarios technology : hive , pig , sqoop , hbase , core java , oracle company name : cerner india health services pvt ltd project title : soarian financials , us duration : 01 03 2015 27 11 2015 role : sr. developer. team size : 7 the production application server for all customer hospitals generates huge amount of logs daily for different purposes and those are kept into sql database as clob data type. a triplet job is running on each environment to do some reimbursement and receivable calculation and storing logs into database throughout the day. we needed some process to analyze those logs and to create category of each errors and figure out the occurrences of each type. using big data ecosystem we store the relevant useful information in hive data warehouse and process them. near real time log processing is achieved using spark streaming which buffers the logs and stores the logs in hbase. hbase is used for fast query processing of the records stored in hive data. responsibility : data ingestion : - were implemented mainly using sqoop for sql server db sources. sqoop job is being used to transfer data from sql server db to hdfs. data integration : - mostly implemented using pig script for consolidating data from various different sources. data analysis : - implemented using hive queries for implementing the complex sql query on top of consolidated data prepared in integration layer hbase is being used for faster retrieval of data for kind of real time processing using spark , we created some services to retrieve data from hbase for output. technology : core java , map reduce , hive , hbase , spark , sqoop company name : siemens information system limited project title : soarian financials , us duration : 20 07 09 - 30 01 2015. role : sr. developer. team size : 18 soarian is a product which is more than revenue cycle management software. it handles receivables as well as claims processing. healthcare industries use this software to manage their financial processes which is not only manages transactions and data , but also automated most of the processes used in healthcare. more than just a traditional healthcare information system , soarian is designed with a workflow orientation that helps you provide optimum care while supporting cooperation across the healthcare enterprise. more than a tool , soarian helps drive and direct workflow processes to reduce handoffs and promote standardization for more measurable and predictable outcomes , enabling you to focus on care delivery , rather than on administrative tasks. responsibility : application development , maintenance and enhancement. issue investigation , root cause analysis and providing the appropriate solution. technology : ibm websphere application server 6.0 , sql server 2008 , core java , jsp , servlet , bpot. company name : niit technologies project title : ruby client : generali verzekerings nv , netherlands duration : 16 11 07 - 18 03 09 role : junior developer team size : 20 ruby is a re - factoring project of a client - server based application called pheonix. its a universal life insurance management application. in ruby we have made it web based application to : make the universal life products more accessible to the brokers and end customers. move from the client - server architecture to a web - portal based architecture. reduce the time to market cycle for the it systems. meaning , it should be possible to change the it systems easily and make them available to the end - users in smaller time frames. responsibility : construction bug fixing technology : ibm websphere application server 6.0 , ibm db2 , ibm rational application developer , struts 1.1 as mvc framework , hibernate as orm. company name : niit technologies project title : sharing. client : ing group , netherlands. duration : 16 11 2006 - 23 10 2007 role : junior developer team size : 16 the bas project is an offshore development project for a leading banking sector in europe .. the bas project was formerly known as sharing. the new project name has been taken up , for developing the system for the ibn bank label. the project aims at offering business and application in a better , different and faster way. the basic objective of the project is to realize the application through component based development , adhering the schedule and quality. the system has 52 business and application components. for developing components , cbd 3.0 specifications are being followed. all the components are developed here using cool : gen with the help of vc++ compiler and ms - sql server as the database. after construction of all the components , the sharing factory will upload these components to the mainframe environment with cics , mvs and db2 etc. technology : cool : gen 6 , sql server 2000 , rational rose 7 , microsoft vss. others qualification : done a course on core and advance java from niit limited. projectcompany name : cerner india health services pvt ltd project title : soarian financials , us duration : 01 12 15 - till date. role : sr. developer. team size : 5 a_ahave participated in junior level state karate championship and got the second position in 1997. have played in district level inter school cricket tournament ( coca - cola cup ) . communication and presentation skills : good communication skills in english , hindi and bengali. capable of upgrading technological knowledge as and when required. educationexamination year of passing percentage b.tech ( comp. sc. ) 2006 73.00 %',\n",
       " 'experiencetotal 6+ years of experience in data warehousing in teradata , sap bo , oracle , inetsoft , style studio and microstrategy. proficient in troubleshooting and problem resolution , root cause analysis , bottlenecks elimination. worked on various methodologies like waterfall model , continuous delivery and agile methodology. able to work under pressure in a multi - task environment , manage continuous change , all with a focus on providing excellent client service. i have built an experienced knowledge base with major rdbms including teradata and some understanding on oracle , vertica , exadata. expertise on testing and developing on sap bo , style studio and microstrategy reports. good hands on experience in optimizing the reports , queries building data model based on the user requirements including both oltp and olap models. possess excellent analytical abilities and technical skills and ability to learn new technologies. energetic , enthusiastic and hardworking , result oriented team player inclined towards achieving career goals and enterprise objectives. altisource business solutions pvt ltd july 16 to till date senior software engineer societe generale august 15 to may 16 senior software engineer ibm india pvt. ltd. sept 14 to august 15 application developer infosys technologies limited march 11 to sept 14 senior system engineer projectperiod : july 2016 - till date company and project description : altisource business solutions pvt ltd , incorporated on november 22 , 1999 , is a marketplace and transaction solutions provider for the real estate , mortgage and consumer debt industries. the company operates through three segments : mortgage services , financial services and technology services trelix is a product which basically deals with the origination of the loan , it includes all the phase of loan including from documentation to loan origination phase. roles and responsibilities : working on a product which is developed for the origination of the loan service. analyzing the user requirement and creating data layers and programming for style studio and microstrategy. working with the business to analyze the requirement and build a robust data model which fits requirement and can solve there all possible queries , this includes both oltp and olap design. designing and plan bi solutions , creating and deploying reports writing relational and multidimensional database queries responsible for business intelligence solution architecture in defining , developing , and enhancing custom dw , bi systems. collaborating with different teams to get the proper picture of the requirement and doing the impact analysis. work on go live activities and resolving the post production issue. worked with all levels of development from analysis through implementation and support. interact with report owners to establish clarify their requirements and develop report specifications. resolve end user reporting problems through collaboration with it and operations. company : societe generale global solution centre project : trade positioning service ( tps ) period : september 2015 - may 2016 onshore : taiwan societe generale taipei , taiwan company and project description : societe generale is a french multinationalbankingandfinancial servicescompany headquartered in paris. the company is a universal bankand has divisions supporting french networks , global transaction banking , international retail banking , financial services , corporate and investment banking , private banking , asset management and securities services tps , basically is a repository where all the data from different sources are accumulated and processed based on different client requirement. source here resembles to the data from the front office and back office. these data can be of any type like trade level data for various products and many more. it is a project where technologies like teradata , exadata , unix are used. roles and responsibilities : responsible for business intelligence solution architecture in defining , developing , and enhancing custom dw , bi systems. developing different data base structures based on consumers requirement. working in unix shell script for the automation of the scripts. collaborating with different teams to get the proper picture of the requirement and doing the impact analysis. analyzing the requirement and doing the data analysis for it. implementing continuous delivery methodology in the project. proactively working with the continuous delivery coach for the responsibility of scrum master. work on go live activities and resolving the post production issue. training the users for the product which we developed , working with them onshore ( taipei ) and solving the issues instantly. company : ibm india pvt ltd. project : business intelligence platform ( bip ) period : september 2014 - august 2015 client and project description : dbs bank is the largest bank in south east asia , it is situated at singapore. it has the market domination in consumer banking , treasury and markets , asset management , security brokerage , equity and debt fund raising in singapore and hong kong. roles and responsibilities : develop code and document artifacts including unit test plans and ensure that the output is as per the given specifications and slas. responsible for developing the stored procedure based on business requirements using teradata and sap bo. creation of complex universe and complex reports based on business requirements. coordination with downstream and ui teams for impact analysis. perform various test cases and applying them for resolving different issues. creation of reports from finance universe and testing for data quality and analysis of mismatches with original report. work on go live activities as per the implementation plan. in this project various performance tickets are also analyzed which are of high priority. this involved performance tuning at universe and report level. company : infosys technologies project : global reporting information database ( grid ) time : march 2011 - september 2014 client and project description : apple incorporation is an american multination corporation headquartered in cupertino , california that designs , develops and sells consumer electronics , computer software , online services and personal computers. in this project we were providing end to end bi solution from etl to reporting layer for the client dealing with the domain like sales , finance and logistics. the solutions were provided using the technologies like teradata , vertica , sap bo , unix , python. roles and responsibilities : responsible for business intelligence solution architecture in defining , developing , and enhancing custom dw , bi systems. responsible for the development of the entire enterprise data warehouse and business intelligence application using teradata and sap bo design etl process involving data quality , testing and information delivery and access to the data warehouse. proactively involved for creating unix scripts in dev , test and prod environment. performance tuning of long - running reports to cut execution time , ease database load and improve scalability. creation of complex universe in sap bo based on business requirement. creation of sap bo reports and testing data quality and analysis of mismatches. responsible for the unit testing of the entire code both in teradata and sap bo. responsible for the integration testing of the entire flow after the fusion of both teradata and sap bo. reporting and resolution of any mismatch. work on go live activities as per the implementation plan. a_afelicitated by lions club of nagaon , assam in 2003. awarded as the best team for the project pos in fourth quarter of 2011 in infosys. awarded by the client apple inc. for the best performer in 2013 in infosys. received various appreciations from apple client onsite coordinator for deploying defect free code. educationbachelor of technology in applied electronics and instrumentation engineering from west bengal university of technology during the year 2006 - 2010. tools technologies : primary database skills - teradata 12 13 , oracle secondary database skills - vertica , exadata , bigdata. scripting - unix shell scripting , autosys. languages - sql , plsql , c. tools and utilities - sap bo , style studio , inetsoft , microstrategy. domain - consumer electronics , supply chain , retail , investment and mortgage banking. methodologies - waterfall model , continuous delivery , agile methodology. project description : company : altisource business solutions pvt ltd',\n",
       " 'project1. job profile - big data solutions : analyzing the data in hortonworks environment , providing the best big data solution. develop mapreduce , yarn code for memory intensive data processing. time analysis on the queries. prepare the sample code in hadoop environment in pig. knowledge in developing udfs for pig using java. importing the data from the mysql and oracle into the hdfs using sqoop. involved in creating hive tables , loading and analyzing data using hive queries. written map reduce java programs to analyze data for large - scale data sets. responsible for managing data from multiple sources. designed and built many applications to deal with vast amounts of data flowing through multiple hadoop clusters , using java - based map - reduce. qa , code review and testing of big data applications. issue resolving in map reduce , pig. 2. job profile - application packaging : working as an application packaging engineer in build and release of applications. upgrading packages already deployed to production. from downloading source to packaging and deployment of applications. using wix customized tool to make msi , apply mst on vendor msis , using install shield for packages where wix is not feasible , testing , writing vb scripts for complex applications , troubleshooting applications with issues. applications packaging sr. associates dell international services , pune : jan 2011 to june 2013 roles and responsibilities : acr of applications. packaging using wix tool verification and qa of packages testing the compatibility of software in different environments so that the client can migrate to different environment. deploying applications using transforms ( mst ) and installer silent switches. packaging the application using different tools like wise packaging studio , orca , and install shield using msi technology. packaging the applications using application virtualization ( appv ) . msi conflict resolution and ice validation using orca. delivered wrapper msi using silent switches. mentoring the new trainees. deployment in test environment using sccm. symantec workspace virtualization , understanding the tool , giving training and assisting others in using the tool. documenting driver packaging and other issues. skillsresult oriented and able to work independently as well as in teams. hard worker with a flexible approach. keen learner with constant zest to acquire new skills. team player with strong analytical and leadership skills. acquired practical knowledge through various academic projects. education1. hortonworks big data certified( hdp certified java developer ( hdpcd : java ) ) : verification link : http : bcert.me shyqwwvf 2. itil foundation certificate in it service management. experience - jan2011 - till now : build and release sr. engineer fis , pune : oct 2015 to till now 1. job profile : build and release of applications. working on build and release of products. maintaining infrastructure , continuous integration. sr. system executive cognizant technology solutions , pune : aug 2013 to oct 2015 be computer engineering ( first class ) from bharati vidyapeeth college of engineering pune ( 2006 - 10 ) . hsc ( 91 % ) from riverdale high school , dehradun ( isc board ) in year 2006. ssc( 84 % ) from riverdale high school , dehradun ( icse board ) in year 2004',\n",
       " \"summary2010 present computer science corporation senior software engineer mar 2008 oct 2010 hcl technologies , chennai senior software engineer mar 2005 mar 2008 xtreme infotech software solutions software engineer projectproject name : wholesale analytics and reporting system project details : ford europe may 2016 till date technology : apache spark , hive , sqoop , python , scala , jsons , avro , hue this project is mainly for the re - platforming of the current wsr existing system to hdfs using spark which can able to process large date sets ( i.e. terabytes and petabytes of data ) to meet the client requirements with the increasing data volume from other departments. the data will be stored in hadoop file system and processed using map reduce jobs. this job includes get the jsons data through ftp ' s and services from upstream systems , process that data to obtain information with respect to their domains ( nscs and fcsd ) , extract various reports out of the processed information and export the information for further processing. educationmaster of computer application from madras university. diploma in business administration in icfai university. certified big data and hadoop developer - simplilearn implemented big data projects over 3 months across 3.5 billion data points. certified apache spark and scala professional - simplilearn implemented apache spark and scala projects over 3 months. sun certified java programmer ( java 2 platform , standard edition 5.0 ) . sun certified web component developer for the java 2 platform , enterprise edition 1.4. oracle certified associate of oracle pl sql developer certified associate.\",\n",
       " \"summaryhaving 6+ years of it experience in software development with expertise in client - server and web based applications in depth and extensive knowledge of hadoop architecture and various components. passionate about hadoop and big data technology. 3 + years hadoop experience. have experience managed the entire training functions for the corporate sectors including content development , and material preparation. trained by more than 500+professionals. familiar with components of hadoop ecosystem : hdfs , hive , sqoop , pig. proficient in java. sound linux background. expertise in hadoop application development. proficiency in using mapreduce to develop hadoop applications and jobs. developed applications for distributed environment using hadoop , mapreduce and java. excellent communication and interpersonal skills and outstanding team player with an aptitude to learn. energetic self - starter with excellent analytical and organizational skills. achieves goals , objectives and in an accurate and consistent manner. experienceworking as a technology analyst in infosys from 2014 - april to till date. working as a software engineer in s s p infotech pvt ltd from 2010 - june to 2014 - march. projectclient : bank of america mar 2015 to till date data warehouse platform application development project for a leading bank in the united states of america. the scope of this project was to find the trade which deviate from the standard of foreign exchange and check the e - communication data for trade breach conversation. actimize team will generate the alert , from there we need to extract trade id and do the cross reference data to find out the trader information. responsibilities : developed simple and complex mapreduce programs in java for data analysis on different data formats developed mapreduce programs that filter bad and un - necessary claim records and find out unique records based on account type processed semi , unstructured data using map reduce programs implemented daily cron jobs that automate parallel tasks of loading the data into hdfs and pre - processing with pig using oozie co - ordinator jobs implemented custom datatypes , inputformat , recordreader , outputformat , recordwriter for mapreduce computations worked on cdh4 cluster on centos. successfully migrated legacy application to big data application using hive pig hbase in production level transformed date related data into application compatible format by developing apache pig udfs developedmapreducepipeline for feature extractionand tested the modules using mrunit optimized mapreduce jobs to use hdfs efficiently by using various compression mechanisms creating hive tables , loading with data and writing hive queries which will run internally in mapreduceway responsible for performing extensive data validation using hive implemented partitioning , dynamic partitions and bucketing in hive for efficient data access worked on different set of tables like external tables and managed tables used oozie workflow engine to run multiple hive and pig jobs involved in installing and configuring hive , pig , sqoop , flume and oozie on the hadoop cluster. involved in designing and developing non - trivial etl processes within hadoop using tools likepig , sqoop , flume , and oozie. working with apache crunch library to write , test and run hadoop mapreduce pipeline jobs involved in joining and data aggregation using apache crunch worked with sqoop to export analyzed data from hdfs environment into rdbms for report generation and visualization purpose environment : apache hadoop , hdfs , mapreduce , apache crunch , java ( jdk1.6 ) , mysql , db visualizer , linux , sqoop , apache hive , apache pig project 2 : customer 360degree of suntrust april 2014 to mar 2015 description : suntrust banks , inc. , is an american bank holding company. the largest subsidiary is suntrust bank. it had us$ 175 billion in assets. the bank ' s primary businesses include deposit , credit , trust and investment services. through its various subsidiaries , the company provides mortgage banking , asset management , securities brokerage , and capital market services. i worked as a hadoop developer in data insights team where i performed analysis on huge data sets and helped the organization get a competitive advantage by finding out the customer trends which helped in ad targeting and network optimization. in ad targeting , we collected the data from all the customers and our team performed the analysis by using various traits such as demographics , purchase history etc. and did the advertising based on the results from the analysis. in network optimization , we collected the data from all the network towers and conducted analysis to find out the peak hour usage of each tower and strategic locations to put new towers to increase customer satisfaction. responsiblities : installed and configured hadoop , mapreduce , and hdfs. developed multiple mapreduce jobs using java api for data cleaning and preprocessing. importing and exporting data into hdfs and hive from a oracle 11g database using sqoop responsible to manage data coming from different sources monitoring the running mapreduce programs on the cluster. responsible for loading data from unix file systems into hdfs. installed and configured hive , sqoop. involved in creating hive tables , loading with data and writing hive queries which will invoke and run mapreduce jobs in the backend. installed and configured pig. wrote pig scripts to process unstructured data and create structure data for use with hive. developed scripts and automated data management from end to end and sync up b w all the clusters. environments : apache hadoop , mapreduce , hdfs , java ( jdk1.6 ) , oracle 11g 10g , mysql , windows , unix , sqoop , hive , pig. project 3 : anz - customer insight and retail analytics feb 2013 - march 2014 description : the australian division caters for the bank ' s retail , commercial and wealth management customers in australia. the retail businesses are responsible for delivering a range of banking products and services to retail customers , while commercial services small to medium enterprises through to smaller corporate. the division also has a dedicated merchant analytics management business designed to meet the needs of high net worth individuals. this solution is concerned with the development of a cost - effective data warehouse using hadoop and hive for storage of large amount of historical data and log data. the raw data will be coming from various sources and dumped directly into hadoop file system through sqoop ( data extracting tool used to extract data from rdbms( oracle , db2 , teradata , etc ) ) . then , data is processed ( like un - normalization , partitioning , bucketing , etc. ) using hive queries. after that , the data is updated ( using customized and optimized queries ) into hive and ad - hoc queries can be run to get any form of data. responsibilities : - support all business areas of anz with critical data analysis that helps team members make profitable decisions as a forecast expert and business analyst and utilize tools for business optimization and analytics involved in infrastructure verification( file system , os compatibility check , java version verification ) analyzing the requirement to setup a cluster created two different users ( hduser for performing hdfs operations and map red user for performing map reduce operations only ) setting - up the hadoop - cluster on 10 vms ensured nfs is configured for name node setting password less hadoop hadoop installation verification( terrasort benchmark test , dfsio benchmark test ) setting up cron job to delete hadoop logs local old job files cluster temp files deployed ganglia on all nodes setup hive with mysql as a remote metastore moved all log files generated by various network devices into hdfs location written map reduce code that will take input as log files and parse the logs and structure them in tabular format to facilitate effective querying on the log data created external hive table on top of parsed data july 2012 feb2013 big data hadoop developer description : project to predict buying habits of customers and display ads on an websites. this was a proof of concept developed to determine if buying habits can be extracted without adding any additional monitoring scripts and with just based on ad interaction and social interaction of the consumer. the data was all gathered from social networks twitter , facebook to observe how vocal consumers are about their purchases. responsibilities : designed docs and specs for the near real time data analytics using hadoop. developed ad - clicks based data analytics , for keyword analysis and insights. crawled public posts from facebook and tweets. wrote map reduce jobs with the data science team to analyze this data. converted output to structured data and imported to rdbms with analytics team. defined problems to look for right data and analyze results to make room for new project. tuning the server for optimal performance of the cluster environment : cloudera , hadoop , mapreduce , hdfs , hive , pig , sqoop , java , red hat linux , xml , mysql , eclipse , junit. : gold plus : complete jewellery management software duration : april , 2011 june , 2012 team size : 50 role : software engineer a team of 5 resources environnent : java 5.0 , java enterprise edition ( j2ee ) , java server pages ( jsp ) , struts - 1 , mysql , description : gold plus is windows based program which will fully manage every aspect of your jewelers business. in gold plus point - of - sale , billing , inventory and bar - coding is 100 % integrated with the accounting system and also customer and vendor ( karigar artisan ) management. all the modules in gold plus are integrated with financial accounting system. on line effect of each and every transaction can be viewed right up to balance sheet. gold plus offers all statutorily required financial reports including reports for vat , sales purchase artisan approval registers , income and expense statement and ledger accounts. responsibilities : involved in gathering the requirements , designing , development and testing lead the team of catalog management module worked on device management module create high level and low level technical design documents involved in setting up all the environments completely involved in creating kits creation , build and deployments in all environments skillsprogramming languages : c# , java , unix ide : visual studio , eclipse big data : hadoop , map reducing , hive , pig , hbase , sqoop , node clustering , distributed computing. web technologies : asp.net , wcf , web services , servlets , html. web application server : iis , tomcat source control : clear case , subversion , tfs operating systems : windows xp vista 7 , ubuntu educationbachelor of computer science engineering from jntu h\",\n",
       " \"experiencea competent software professional with 7 years and 4 months of experience in the areas of big data hadoop , software and application development , team management and client servicing in the it sector. recently associated with aig data services pvt ltd. as a big data developer. certified in cloudera hadoop developer ( ccd - 410 ) . good knowledge in hadoop ecosystem , hdfs , map reduce hive , pig , hbase , hue , oozie , apache kafka , sqoop , zookeeper. good expertise in unix shell scripting. good experience in java web services. writing sqoop script for data ingestion into hdfs hive. thorough understanding and exposure in all phases of sdlc along with skills in mapping client requirements. hands on experience in working with ecosystems like hive , pig , sqoop , map reduce jobs. possesses exposure in reviewing and mapping the functional business requirement documents and developing new applications. a team player with strong analytical , technical , negotiation and client relationship management skills. delivering and implementing the project as per the scheduled milestones. participating in system integration onsite deployment , customer support and product up gradation. interacting with onshore team for requirements gathering , analysis , implementation and testing of the system. structural designing and coding of solution for developing new applications. designed different test plans using test management tools. designing coding , validating forms , coding programs and finally integrating them. designing help manual for different e flows and scenarios. company name position held duration mastek limited software engineer jul , 2008 to nov , 2010 tesco hsc software engineer sep , 2011 to sep , 2013 tech mahindra tech lead nov , 2013 to dec , 2014 aig data services big data developer feb , 2015 to till date application performance availability dashboard( apad ) is a dasbaord designed for checking the availability and performance of applications by the cto , which are running and being used by users across the organisation in various global locations. data related to different applications are stored from different data sources are ingested into hdfs cluster and on connecting with qlikview , the performance and vailability will be displayed in visualisation dashbaord. . roles and responsibilities : - writing sqoop script to load data from rdbms to hive. writing hive queries to load the data into hive tables. writing the unix shell scripting to automate the process of file conversion and file loading into hive. testing the application. global personal insurance operations ( gpio ) worked over the past year and a half to establish an an investment strategy framework for the consumer insurance organization that would enable senior leadership to effectively and efficiently prioritize and make decisions on project investments globally. gpio led assessments , which identified gaps in operations and it capabilities at business segment level in priority business segments. the consumer organization will then define appropriate operations and technology solution sets to resolve these gaps to ensure that the consumer organization can achieve its business objectives. the team , which defined the proposed solution sets , will estimate for both , costs and associated benefits , to feed into the annual budgeting process for approval. . roles and responsibilities : - writing java code for converting the excel files into .csv files. writing hive queries to load the data into hive tables. writing the unix shell scripting to automate the process of file conversion and file loading into hive. writing the file validation codes to check the data and validate the data in input files. writing the jsp web app code to load the data using the web application. testing the application. since nov 2013 tech mahindra tech lead data management system is a rest web service based mobile web application where in a web server the mobile web application is to be deployed and there for downloading various types of documents like pdf , mp3 , mp4 , audio , video , ppt , doc , png , xml files. . roles and responsibilities : - creating restful web service using jersey. coordinating with ui team in designing front end of the mobile application. designing and testing the web application. developing the server side part of the restful web application. satisfying german privacy and data protection legislation poses additional challenges on anonymisation of subscriber ' s data. in particular it needs to be ensured that the data is not only protected from third parties , but also from illicit access and storage within telefonica germany itself. this was the challenging project which deals with secured data anonymisation process and multiple service components like secure store , map ( multilevel anonymisation process ) , alip ( anonymisation long term indexing process ) , event data filtering , short term indexer , long term indexer static crm data filtering. process monitoring , data pooling and scheduling. this project has been developed to process crm and telecom event data for the analysis in secure in very manner with physically storing in non volatile memory. this is all about dealing with real time data and providing customer to analysis and use for business enhancements. . roles and responsibilities : - java map reduce programs and their unit testing using mr unit. used to write hive and pig queries on different data sets. creating restful web service using jax - rs. writing shell scripting for automation. scheduling hive , pig , map reduce jobs using oozie. since sep 2011 tesco hindustan services centre software engineer deals with more than 1 tb of data and a 40 node clusters. we use to analyse historical sales information and generate reports on monthly basis and compare sales between months , compare sales of different product categories , analyse growth and decline of sales , generating weekly , monthly and yearly report for the same. doing forecast for future sales on also comparing the target achieved or not for weekly , and yearly basis. when selling products , in order to prevent products stock out due to the uncertainty events , enterprises usually reserves a certain amount of safety stock. the amount of safety stock is directly related to inventory costs and no. of sales. determining the amount of future safety stock as per the previous sales report. roles and responsibilities : scheduling hive , pig , map reduce jobs using oozie. java map reduce programs and their unit testing using mr unit. used to write hive and pig queries and udfs on different data sets and joining them. used to sqoop for data transfer between ms - sql 2005 ( rdbms ) and hdfs. writing shell scripting for automation. used cloudera impala for ad hoc query testing. writing map reduce job to read data from hbase. jul 2008 to nov 2010 mastek ltd. software engineer core elixir is an insurance flagship product of which provides end - to - end solution to the customer. its a product meant for insurance industry to manage lifecycle of a policy and creating and maintaining the accounts of the customers associated with the policy. the maintenance of the policy involves creating the policy for the customer , premium deposit and withdrawal of the amount. roles and responsibilities : performed prescribed , process and sla monitoring. identified recurring problems and escalation for permanent fix. resolved the unix related issues ; ensured sanity checking. reported technical issues to the l3 development team. projectapplication performance availability dashboard software environment hadoop , hdfs , hive , unix shell scripting , linux , java , qlikview ui environment tools hive( to load the data from csv files ) , language used python job scheduling tool : autosys etl tool apache sqoop hadoop framework hortonworks 1.3 database used : sybase , ms - sql server data warehouse tool : hive data visualization : qlikview team size : 16 project type : development project period : may 2015 till date capability assessment and investment strategy planning visualization dms mobile web application tap - anonymization( tefg ) tesco sales analytics software environment hadoop , hdfs , map reduce , crunch , hive , pig , hbase , oozie , sqoop , impala , hue shell scripting , unix , ms - sql server 2005. environment tools java map reduce ( parse test files and do calculations ) , sqoop , hive and pig queries on imported files hadoop framework cdh team size 25 project type development project period sep 2011 - sep 2013 account management client idbi bank software environment shell scripting , sun solaris and oracle 9i ( sql ) team size 20 project type support and maintenance period dec 2009 to nov 2010 brief synopsis : the purpose of this module is for creating maintaining saving accounts. this banking software provides a complete solution for the end user account holder , it consists of different type of account : - current account , savings bank ( sb ) account , recurring deposit ( rd ) , fixed deposit etc. this module has two - user level , a customer and an authorized bank. this software been web enabled , the account holder can access his account information and personal details and use the net banking facility and fast cash through multi geographical location roles and responsibilities : proactively and independently worked on resolving issues before they impact the client. monitored the running jobs and reviewed log files if any issues ; ensured completion of jobs on prescheduled time. created the rfc and supported code fix in case of code fix. performed the daily health check of the application on production environment. worked on ways to minimize the no. of alarms and improved the mttr. resolved unix related issues ; executed the scheduled jobs and routine jobs. provided l1 l2 level support depending upon the priority. capita insurance client capita software environment unix ( sun solaris ) , oracle 9i ( sql ) , ms - sql server 2005 team size 25 project type support and maintenance period oct 2008 to nov 2009 skillsapplication development production support and maintenance sdlc planning implementation and support client relationship management team management requirement gathering documentation reporting project co ordination hadoop bigdata : hadoop ecosystem : hadoop , hdfs , mapreduce , hive , pig , hbase , sqoop , oozie , apache kafka certified cloudera developer ( ccd - 410 ) . language and framework : map reduce , java software environment : hadoop , hdfs , map reduce , hive , pig , oozie , kafka , apache cxf , jersey operating systems : windows xp , unix ( sun solaris , aix ) , linux ( centos ) databases : ms - sql server 2005 , oracle 9i , sybase scripting : unix shell script , web service : jax - rs , restful web service , jersey , data warehouse script hive , cloudera impala , pig etl tool apache sqoop , apache flume job scheduling tool : oozie unit testing framework. mrunit , junit ui : hue , eclipse , spring tool suite 3.4 build tool : maven issue tracking tool : jira distributed coordination tool : zookeeper organisational scan since october 2015 aig data services big data developer insurance software environment hadoop , hdfs , hive , unix shell scripting , linux , java , qlikview ui eclipse , jsp environment tools java ( to convert excel input files into csv files ) , hive( to load the data from csv files ) , hbase( to load data from csv files ) . hadoop framework hortonworks 1.3 database used : hbase data warehouse tool : hive data visualization : qlikview team size : 16 project type : development project period : march 2015 sep 2015 telecom software environment java , jersey , rest web service ui eclipse , spring tool suite 3.4 build tool maven issue tracking tool jira team size 2 project type development project period nov 2013 - dec 2014 telecom software environment hadoop , hdfs , map reduce , hive , pig , oozie , apache kafka shell scripting , linux , java , jax - rs , apache cxf , web service ui hue , eclipse , spring tool suite 3.4 hadoop framework hortonworks 1.3. build tool maven issue tracking tool jira environment tools java map reduce ( parse csv files ) , sqoop , hive( and do calculations ) and pig queries( to do joins ) on imported files. team size 25 project type development project period nov 2013 - dec 2014 educationcloudera certied hadoop developer( ccd - 410 ) apache hadoop cdh4. b.tech. ( computer science ) bput , orissa 2008 7.84cgpa 12th c.h.s.e. bjb college ( orissa ) 2002 66 % 10th b.s.e. b.n.b.p. ( orissa ) 2000 90.5 beyond curriculum joined ncc , and red cross in the school level. attended trekking expedition in nilgiri mountains in ooty.\",\n",
       " \"summaryas a hadoop developer : 2 years experience in big data analytics hadoop echo systems. good experience in understand hadoop core concepts like hdfs and map reduce. expertise in writing pig latin queries to process data. experienced in performance optimization of hive queries of enterprise data warehouse system. good experience in data ingestion tools like flume and sqoop to transform the data from different sources to hdfs practical exposure on importing and exporting data from traditional data ware house to hadoop ecosystem with sqoop. knowledge on oozie in scheduling jobs in hadoop echosystem. good knowledge on nosql database with hbase. having good knowledge in data extraction , transformation and data loading in data warehouse environment. having good awareness on business analysis where can be find the big insights to give solutions for business problems. as a .net resource : experience developing web applications with c# and asp.net. good working experience on wcf services to communicate distributed systems good experience on sql server writing queries and tsql. rich exposure to system development life cycle ( sdlc ) . experience on developing application in mvc having strong experience debugging and unit testing applications effective team player with good communication and analytical skills. experienceworking as sr software engineer in techmahindra march 2014 to till date. working as sr software engineer for artech infosystems from feb 2012 to march 2014. worked as software engineer for magna infotech from nov - 2010 to nov 2011. project skillshadoop stack : mapreduce , hdfs , hive , pig , sqoop , flume , oozie and hbase. big data tools : hortonworks , cloudera , languages : c# , java web technologies : asp.net , ado.net , javascript , xml , wcf , ajax , database : sql server 2005 2008 , oracle. ide : visual studio , eclipse. project title : rapid 360. duration : jan 2015 to till date. environment : hadoop , pig latin , hive , map reduce , oozie. data ingestion tools : sqoop , flume. team size : 4 description : the recommendation engine will be a solution , based on the past and current records of the customers based bunch of data collected , and considering several parameters the approach needs to be designed to create advanced next product to buy algorithms integrate long term behavior with most recent data to make smarter offers an approach to target customers based on several parameters solution can be used for varied purpose. it can be used in e - commerce to showcase the target customer specific product depending on his liking and spending habits. as well as considering various other parameters such as taking his max purchase amount , average or so. can be used in the insurance domain , for selling specific products according to customer ' s spending capacity. responsibilities : worked with hadoop ecosystem ( hive , pig ) for structured database .. writing business logics using pig latin to process log data according to rules. involved in create hive external table to store the processed data. configuring and using sqoop and flume for importing and exporting the data into the hdfs db. involved with customers to understand rules and suggested insights based on the data. researched , collected , edited content related to hadoop development and tools. shared knowledge with peers. project title : nat. duration : march 2014 to dec 2014 . environment : java , ibm websphere , oracle. team size : 6 client : nestle swiss. application description : application called it as nat( nestle audit tool ) the application use to perform the audits in all nestle markets across the globe.which is used by nestle internal auditors which manages complete audit flows tipically having four stages that are audit planning , audit scheduling , field execution publish aduits.four stages covers the complete audit nma and nga process. roles and responsibilities : handling production support and bug fixes acted as sme communicating and getting work done from vendor teams. handling application server issues ( websphere ) . cr promoting and deployment issues handling. project title : wfs ( work flow system ) . duration : march 2012 to till march 2014 . environment : c# .net , mvc , iis ( 6.0 ) , asp.net , java script , wcf , xml , sql server ( 2005 ) , agile process. team size : 6 client : vodafone , italy. application description : wfs is the work flow management system which handles requests from different external systems like ds , pegaso , oca , oag omni sales , and online users , for activation equipments , sim , link , pbx , vas etc , and which validates the data , through communicating by external systems and completes the flow based on the responses. pegaso ( wfc workflow corporate ) is a workflow management gui system used by the call centre operators. this application is mainly used to create and manage dsl connection requests for corporate customers. the main requests created through pegaso are requests for activation of sim cards , sost ( substitution e.g. , change of sim card ) and var ( variation e.g. customer wants to change the phone instrument mobile or fixed phone ) for corporate customers. pegaso is a web application used by call centre and accessed through vodafone intranet. roles and responsibilities : developing business logics. taking the owner ship particular modules end to end deliver. analyzing business requirements and design low level documentation. involving in unit testing and bug fixing. taking responsibility quality control document maintaining and reverse eng. project title : ariostos mobility. duration : 2010 nov to 2011 nov. environment : asp.net , ado.net c# , xml , sql server , oracle. team size : 6 client : at and t , usa. application description : aristos mobility provides enterprise on demand ( eod ) services to the customers eod is a flexible , boutique telecom wireless service offering designed around the needs of medium to large business customers. through eod , customers can order sims , modems and other wireless equipment , manage their own activations and service changes , as well as view and download invoices and usage information. roles and responsibilities : designing and development of ui with rich look. involving in development of low level design documentation. developing business logics. writing sql scripts. educationmaster of computer applications ( mca ) from jnt university( 2007 - 2010 ) , anantapur.\",\n",
       " 'summary6.10 years of total experience in telecom mediation and billing domain , performing technical roles across development and problem management , including 2 years at client ( british telecommunications ) location ( milton keynes , uk ) for mediation billing platform upgrade , migration and support. 1.5+ year of experience in hadoop bigdata mapreduce hive pig spark sqoop flume itil v3 foundation certified. cloudera hadoop and spark developer certified ( cca175 ) . license : 100 - 017 - 370 proficient in telecom domain. end to end business knowledge and flow of cdrs ( call detail record ) in products like pstn , interconnect , and broadband. hands on experience in mediation , rating billing and revenue assurance. team oriented individual with a result driven attitude , quick learner , strong interpersonal and communication skills in effective information exchange and interaction with clients at all levels of management. member of aqt automation group formed in tech mahindra to identify and implement automation in telecom billing and payments platform. experiencetelecom mediation products fusionworks 4.11 ( openet ) , oracle network mediation 5.1 and 6.0 ( oracle ) 6.5 years hadoop bigdata mapreduce , hive , sqoop , pig , flume , oozie , spark( scala ) , core java , avro tools 1.5year billing rating products aztec ( based on onm ) 2.5 years languages sql , pl sql , unix , html , dsd , shell scripting , awk scripting 6.5 years operating system windows , unix , linux ( red hat 4.1.2 ) 6.5 years tools used amdocs clarify , eclipse , edit plus , hpsm , toad 4 years database oracle 10g and 11g. 6.5 years protocols ftp , telnet , sftp , xfb 6.5 years tech mahindra limited , pune ( march - 2010 to till date ) determines operational feasibility by evaluating analysis , problem definition , requirements , solution development , and proposed solutions. develops software solutions by studying information needs , conferring with users , studying systems flow , data usage , and work processes , investigating problem areas and following the software development lifecycle. communicating with internal external clients to determine specific requirements and expectations , managing client expectations as an indicator of quality. projectproject 1. client : bt ( british telecommunications ) application setup and development revenue assurance period : from nov - 2015 to till date technologies : hadoop big data , hive , pig , sqoop , flume , oracle apex , map reduce description : hercules is part of bts strategic fraud management platform.the cdr usage data is landed on ur ( usage repository ) by various billing systems in bt. bt has numerous oss stacks , split on lob and market facing units and indeed sometimes different products sold within market units. re - use of ur cdr warehouse provides a valuable history of information available without having to replicate vast amounts of data in the hercules tool at a high cost. project will feed bad debt and fraud address contact information directly to hercules. roles and responsibilities : role : it consultant at milton keynes ( uk ) + pune ( india ) set up 4 node hahoop cluster by using apache hadoop 2.6 version and configuration hadoop tools apache pig , hive , sqoop , flume , spark , and data visualization tools oracle apex. worked with systems engineering team to plan and deploy new hadoop environments and expand existing hadoop clusters. implementing the queries in pig script for large volume of data sets. loading of huge data set using flume map reduce program. created the hive tables and loading data from hdfs. applying the hiveql on those table for data summarization and validation. importing exporting the large volume of data sets from rdbms to hdfs by using apache sqoop. report creation via oracle apex by extracting data from project 2. client : bt ( british telecommunications ) application migration and support onm and aztec ( oracle network mediation ) period : from apr - 2013 to oct 2015 technologies : onm , unix linux , shell scripts , plsql. description : oracle network mediation is the bt strategic mediation system. it provides a flexible and scalable architecture for the collection , aggregation , enhancement , and distribution of network data. aztec ( a to z telephony event charging ) uses oracle network mediation as a convergent mediation solution that transforms raw network data into rich data assets. designed for traditional and next - generation multi - service , ip and mobile networks , the solution provides comprehensive data collection , aggregation and enhancement for upstream business critical systems like billing , reporting and analytics across the enterprise. aztec is currently being used for interconnect or conveyance usage rating , which involves complex pricing logic that off the shelf billing products are not capable of managing. roles and responsibilities : role : it consultant at milton keynes ( uk ) played vital role in migration phase for onm application. migration was done from openet - fusionworks to oracles oracle network mediation at cloud infrastructure. major role in setting up billing journey for bts newly launched product bt sports and centrex ( broadsofts unified communication ) . development of various support related scripts and configuring the traps by developing shell scripts , logmon , using automated tools like hunter for better application monitoring. end to end proactive monitoring for the onm which is live on production. developed automated monitoring , so real time monitoring can be achieved using shell scripting , html and sql. project 3. client : bt ( british telecommunications ) billing and payments - fusion works period : from mar 2010 to mar 2013 technologies : unix , dsd scripts , oracle 10g , plsql , fusion work product description : fusion works is a mediation tool provided by openet. files are received from upstream switches which get parsed and processed at fusion works mediation end and then are routed to billing , revenue assurance , cfs downstream system. the code for this is written in dsd language. we have been working for 3 lobs of global , retail and wholesale for bt telecom services. roles and responsibilities : role : software engineer application development and support at pune ( india ) understanding the requirement from designers. development work involved pl sql query handling. used cursors , triggers , stored procedures , functions , schema and views. pursue necessary code review , unit testing guidelines set to enable high code quality. provided solutions for complicated issues across e2e environments , sit environments , and productions to ensure speedy resolution. identification and resolution of the technical problems in fusion works to resolve the queries from billing systems , reporting , revenue assurance system , fraud management systems. performing the tasks like service request , risk management , capacity management , incident management , problem management. skillstool a_areceived multiple appreciations from delivery manager and project manager for efforts put in the project and completing tasks in stiff deadlines. got cookies and valuable team player award in tech mahindra ltd. organized and participated in various team building activities. done diploma in embedded systems from hyderabad. extra curricular and hobbies travelling and cooking. member and organizer of technovision - 07 08 a national level symposium under ieee ( institute of electrical and electronics engineers ) . member of national service scheme in the year 2007 - 08 and volunteered in 05 - 06. educationbachelor of engineering ( electronics ) college : shri ramdeobaba kamla nehru engineering college , nagpur university : nagpur university , nagpur ( maharashtra ) aggregate : 63.5 % year of passing : 2008',\n",
       " 'experienceworking as sse in cgi bangalore with total it experience of 76 months. total it experience in big data is of 24 months as hadoop developer. proficient in apache hadoop ecosystem , mapreduce proficient on data integration tool pentaho kettle spoon good knowledge of core java , rdbms. designing and developing coding software components in bigdata technologies that includes various tools like pig latin , map reduce. ability to write and understand code based on technical designs in programming language ie core java , pig latin. good knowledge in writing sql queries ( ms sql server 2008 and 2012 ) . good handson experience of linux and unix like operating system. hands on experiance on sftp , batch scripting , shell scripting. good knowledge of bmc remedy tool ( itsm and itsp ) . hands - on experience of soap ui testing tool. good knowledge of bmc remedy ( itsm and itsp ) tool. specification and management of coding , usability and quality standards interpreting requirements into application code co ordinting with qa team members evaluating and learning new technologies that would provide business value quick learning skill and flexible. keen to learn new technologies. strong communication and presentaion skills , in english. 2 months rigorous training attended on data science technology ( big data ) within organization. topics covered : introduction of big data , pig latin , hdfs environment setup , map reduce , h base , mongo db , hive. projectproject title itsm coe client bell canada and multi clients description working as hadoop developer in a middleware project which provides common interface for multiple externalsystems to interact with ( itsm ) with huge volume of structured , semistructured and unstructured data. hadoop framework stores this data in hdfs file systems and processes it based on business needs. environment apache hadoop eco - system 1.2.1 on redhat enterprise linux server 6.0 pig latin - 0.12.0 duration march 2014 to till date. role responsibility hadoop knowledge and administration * actively participate in requirements gathering and analysis. * work with business analysts and client representatives. * writing and running map - reduce programs for data processing * writing and running pig scripts. * create , execute and debug sql scripts to perform data validation. * hadoop ecosystem health monitoring and optimization. * hdfs and map - reduce environment setup and configuration. * preparation and analysis of large data sets ( big data ) , both structured and unstructured. * consultation and coordination with participating business and it areas also fixing and unit testing tibco bw , db or java related bugs. contributions people module , inbound module , outbound module , error module , external system , mock service , alarm monitoring , hba adapter , batch scripting team size 4 in india and 10+ members across globe project title chorus client bell aliant description worked as a test engineer on customized bmc remedy tool based on client requirement. environment remedy duration september 2010 to feb 2014 role responsibility * create test plans. * prepare review test conditions , test cases and test scripts. * execute test cases scripts. * customer interaction. * create and maintain all testing artifacts. * log and track defects to closure. contributions as a team member , i was part in all possible testing activities and raise defects if found any. i was also a key member for client interaction in status update meeting team size 5 skillsdeveloper big data hadoop eco system data integration using etl pig latin programming core java rdbms map reduce operating systems red hat enterprise linux languages core java tools # pentaho v5.2.0.0 # bmc remedy 7.6 # ms sql server 2008 , 2012 # eclipse 3.6 # soapui2.5 # tibco active matrix 5.6 , 5.12 # svn # beyond compare # putty networking telecom educationb.e in computer science engineering 2009 from bvbcet hubli , affiliated to v.t.u , belgaum. 10+2 from cbse board , ranchi jharkhand. 10th from bihar state board , muzaffarpur bihar',\n",
       " 'projectproject : appvisibility module on security director client : juniper networks duration : 1st nov 2015 to till date role : technical lead technologies : core java , log4j , junit , unix , spring , hibernate , jasper report , maven , git , design pattern , hdfs , hadoop , mapreduce , hive , pig , sqoop , flume description : juniper networks junos space security director , an application that runs on the innovative , intuitive , and intelligent junos space network management platform , provides detailed visibility into application performance , reducing risk while enabling users to move quickly from knowing something is wrong to doing something to fix the problem. client : avaya duration : 25 june 2015 to 30 oct 2015 role : technical lead technologies : core java , mapreduce , hdfs , hive , pig , flume , sqoop log4j , junit , unix , design pattern , servlet , jsp description : the wireless management system ( wms ) provides a single point of configuration and monitoring for multiple mobility domains. the management solution comprises of a wms server that interacts with wireless controllers in the mobility domain and a web ui that interacts with the wms server. client : alcatel - product duration : 24 sept 2012 to 30th april 2015 role : technical lead technologies : core java , shell scripting , log4j , junit , unix , hadoop , mapreduce , hive , pig , sqoop , design pattern description : performance monitoring is the ability to perform low level quality monitoring in the network by counting certain parameters ( e.g. number of errors ) . fault localization : to find low level faults in the network ( failing lasers equipment ) in general the ne will count certain events for a fixed period of time and store the results internally. project : web - fm on 1350 oms client : alcatel - product duration : 1 oct 2014 to 30th april 2015 role : technical lead technologies : core java , spring , hibernate dojo1.9 , html , design pattern , python description : a real time application to show the status of network as soon as any fault or event occur in the network. this is most useful to alert operator if any fault occurs in the network. along with showing the status of network , this tool provides the remedial action for the user to resolve the issue. educationmasters degree in computer application ( mca - 2008 passed out ) highly qualified professional with an experience of 7.5+ years in the it sector. having 4 years experience on bigdata hadoop technologies hdfs , mapreduce , pig , hive , sqoop , flume , having knowledge on hbase , spark , kafka ) etl tools talend 5.4 , pdi 7.0 good understanding of object oriented design principals and design patterns and data structures database - db29.1 , mysql5.5 , oracle10g to write sql queries , functions , stored procedures. extensive experience in programming , deploying , configuring , fine - tuning of middle - tier j2ee application servers like was 5.1 6.1 , tomcat , jboss application frameworks struts 1.x , spring 3.1 , orm tool - hibernate java j2ee - servlet , jsp , jdbc , log4j , jasperreport development tools - rad 7.0 , eclipse 3.2 , netbeans 6.1 source control : cvs , svn , clear case , git , gradle shell scripting , linux , python having knowledge on elasticsearch and logstash received professional training on big data. passed sun certified web component developer ( scwcd ) examination with 98 % score. passed sun certified java programmer for the java 2 platform 1.4 ( scjp ) examination with 100 % score. awards @alcatel - lucent received team award @alcatel - lucent received employee of the month award @alcatel - lucent for good work received unsung hero award for being consistently a good performer received award of excellence @alcatel - lucent',\n",
       " \"summaryaround 7 years of experience in software development in java j2ee technology with sound exposure of spring framework , design patterns , soa , web services , hibernate , jpa and hadoop ecosystem bigdata on amazon cloud environment. experience on hadoop , bigdata , in - memory distributed caching and nosql. extensibly used apache hadoop technology stack like hdfs , mapreduce , hbase mapreduce api , cascading api , oozie , sqoop , impala , hive , pig and amazon redshift. rich experience in developing applications using java , j2ee with spring , hibernate framework under linux and amazon ec2 cloud environment , unit testing framework such as junit and also hands - on experience in web services. experience in long - term projects and full life - cycle software projects ( analysis , design , development , testing , implementation and documentation ) . strong knowledge on algorithms , architectures and programming languages. domain expertise in internet advertising , banking and finance , travel and hospitality and media and entertainment. excellent understanding of object oriented concept. good understanding of bigdata products in the market. proficiency with linux unix , open source tools platforms and amazon cloud. very proactive in learning and leveraging emerging technologies. experience delivering effectively in a fast paced environment following agile methodology with an ability to adopt quickly , able to work alone and as a part of a team. adept at working in deadline. dynamic , pro - active and enthusiastic person with a positive can - do attitude. interested in learning new technologies and facing new challenges. fast learner with the ability to self - train , friendly , flexible and always happy to assist others. experienceaccenture services pvt. ltd. , bangalore as a consultant ( career level 9 team lead ) from july 2015 to till date. citicorp services india limited ( citigroup ) , pune , india , ( c - 10 , assistant manager ) from july 2014 to july 2015. cybage software pvt. ltd. ( an sei - cmmi level 5 and iso 27001 company ) pune , india , sr. software engineer , sept 2009 to june 2014. responsibilities : designed and built robust hadoop solution for big data problem. worked on full life cycle for big data solution including requirement analysis , technical architecture design , solution development , testing and deployment. worked on analysis of various in - memory distributed caching framework like hazelcast 3.2.1 , coherence 12.1.2 , terracotta bigmemory 4.1 and infinispan 6.0. worked on web based projects with java , j2ee , spring , hibernate , struts. worked on creation and publication of numerous type of xml request through soap restful web services. involved into requirement gathering , understanding , analysis and scope estimation of projects. involved deeply into development and requirements enhancement phases of project. worked on projects for the largest advertising , finance and banking project. worked on projects for travel and hospitality. project1. project name : credit suisses digital private banking ( july 2015 till date ) role : back end developer technologies : java 7 , jpa , eclipselink , spring , jax - rs , apache cxf webservices , jms , pl sql , apache camel , jersey , jaxb , temenos xpml , ejb , jap 8 , junit other tools : bea weblogic server , postman , oxygen , teamcity , oracle 11g , oracle pl sql developer , soapui , jira , confluence , tectia ssh sdlc methodology : agile , test driven development , pair programming , scrum , continuous integration. project description : in rolling out of credit suisse ' s digital private banking capabilities , we are redesigning the way in which credit suisse ' s legacy private banking and wealth management business was serving their clients , here we are building digital private banking solutions for the rms so that they can have deeper insight into their clients preferences and investment objectives. role and responsibility in project : involved in the requirement gathering , understanding and analysis of building one of the rm channel module - planning center. involved in building the restful web services with jax - rs. involved in implementing fapi lair using java apache camel api which is used as enterprise integration pattern. implemented routing , processing , enrichment and validation of request and response through java apache camel api. extensively used jackson json processing api to handle json request and response. involved in implementing securityhelper and login module. involved in implementing authentication mechanism by using jwts. implementation of xstream api for serialization and deserialization of xml. involved in building the portfolio module to read the financial data from the alis. end to end implementation of stories created on jira. 1. project name : citis bigdata applied engineering ( july 2014 july 2015 ) business unit : cate ( citi architecture and technology engineering ) designation : c - 10 , assistant manager ( back end java developer ) technologies : java , apache hadoop , cloudera search , spring saml other tools : cloudera manager , eclipse , wasce , jira , tectia ssh , planview project description : the applied engineering team is responsible to ensure the big data platform technology and frameworks are integrated with citi engineering products and processes , the cate big data platform is an enterprise o and t strategic initiative by cate , dio , gpa , gfds and cti that enables citi to harness the power of big data and unlock the value of executing analytics at scale. task performed : worked on dataingestor utility. worked ldap - synch utility. provided custom implementation for hadoop ecosystem : hive udf serde , hbase rest client , hadoop api client engineer custom scripts required for maintaining 3rd party libraries in xenv. worked on cloudera search with sentry enabled cdh cluster. worked on poc related to saml based sso in spring security applications. research work on cirro data hub , apache hive , beeline , apache stinger , apache tez and apache spark. worked on solving snow jira issues. 1. project name : infogroup_yesmail_bi_hadoop ( feb 2013 april 2014 ) client : infogroup , inc. pappillion , ne ( http : www.infogroup.com ) designation : sr. software engineer technologies : java , apache hadoop , mapreduce , cascading api , oozie , sqoop , hbase , hive. cloud environment : amazon ec2 , cdh4 , centos nosql : mongodb , hbase. database : amazon redshift , impala , oracle , hive. domain : internet advertising. other tools : cloudera manager , jaspersoft , eclipse juno , putty , winscp , ssh secure client. project description : yesmail product has 13 oracle instances called e7 to handle the transactions of millions of users of almost hundreds of clients. e7 serves as a source to some of ad hoc reports however standard reports are generated through data warehouse which is periodically etled from ods which in turn etled from e7. the entire process for maintaining and keeping such a voluminous data on oracle dw is very cumbersome and costly affair so hadoop is thought as a replacement for storing and processing data. role and responsibility in project : involved in the requirement gathering , understanding and analysis of replacing oracle dw to hadoop. involved in writing etl code using cascading api for aggregating data from hbase ods to hbase dw. involved in writing mapreduce jobs , hbase mapreduce and hbase java client api. involved in writing incremenatal sqoop job actions within oozie workflow for data transfer between oracle and hbase. single handedly implemented a java application using amazon java sdk to load and monitor data from amazon s3 to amazon redshift. involved in performance optimization of cloudera impala. 1. project name : coe - cloud ( jan 2013 june 2014 ) and expedias carrental.com ( april 2014 june 2014 ) client : cybage internal projects , http : www.carrentals.com ( an expedia , inc. company ) designation : sr. software engineer technologies : spring data , apache hadoop technology stack hive , pig , mapreduce , in - memory distributed caching frameworks - hazelcast 3.2.1 , coherence 12.1.2 , terracotta bigmemory 4.1 , infinispan 6.0 os platform : centos , clouderavm , cdh4 , amazon ec2. nosql : mongodb , hbase other tools : maven , eclipse helios , jira , github , putty , winscp , ssh secure client. project description : coe team keeps on exploring emerging technology , doing r and d and giving assistance to other projects when technical issues arises. role and responsibility in project : involved in analysis of various in - memory distributed caching framework like hazelcast 3.2.1 , coherence 12.1.2 , terracotta bigmemory 4.1 and infinispan 6.0. involved in creating spring data mongodb configuration for various crud operations. since cascading is still not supported in spring data so did r and d with spring api to write customized cascading. implemented cross - store persistence for mongodb and oracle. involved in using restful api to interact with jaspersoft for automating several manual processes. 1. project name : world book ( july 2012 feb 2013 ) client : world book , inc. chicago , illinois ( www.worldbookonline.com ) designation : sr. software engineer technologies : spring , java , servlet , jsp , jstl jquery. build tool : maven , git operating systems : linux , solaris , windows. domain : media and entertainment. database : oracle. other tools : maven , eclipse helios , jira , github , putty , winscp , ssh secure client. project description : word book is one of the older and largest encyclopedia in the world. this has almost 20 products which are used across all american and canadian colleges and school. earlier its all products were running on legacy sun - one server so now it is migrated to tomcat server. all products which were implemented using struts1.1 are to be re implemented in spring for i - pad version of project. role and responsibility in project : involved in the requirement gathering , understanding and analysis of migrating the project from struts to spring. involved in migration of all 18 products of the project from sun - one legacy server to tomcat. single handedly involved in the scope estimation of related modules. single handedly involved in tiles implementation in spring. as new requirement is for i - pad so implemented html parsing in all the content files which are generated thorough cms. extensively worked on xml and html parsing. 1. project name : channel manager ( aug 2011 july 2012 ) client : bookassist , madrid , spain ( www.bookassist.org ) designation : sr. software engineer technologies : java , design patterns and web services. unit testing framework : junit4 operating systems : windows. domain : travel and hospitality. database : mysql 5.0. other tools : eclipse helios , rally , cvsnt , soapui , mysql query browser , pmd , findbugs and cobertura. project description : from managing multiple channels for room - distribution to maintaining rate parity across all channels , channel manager helps hoteliers perform tedious tasks easily and efficiently. this product is a web - based platform with the capability of managing multiple extranets. rates , inventory and restrictions can be managed across multiple merchant and retail distribution sites from the efficiency of a single interface. this product aim to give clients total control of all online inventories while cutting down on human resources diverted for updating rates and other tedious tasks. role and responsibility in project : single handedly involved in the requirement gathering , understanding and analysis of venere channel. single handedly involved in implementations of various components for the venere channel. involved in implementation of creating various xml request using apache axiom library and then publication of the same through soap rest web services for rate , availability and restriction to various channels. involved in implementation of various design pattern for connector module. involved in the coding and enhancements of the connector module. involved in implementation of publication of availability and restriction on expedia channel. 1. project name : advertise.com ( sept 2009 july 2011 ) client : internext media corporation , usa ( www.advertise.com ) designation : sr. software engineer technologies : java , j2ee , spring , hibernate , extjs , maven. unit testing framework : junit operating systems : windows. domain : internet advertising. database : mysql 5.0. other tools : smart svn , ssh secure shell client , putty , eclipse , sqlyog , firebug , tt pro and bugzila. project description : abcsearch.com ( recently branded as advertise.com ) is a pay - per - click ( ppc ) search engine that displays sponsored listings for the search keyword phrase. advertise.com not only shows its own advertiser results but also integrates them with results of other ppc search engines like yahoo , findwhat.com , kanoodle.com. it mainly serves results to many other ppc search engines acting as an upstream engine to them. the web traffic to abcsearch.com comprises of search engines requesting for results and the queries from users on the abcsearch web site. to display the listings for a search keyword , the advertiser has to register with abc where in advertiser can bid for a keyword. backend crucial metaservers to respond to the queries to get instantly results for the keywords from abcsearch internal engine and from the other search engines ( upstream engines ) with in limited timeout period of 0.6 sec. involved in developing modules like internal engine , click tracking , metaserver. role and responsibility in project : involved in the coding and enhancements of the project using java , j2ee , spring , hibernate and client framework such as extjs. involved in developing various reports for reporting module. involved in development of ecn campaign creation flow. involved in development of referral payout module. involved in fixing of dimension issues. involved in fixing of cross browser compatibility issues. involved in resolving client feedback issues. diploma in advanced computing ( dac ) acts , c - dac , pune completed diploma in advanced computing from one of c - dacs premier institute acts ( advanced computing training school ) , pune. skillstechnologies java 6 7 , j2ee , servlet , jsp , jdbc , spring 3.0 , ejb3.0 , hibernate 3.4 , jpa 2.0 , eclipselink , and struts 1.1 hadoop ecosystem hdfs , mapreduce , hbase - mapreduce and hbase java client api , cascading api , amazon java sdk , oozie , sqoop , hive , pig , apache solr , apache sentry and cloudera impala. cloud environment amazon ec2 , cloudwatch , redshift , s3 web services soap , restful , jax - rs , apache cxf webservices nosql mpp hbase , mongodb , cloudera impala , amazon redshift orm hibernate 3.4 , jpa2.0 , eclipselink , springdata databases mysql 5.0 , oracle , postgresql unit testing framework functional testing tool junit4 , soapui - 5.* , postman rest client language independent data format scripting languages json , xml , shell script , javascript , python pm build tools maven , ant , jira , rally , confluence code quality tools pmd , findbugs , cobertura version control tools terminal emulator smart svn , cvsnt , putty , winscp , ssh secure shell client , tectia ssh os windows family , linux [ ubuntu , centos , debian , rhel ] , solaris educationcompleted mca from himt gr. noida , affiliated to uptu , lucknow with 67.51 % . completed b.sc. from dr rml avadh university , faizabad with 61.77 % . passed 12th with 74.40 % from mppic balrampur ( up board , allahabad ) . passed 10th with 72.50 % from bmhss balrampur ( up board , allahabad ) .\",\n",
       " 'summary overall experience of 9 years in object oriented design , programming , implementation of core java , j2ee web , means and big data solutions worked as product developer , senior developer , team lead and onsite technology consultant. 6 years working experience in java , j2ee , pl sql and 3 years of experience in analytics based solutions using apache spark , hive , hortonworks data platform , rule engine , cognitive systems , real time reporting , nodejs , angular js , nosql , d3 , tableau , spotfire etc. good exposure to analytic solution design and development. good exposure to design and architecture of scalable and reusable product development. exposure to apache solr search engine implementation using mapreduce and hdfs. intermediate level with python nltk and statistics ( r ) , marklogic and mongo db nosql db. onshore consulting experience : involved in the technology selection and poc implementation for an analytic product development. involving in the product roadmap and development pipelining activities.( current engagement ) project hands on experience on data and analytics tools such as alteryx modeler and talend. period technology customer poc organization asset 2016 oct jboss drools , core java , python , machine learning , apache spark , work flow and still growing tech - stack global innovation team : tax proposed platform is a game changer. connecting tax domain experts and technologists together to automate the filing tax return for the customers. tax compliance at the push of a button 2016 feb node js , angular js , sql server , grunt , karma , jasmine unit testing. standard life investments , envision healthcare , ey internal products. ey internal analytics platforms reusable analytics ui development accelerator. 2016 jun apache spark , spark sql , hdp , hive , tableau , python , web - services standard life investments multi - tenant big data platform for analyzing financial data for investment banking customers 2015 - 2016 jan hdp , apache spark , hive , python , hdfs ey analytics coe , sponsored by global analytics coe team , us building confidence around the experiencecognizant technology solutions developer technical systems analyst - august 2007 to july 2013 ernst and young ( ey ) senior developer lead developer ( data and analytics ) august 2013 to present skillsmaking the commitment to the customer. mailto : anoop.pallippattuillam@gmail.com anoop.pallippattuillam@gmail.com &#124; +91 9746781223 20142015 mar core java , swing , multithreading , multi process , process pipelining custom protocol models. ey audit , uk and ir can be used to extract data from multiple erp systems such as oracle , msnav , jdedwards. extensible design for other erps 2014 jun mapreduce , hdfs , apache solr , infosphere stream , spotfire automation service ey internal strategy and architecture team , us 2013 - 2014 feb j2ee , spring mvc , jython , python nltk , xjc , marklogic nosql , jquery , xquery , ant ey cyber security innovation team , us 2012 - 2013 jul j2ee , custom web framework , wsdl , spring aop , jpa , ibm mq , jboss , ant inttra shipping 2010 - 2012 aug j2ee , spring mvc , spring templates for ws and jpa , spring core , apachecxf , apache axis , hibernate , jquery , tomcat , maven dun and bradstreet d and b maxcv report. 20092010 jun core java , j2ee , struts , pl sql , unix , jdbc , dojo , oracle 11i , tomcat , ant arbitron ppm arbitron gen2 platform 2007aug - 2008 dec j2ee , spring portlet mvc , hibernate , soap , jquery , ajax , weblogic portal server , maven capital one banking beanmapping utility : capital one banking a_a cognizant lime light yearly award delivery excellence. ey quarterly award for performance improvement. ey quarterly award for providing leadership support. educationmaster of science ( msc ) - electronics engineering june 2007 cochin university of science and technology , thrikkakara , kerala , india. cgpa : 8 10 anoop.pallippattuillam@gmail.com &#124; +91 9746781223',\n",
       " 'experienceperiod organization designation mar 2015 till date virtusapolaris software services pvt. ltd. chennai. lead consultant jun 2007 jan 2015 tata consultancy services ( tcs ) , chennai. assistant consultant nov 2005 may 2007 yalamanchili consultancy services ( ycs ) , chennai software engineer projector and robt - big data project client british telecommunications plc. london , united kingdom. employer virtusapolaris , dlf it park , chennai role and responsibilities tech lead responsibilities : involved in the project design discussions and providing the component specific design to the project specific stories. responsible for the development of various modules under the project using hive , pig , sqoop , shell scripts and oozie scheduler. developed the code from scratch for different modules of the project and provided complete support till live deployment. worked automating alert mails , oozie sla configuration , feed volume alert , record , volume alert and corrupted file alerts. worked on automating incident raising mechanism in case of oozie job failures , by which the person on the production support gets alert call from bt. provided the production support on live deployment for the project. application support has been provided by monitoring the live production system. resolving the issues on job failures. reprocessing the jobs by rerunning the workflows and monitoring. raising the appropriate incident request on the issues. creating the change request for the deployments. doing performance analysis of the code with various ranges of data. have used impala for analytical work of the adhoc requests. duration january 2016 to till date bt sales account ( bt sac ) client british telecommunications plc. london , united kingdom. employer virtusapolaris , dlf it park , chennai role and responsibilities developer responsibilities : good knowledge and hands - on experience with hadoop ecosystems like hive , pig , sqoop and oozie. used sqoop to rdbms to hdfs data move. used pig for analysis and moved the analyzed report to rdbms using sqoop. writing pig scripts based on the ab initio data flow diagrams. planning , organizing the project and interacting with client for requirement clarification. database designing as per the business requirement. once the request is received , will be an estimate and application design , components design and test case documents submitted to the customer approval. based on the customer approval the coding and testing will be started. development and testing are finished we will send test result to customer consent. duration march 2015 to december 2015 bt hp asset management ( bt hpam ) client british telecommunications plc. london , united kingdom. employer virtusapolaris , dlf it park , chennai role and responsibilities tech lead responsibilities : - development and production support activities. - providing primary and secondary support from offshore. - once the request is received , will be an estimate and application design , components design and test case documents submitted to the customer approval. - based on the customer approval the coding and testing will be started. development and testing are finished we will send test result to customer consent. - on the basis of consent will move changes into the qa environment , and finally forwarded to the production. duration march 2015 to august 2015 core banking and credit applications. ( retail business ) nalsucc , vldm , credit contact( dbcf online ) , bob( back office maintenance ) , fortis integrated risk surveillance tool( first ) , recovery( pcrv ) , financial analysis ( pcaf ) . role and responsibilities senior developer project coordinator responsibilities : - development and production support activities. - providing primary and secondary support from offshore. - taking responsibility on priority 1 , 2 and 3 production tickets. - handling of on - demand request ( odr ) and change request ( cr ) - once the request is received , will be an estimate and a dou ( document of understanding ) , application design , components design and test case documents submitted to the customer approval. - based on the customer approval the coding and testing will be started. - development and testing are finished we will send test result to customer consent. - on the basis of consent will move changes into the qa environment , and finally forwarded to the production. client bnp paribas fortis bank , brussels , belgium employer tata consultancy services , siruseri , chennai duration october 2013 to january 2015 re insurance non - life applications. ( facultative business ) - frog , frogmis , frogjobs , glance batch and interface check tool. role and responsibilities senior developer project coordinator responsibilities : - development and production support activities. - providing primary and secondary support from offshore. - taking responsibility on priority 1 , 2 and 3 production tickets. - handling of service request ( sr ) , maintenance request ( mr ) and incident requests ( ir ) . - once the request is received , will be an estimate and a par ( preliminary analysis report ) and test plan documents submitted to the customer approval. - based on the customer approval the coding and testing will be started. - development and testing are finished we will send test result to customer consent. - on the basis of consent will move changes into the qa environment , and finally forwarded to the production. client gen re , cologne , germany employer tata consultancy services , siruseri , chennai duration august 2008 to september 2013 re insurance life application. corporate life administration( cola ) role and responsibilities module leader responsibilities : - development and production support activities. - providing primary and secondary support from offshore. - taking responsibility on priority 1 , 2 and 3 production tickets. - handling of service request ( sr ) , maintenance request ( mr ) and incident requests ( ir ) . - once the request is received , will be an estimate and a par ( preliminary analysis report ) and test plan documents submitted to the customer approval. - based on the customer approval the coding and testing will be started. - development and testing are finished we will send test result to customer consent. - on the basis of consent will move changes into the qa environment , and finally forwarded to the production. client gen re , cologne , germany employer tata consultancy services , siruseri , chennai duration november 2007 to september 2013 office of foreign asset control ( ofac ) role and responsibilities role : developer responsibilities : development and production support activities. taking responsibility on priority 1 , 2 and 3 production tickets. handling of maintenance request ( mr ) and incident requests ( ir ) . once the request is received , will be an estimate and a par ( preliminary analysis report ) and test plan documents submitted to the customer approval. based on the customer approval the coding and testing will be started. development and testing are finished we will send test result to customer consent. on the basis of consent will move changes into the qa environment , and finally forwarded to the production client gen re , usa employer tata consultancy services , chennai duration june 2007 to november 2007 naradatm card management system role and responsibilities role : developer responsibilities : development and production support activities. taking responsibility on priority 1 , 2 and 3 production tickets. handling of maintenance request ( mr ) and incident requests ( ir ) . once the request is received , will be an estimate and a par ( preliminary analysis report ) and test plan documents submitted to the customer approval. based on the customer approval the coding and testing will be started. development and testing are finished we will send test result to customer consent. on the basis of consent will move changes into the qa environment , and finally forwarded to the production. client nepal development bank ( nepal ) , shamrao vithal co operative bank ( mumbai ) employer yalamanchili consultancy services pvt. ltd , chennai duration november 2005 to may 2007 skillstechnical leadership system design and implementation client relationship management team management and leadership good knowledge in the sdlc and quality processes client server application development release deployment management good comprehensive problem solving and analytical abilities tools and technologies spark 2.1.0 scala 1.6 pig 0.15.0 hive 1.2.1 sqoop 1.4.6 flume 1.5.2 oozie 4.1.0 hue browser core java ambari 2.2 powerbuilder 12.5 middleware : proximav8.0600.63 visual basic 6.0 vb script ms access pl sql oracle 9i 11g sybase 15 sqlanywhere crystal reports 4.6 ms - data report 6.0 hp asset manager 9.5 windows 2000 xp nt 8 unix project responsibilities act as overall technical authority for the project manage all managed services teams and provide technical leadership gather requirements and identify requirement gaps design the application , participate in design discussions , and review design artifacts handle client communication regarding requirements , design , etc. review the developed code and make sure it adheres to the design , standards and guidelines of the client and virtusapolaris. support the onsite team on technical issues support the client on estimations , and manage expectations manage client delight , and build a strong client relationship provide guidance and support to the clients internal teams such as the uat team , business analysts , and stakeholders participate in meetings related to project management ( with the client ) and related to technical deliveries deploy minor major releases manage onsite incidents organizational responsibilities mentor and coach junior team members contribute to the development of the software engineering competency conduct interviews , training sessions and techtalks telecom technology oozie , pig , hive , impala , sqoop , shell scripting , oracle , yarn ( mrv2 ) , java 7. project description bt - or ( bt open reach ) : or big data project is a repair transformation program in the open reach area of bt , aims at reducing the engineering cost on dealing with the broadband issues. these broad band issues arise when an openreach copper line tests ok , however the end customer has reported an issue with their broadband service. in this scenario a communications provider ( cp ) can ask openreach to try and resolve the issue and improve the end customers broadband service ( speed ) . for this the project aims at providing engineers with a set of guidelines that will help to determine the capability of a line to support a broadband service and identify whether issue there at a line or not , through which engineer understands about the line health easily and the ineffectual visits of the engineer is avoided. robt ( rest of bt ) : robt covers the different components which reside on haas. it maintains billing , network , purchase and consumer details. the existing systems are migrated into haas environment. the components omniture , any3 , eloqua , eu - business , eu - vat and qoe receive feeds from various source systems ( up streams ) . these components are transforming the source data and sending into different down streams. the different reports are generated and used for business analytics. each component has scheduled to run on hourly , daily , weekly basis. the live data will be captured in haas and provide the source for all other systems. telecom technology pig 0.12.0 , hive 1.1.0 , sqoop 1.4.5 , oozie 4.1.0 , distributer : cloudera 5.4.4 , os : cloudera 5.4.4 project description sales account ( sac ) is an entity in cmf ( customer master file ) model that defines sales channel for each bt customer and aligns them to definite line of business and bt division ( bt business , bt global services etc. ) c - cat is treated as the source system and data will be provided to sim by directly accessing the c - cat database. each sim table will be built on one or more c - cat tables to provide the sim with one table per sim entity there are also a number of relationship tables. re - writing existing ab initio project to hadoop framework. it service management ( itsm ) technology hp asset manager 9.41 , 9.5. oracle 11g , apache tomcat 7.0 project description - build software counters to manage the sw licenses. creating calculation script of counters with hp best practices. revamping the counters to sync with the english rules of license and installations , creating new counters , updating counters installations , licenses , creating new licenses , license model , installation model and installation type. - uploading exemption data , sending report data to bt vulnerability management team on request for identifying security issues. - creating new wizards , connect - it scenarios to extract transfer hpam data as per the business requirements. - migrated the application from version 9.41 to 9.50 - setup the am web client and configured apache tomcat am web services. banking and financial services ( bfs ) technology powerbuilder 12.5 , pl sql , sybase 15.5 , sqlanywhere and proxima v8.0600.63 project description nalsucc ( succession ) : the goal of nalsucc application is to meet the demands and expectations of the internal and external actors , by producing in the required or legal period the documents or services expected by them. the application nalsucc is used in the branch offices for the introduction or declarations or decease. when a declaration is fully introduced , it is forwarded to a regional site where the further administration for the file is done. vldm : tool used by back offices to manage their files and comply with civil and tax laws. the application vldm client is used in the regional sites for the introduction and settlement question of files like declarations or decease ( only handling , because introduction is done using nalsucc ) , bankruptcy , judicial investigation , etc. , credit contact ( dbcf online ) : dbcf - online is a front - end application linked to dbkd( mainframe db ) . it is used to correct errors coming in from other acquisition systems and perform changes which cannot be performed by them. the application functionality : introduction of new credit and collateral , capture and manage credit contacting data , credit agents , collaterals , goods etc. credit status and negotiation status , collateral deed and appendix and different credit modes .. bob ( back office beheer ) : bob acronym stands for back office beheer. it was built for follow - up of servicing activates and now it is also used in contracting , helpdesk , audit and other activities like controlling scanned documents and sending e - mails. search and select tasks , task distribution , client transfer , mass consultation of task , multiple creation modification of task , create actions , etc. , reporting , webi - this opens webi web service. first( fortis integrated risk surveillance tool ) : the first application gets the global overview in case the total risk of a credit client passes a certain threshold. first is a tool used in the department of credits by risk surveillance , intensive care and recovery for a given target group ( kk whk ) different kind of simple triggers ( potential risk indicators ) are uploaded by other applications in first. the daily first filter process will complete the info of the simple triggers with common parameters ( e.g. commercial segmentation , number of days simple trigger is active , etc. ) and determine the individual trigger. each individual trigger has a linked weight. in case the total weight of the set of individual triggers for a credit client passes a determined threshold , a dossier will be created. the dossiers will be assigned to a risk manager who can handle these open files via the front end application of first. recovery ( pcrv ) : this application handles all the recovery functionalities like creating claim for the denounced credit or account , distributing the movements and maintains all details of the claim ( customer info , movements , debts , components of it , imputation , conditions of interest , security and non - security , third party , reserves , actions , repayment plan ) . financial analysis ( pcaf ) : the goal of the application is to help the commercial function and the credits to analyze in a uniform way and review the balance sheets of the companies. choose the balance sheet correction modify balance sheet create new balance sheet duplicate the existing balance sheet reports are created on powerbuilder application screen and we can print the report insurance re insurance technology powerbuilder 6.5 , 11.1 and 12.5 , pl sql , sybase 12.5 and 15.5 , sqlanywhere. project description frog ( facultative re organization group ) : frog is an application framework to support the facultative non - life business at gen re. it is used to quote , accept , manage , account and administrate facultative property , engineering , marine and casualty reinsurance business for the company. additionally , the runoff of aviation and credit bond business is managed via frog. frog jobs : night time batches to feed the frog gaap database , the frog mis database , the mainframe kiwi fac system , the mainframe accounting system inclusive retrocession , the mainframe claim system. frog jobs runs both gui and batch mode. frog mis : the frog mis application is a separate report generation application. the mis reports are categorized into five broad groups called premium production , gaap , outstanding losses , gcsa ( general cologne south africa ) and others. apart from the above categories , report on terrorism exposure and reminder report are also available. glance batch : transfers the changes done in frog to the glance system. this runs every ten minutes. frog interface check : this is an online application to monitor the status of the interface batches and to check the result of reconciliation procedures. insurance re insurance technology powerbuilder 6.5 , 11.1 and 12.5 , pl sql , sybase 12.5 and 15.5 , sqlanywhere. project description cola ( corporate life administration ) : application supports life health underwriting and policy administration. it is able to administrate different subsidiary companies of genre and branches. cola is currently used in different branches based in cologne , london , paris , vienna , warsaw , tokyo , hong kong , singapore , shanghai , taipei , johannesburg , cape town and mexico. support l h facultative underwriting store significant and relevant data of l h applications and underwriting assessments. assessment is done for the underwriting and produce response to cedant through letter either in print or fax. cedants can submit applications through facworld internet application. the applications received in this way are verified and stored in cola by the underwriter. cola also stores the application receiving date for calculating the response time of the uw applications. support policy administration and automatic transfer of single policy data through cola - audi. maintenance and calculation of actuarial reserves and sum at risk for policy administration based on plan definition vectors. accumulation control and calculation of retrocession needs depending on type of risk and retention policies of different firms. insurance technology visual basic 6.0 , vb script , oracle 9i , sql , pl sql and ms access project description office of foreign asset control ( ofac ) : ofac used to get the background details of insured. it will check the insured name , address , date of birth , etc. , with centralized database and provide the hit listed person details and scores. it can be access both front - end and batch job and this application generated files are input to the mainframe systems. banking and financial services ( bfs ) technology visual basic 6.0 , oracle 9i , sql , pl sql and crystal reports 4.6 project description naradatm card management system : - the card management system manages the entire life cycle starting from managing the customer details , account and card details for the customers applying for the card. the application has been designed to work in the client server technology. the application also supports in remote embossing of the cards , interfacing with hardware software security modules for generating and printing the pin mailers , managing the card maintenance and generating the respective card authorization file in case of integrating with the external switch software. the application can also manage the data flow of the details in distributed environments. naradatm user access management : - all the other applications developed in the company uses user access management. user access management controls the rights to the user to access the particular application. uam captures the details of the user and allocates the users to the particular group. all the applications are captured in uam and rights are also assigned on the basis of modules available in the application. uam also provides reports on details of each application , users and rights granted to the users. naradatm transaction management system : transaction management information system is developed to provide comprehensive information regarding the transactions that took place in the atms. transaction mis updates the management with information which helps them to analyze and compare the transactions at various levels. visa quarterly report , ageing report , merchant settlement report , successful cash withdrawal , cash dispensed report , transaction volume report , atm downtime report and reversal reports are few examples of transaction query. this intranet application was initially developed using visual basic. educationprofessional degree : master of computer applications ( mca ) with 80 % - at sri vasavi college , erode , affiliated to bharathiyar university , coimbatore. hortonworks data platform certified developer ( 2016 )',\n",
       " \"summary8+ years of it experience which includes experience in apache hadoop , java and mainframes. possess good technical and architectural knowledge in big data hadoop ecosystem such as hdfs hadoop hive pig hbase phoenix sqoop flume map - reduce core java oozie hadoop ecosystems and open source projects. extensively worked on data extraction , transformation and loading data from various sources like db2 and ims , xml , json files experienced in deploy hadoop cluster environment able to assess business rules , collaborate with stakeholders and perform source - to - target data mapping , design and review 4+ years of experience in requirement gathering , analysis , design and implementation of projects by closely interacting with clients of u.s locations extensive experience in software development life cycle ( sdlc ) model excellent analytical , troubleshooting and user - interaction skills experienceworking as a software engineer in galaxe from aug14 till date. worked as a software engineer in ust - global from mar13 till jul ' 14 worked as senior software engineer in ibm bangalore , as a business associate from sep08 till oct12. worked as senior software engineer in accenture services private limited bangalore , from nov06 till jun08. organization : galaxe solutions. title : navigator client : optumrx duration : mar16 till date. role : senior developer environment : apache hadoop platform , pig , hive , hbase , mapreduce , sqoop , cron job description : this project aims to do etl , data aggregation and data analytics on top of navigator healthcare data which is a pharmacy benefit management company ( pbm ) . it is a managed care organization ( mco ) that focuses specifically on the cost - effective and appropriate delivery of prescription medications to the market. the pharmacy dataset contains millions of records about the pharmacies and the type of drugs and its usage. the raw data will be from various sources and dumpled directly in hadoop file system through sqoop and classes which takes data from rest web services. we wrote several pig hive scripts to do etl and drug analysis. we used hive to do adhoc querying and reporting. roles and responsibilities. involved in client meetings to understand the problem statements and requirements support all business areas of navigator healthcare data with critical data analysis that helps team members make profitable decisions as a forecast expert and business analyst and utilize tools for business optimization and analytics data migration from source systems to hadoop wrote hive mapreduce pig programs for etl created hive schemas for complex json documents created hive table schemas , data loading , report generation. delivering the reports and getting feedback from customer to enhance analysis organization : galaxe solutions. title : express scripts client : optumrx duration : aug14 mar ' 16. role : team member environment : apache hadoop , java , mapreduce , hbase , sqoop , oozie , phoenix , hue , winscp , spark sql , kafka , spark streaming description : optum is one of the usas largest insurance company with different types of insurances in health care. this is basically a development project to migrate data sitting on ims db to hdfs for rxexpress. the aim of this project is to produce the reports and analytics done by legacy systems through hadoop ecosystems. we migrated healthcare data , billing datas sitting in several tables to hdfs. the data migrated helped in producing several monthly reports and analytics without the need of running mainframe jobs to run the reports. we have used spark sql and spark streaming as well for streaming data from web portals of optum for transactional status analysis. roles and responsibilities. involved in client meetings to understand the problem statements and requirements analyzing new requirements and identifying test scenarios. involved in setting up hadoop hbase cluster. data migration from source systems to hadoop wrote mapreduce programs for extracting data from ims incorporating new changes as per the client requirements. documented and delivered it to the customer. organization : ust - global. title : wgs healthcare migration client : wellpoint duration : mar13 jul ' 14. role : team member environment : core java , eclipse , xml webservices , hibernate , spring 3.0 description : wellpoint is one of the usas largest insurance company with different types of insurances in health care. wgs membership system handles member enrollment and premium collection from members for the services rendered by wellpoint. wgs ( wellpoint group system ) deals with the insurance coverage provided for mostly large groups ( groups that have 51 or more subscribers ) and few individual members. roles and responsibilities. involved in client meetings to understand the problem statements and requirements analyzing new requirements and identifying test scenarios. wrote java classes for new requirements and crs. incorporating new changes as per the client requirements. documented and delivered it to the customer. projecttitle : myer retek transformation client : myer , australia duration : oct 10 oct12. organization : ibm. role : team member environment : java , eclipse , xml webservices , hibernate , struts 1.0 description : myer retek transformation project consists of transforming the mainframe components to retek software which included java components related to the gift registry system. the gift registry system is used by myer to register new gift registry number for events like marriage , birthday and special occasions. the new registry numbers will have the gift items added to it for that particular occasion. the project is basically transformations of legacy written code to java. i was involved in doing reverse engineering for existing code and writing and executing the same in java. roles and responsibilities. coding and problem fixing for assigned defects. involved in reverse enginerring for legacy programs involved in preparing design documents and flow charts involved in preparing unit test plans , unit testing involved in code review documentation as per the quality procedures. title : myer client : myer , australia duration : sep08 sep10. organization : ibm. role : senior team member environment : ibm os 390 , cobol , db2 , jcl , cics , ctm , tiwoli description : myer is one of the australias largest retailers consists of around 65 stores. myer deals with fashion garments for men women kids , cosmetics , electronics. ibm is providing the application management services for the myer it group. eq is an order processing system which processes all types of orders like wms orders , retek orders , reverse purchase orders and lost and found orders. the processed orders are sent to suppliers through gateway and asns for the orders are received from the supplier and receipts are received and sent to various systems .. roles and responsibilities. production support 24x7 for the application and maintaining tickets within slas coding and problem fixing for assigned requests. involved in preparing unit test plans , unit testing , application testing , regression testing. involved in code review and unit testing. involved in coding fixing and functionality testing using java during transition phase documentation as per the quality procedures. title : defakto client : telenor , norwich duration : nov06 jun08. organization : accenture services private limited , bangalore. role : senior team member environment : ibm os 390 , cobol , db2 , jcl , skel , vsam , and clist description : telenor is one of the leaders in world telecommunication and information service industry in europe. telenor facilitates all kind of new technologies and services to norway and european customers. defakto is a call is a rating and billing system for telenor. defakto mainly has two systems , one is the message processing ( mp ) and the other is the billing. message process rates all unrated call details and the billing part of it provides the billing details after validations for qualifying calls. depending on the specific call scenario the call records are rated and billed and given discounts for all the qualifying calls in the billing cycles. roles and responsibilities. involved in knowledge transfer from onsite production support for the application and maintaining tickets within slas coding and problem fixing for assigned requests. involved in preparing unit test plans , unit testing , application testing , regression testing and user acceptance testing. involved in code review and unit testing. documentation as per the quality procedures. played the role of senior team member and has given knowledge transfer to new joiness. skillsoperating systems ubuntu , cent - os , windows. programming languages java , scala , cobol , db2 , jcl. web technologies xml , json , servlets , jsp , webservices sql databases db2 , mysql , oracle big data technologies hadoop , spark sql , spark streaming , kafka , mapreduce , sqoop , pig , hive , hbase , phoenix , lipstick , flume , oozie , cronejob , hadoop ecosystems and open source projects. build tools ide eclipse , ant , maven educationb.e in mechanical from mangalore university , karnataka , 61 % ( 2001 ) p.d.c mahatma gandhi university , kerala 69 % ( 1996 ) s.s.l.c - board of examinations , st : thomas school , kozhencherry , kerala 59 % ( 1994 )\",\n",
       " 'experiencecapgemini : consultant and module lead in data warehousing bi team ( apr 2015 till date ) tesco hsc : senior software engineer in store ordering team ( nov - 2010 to mar - 2015 ) project 1 ( current project @ capgemini ) application name : dell digital marketing analytics workbench client : dell description : this entire workbench deals with a number of use cases for dell digital marketing. this program allows dell to communicate better with customers and making strong business campaign and strategy through strong analytics platform. each use case itself is an individual project having certain goal for the marketing activity .this keeps track of the different visitors who have been browsing on the dell.com website on a specific product page for a specified length of time and have not made a purchase. the campaign serves to remind customers about their browsing activity and drive them back to the site to consider making a purchase. use cases dell digital marketing analytics workbench maw - online data mart maw customer journey data mart ingestion of eloqua data into dell hadoop data reservoir maw - mediaplex data ingestion maw - browse to buy maw package web summery maw lead account scoring maw e2e industry report maw account identification maw - package web summery aster data etl migration to hadoop etl technical environment : teradata , hadoop , aster , informatica - bde , teradata 13 , informatica , unix shell scripting duration april 2015 to till date team size 8 type of project development responsibilities working as an etl technical developer for the transformation of the proposed solution into feasible approach. etl development of hadoop components( hql scripts and bde components ) etl development using aster development of the teradata sql scripts by pulling data from aster environment and teradata environment as per the requirement. testing the end to end process. preparing technical design document , etl specification documents by understanding data model and the requirement. giving presentation to business stake holders skills5 years 9 months of it industry experience encompassing a wide range of skill sets. good technical skills on teradata and hadoop technology with strong database skills and development knowledge. worked as a onsite coordinator( from uk ) for 6 months in my first organisation. hands on experience in all phases of software development life cycle including analysis , design , coding , testing , maintenance and support. proficient in analyzing system and translating business requirements to technical requirements and architecture. well developed interpersonal , analytical , problem solving and communication skills with good documentation and presentation skills. good team player , focused , self - disciplined , self - motivated , quick learner , committed to continuous self - improvement. flexible and versatile to adapt to new environment and working on new projects. have experience on working with agile - scrum methodology. etl and analytics working experience in aster. extensive etl experience in using teradata utilities like bteq , fastexport , fast load and mload strong scripting knowledge in unix. experience in extracting , transforming and loading data using informatica powercenter. got extra mile award in capgemini in 2016. database systems : teradata v14.10 scripting : sql , bteq , mload , fload , ibm mainframe jcl , unix schedule and monitoring : ca - 7 , teradata viewpoint etl tools : informatica big data tools : hadoop hdfs , hive , scoop , informatica bde , pig , yarn concepts , datameer educationbe in computer science ( 2010 ) , with cgpa 70 % from sagar institute of research and technology ( rgpv ) bhopal.',\n",
       " 'summary#1 big - data platform development for iot - pitney bowes software this project won iot solutions world congress , barcelona 2016 award for best business transformation iot solutions are usually a big data problem and we required to address the challenges of large data storage and processing to answer real time as well as batch processing. as a part of core engineering team of big data at pitney bowes , have contributed in , development of multiple components of overall architecture solution using various big data technologies java spring solutions to expose rest based apis for integration with upstream downstream systems event driven aws lambdas using node.js elastic search implementations nosql data modeling hive jobs for data processing recommendation engine using hive and custom udfs integration with big data lake to serve analytical reporting needs performance tests to conclude on scalability and sizing of cloud infra integration with iot hardware to push events to kinesis stream cloud formation using ansible , log analysis using sumo logic analytics driven approach push domain specific data to elastic search , grafana dashboards. code refactoring and cleanup sr.software engineer at pitney bowes software november 2015 present big data application developer at ibm india august 2013 october 2015 consultant developer at ibm [ payroll of shilpin consulting , pune ] april 2011 august 2013 technical associate [ entry level developer ] at tech mahindra ltd , india april 2010 - april 2011 trainee software engineer at shilpin consulting pvt. ltd , pune october 2009 april 2010 #4 ibm biginsight ( hadoop ) implementation for telecom vigilance system icomply [ ibm india ] data size : 80 tb vigilance system is government mandate and needs to maintain 7 years customers call detail records. this system is used by nodal officers and government agencies like police , intelligence bureau for various investigations and is critical to provide accurate , complete and timely data. an operator with 141 million subscriber base have massive data stored in oracle database systems. most of the data is kept on backup tape storage to save on db instances and required storage. this helps in saving license and hardware cost but increase the operational cost and overheads involved in data restoration from offline storage to online database is resource , efforts and time consuming. #3 data ware house migration to hadoop eco system , ibm client for a ibm client , migration of warehouse from legacy rdbms systems to new hadoop implementation was required with minimal impact on ongoing business activities. developed automated and simple to use migration tools for use of operation team contribution in optimum planning of migration automated publish of status and summary of migration run everyday evaluation of various approaches and tools before finalizing the solution #2 data hub and data lake [ fusion platform ] pitney bowes software shipping and mailing business for e - commerce fusion data lake is being developed to address big data challenges of growing digital footprint of various verticals of business like ecommerce , location intelligence and postage meters data lake has many components and integration with multiple systems and in - house products. contribution to hive jobs to build recommendations for 200 , 000 customer base responsible for developing and maintaining udfs for hive and pig scripts map reduce jobs for various scenarios poc with pentaho and one of the in - house product integration role : developer number of connected iot devices : 600 , 000 number of events : 30 million per day data size : 1.7 tb per month transactional data with 10 % growth every quarter experiencethumbs up award for outstanding performance july 2016 with proposed hadoop solution using biginsight , data is stored on hdfs with high availability assurance. cost of hardware is reduced by 35 % . 100 % data is online and stored in hdfs for quick access. set - up of 20 node clusters of ibm x series linux boxes hive table design and query conversion from rdbms sql to bigsql hive developed hdfs file management utility using hdfs api integration of existing legacy java based gui to hdfs data integration with upstream and downstream systems involved in architectural designing automation for data migration from oracle db servers to hadoop cluster performance tuning #6 prepaid ods system real time big data analytics and high performance oracle ods [ ibm india ] prepaid ods system is designed and developed using infosphere streams and oracle database. this implementation has helped client to reduce the cost of repeat call to customer care by 30 % - huge saving on cost. for every events on telecom network , a cdr is generated. such cdrs are processed at real times using infosphere streams and loaded into oracle database. this is continues writing and at the same time high tps reading by crm. business demanded tight sla of 5 minutes. with use of infosphere streams processing is being done at very fast speed and optimized database design helped to meet sla. design and developed database to meet performance requirement developed a robust perl module for db integration which was backbone for performance stream job development provided fix for critical performance issue which was a show stopper appreciated with managers choice award developed a robust perl framework , responsible for time based critical partition exchange , data loading operations , which is critical to meet sla and any failure would lead to customer service impact. delivered solution with 99.99 % uptime. awarded for exemplary technical contribution #5 ibm biginsight ( hadoop ) implementation in business intelligence domain service plus [ bi ] - customer segmentation [ 7 million customer base of telecom ] [ ibm india ] for a telecom operator with second largest subscriber base in india , legacy bi system was in place for various aggregations on structured data using db2 proposed and delivered solution using infosphere stream for network event processing and customer segmentation using hadoop. to identify customers monthly , quarterly usage pattern various aggregates were running as db2 jobs , stored procedure which were replaced by hadoop solution and bi reports are generated 70 % faster and overall hardware cost reduced by 35 % existing reports were day - 1 and enhanced to hour - x , where x>3 and x< 8 pig scripts , udfs hdfs file management data migration pipeline implementation part of university relationship program of ibm 2014 - 2015 seminars and technical training on hadoop techathon 2013 , hachathon 2015 by cii additional participations # 8 multiple development and maintenance projects in area of complex event processing for ibm clients while in employment with shilpin consulting pvt ltd , pune , india #7 [ big data ] real time analytics for telecom revenue assurance shilpin consulting pvt ltd , pune , india developed and delivered infosphere stream jobs for ra system. streams jobs performs analytics on asn and binary files and can process up to 100000 cdrs per seconds in distributed mode. developing streams jobs using spl , c automated job deployment package using perl develop asn parser projectmaster of computer application , university of pune [ 2009 ] 64.00 % bachelor of computer science , bharti vidyapeeth university , pune [ 2005 ] 75.33 % higher secondary certificate , amravati board , maharashtra [ 2002 ] 64.00 % secondary school certificate , gujarat board [ 2000 ] 73.86 % skillspersonal strengths creative problem solver excellent presentation and inter - personal skills and public speaker logical and analytical skills , quick learning abilities educationthumbs up award pitney bowes , july 2016 treasure wild duck award for hackathon 2015 - march 2015 techathon winner , ibm 2014 certificate of merit from government of india [ deity ] for mygov.in participation managers choice award - july 2014 certificate of excellence - april 2014 winner of ibm gbs communication sector logo design , ibm 2014 project start award , shilpin consulting , june 2012 certification of appreciation - september 2010 first patent attempt published as paper on ip.com as per ibm jury decision ip.com disclosure number : ipcom000244811d https : priorart.ip.com ipcom 000244811',\n",
       " 'summaryseeking position as a hadoop developer utilizing 5year 9 month of it experience with multiple platforms with special skills in java , linux solaris and scripting. self motivated , dedicated and can adapt to the work environment. experiencecurrent organization : hewlett - packard global software limited duration : june 2010 to till date position : software engineer projecttitle : vic - scv operating system : linux team size : 12 role : sr. developer project description : vic - scv ( visibility and integration cloud introduction ) ( supply chain visibility ) onehp scv solution provides a consolidated physical supply chain site data management solution that will enable visibility tools for business continuity planning and management , physical supply chain threat awareness , and risk evaluation. aimed at minimizing incident management times and providing a versatile set of data to meet cross functional sc requests. responsibilities : requirement analysis and understanding of existing perl scripts. development of shell scripts for monitoring the hlz. development of hadoop map reduce programs and hive scripts. deployment into the test regions. imported and exported data between hdfs and rdbms using sqoop. unit testing , volumetric testing and testing the converted scripts. support system integrated testing. created workflows and configured in the bedrock server. environment : java , apache tomcat , mapreduce , hdfs , hive , hbase , pig. title : vodafone consumer online client : vodafone team size : 18 role : tester project description : the ecare program builds a consistent and complete vodafone ecare approach for all vodafone products ( mobile , fixed - net , lte , vodafone tv , etc. ) uniformly over all vodafone local markets. the program increases ease of use , customer satisfaction , functional coverage , reusability and builds the foundation for next evolution steps. missing self - service transaction are delivered to expose the full customer care portfolio over the online channel. responsibilities : preparing the test scenarios test cases. understanding the business requirements and hld for their testability. test execution , defect logging and tracking till closure in hp - qc. executing the test cases in alm. understand the master spread sheets given by business title : retail market predictive analysis client : leading retailer of us team size : 18 role : developer project description : it is based on a customer case study to find out which product is high on sale in a particular season or if tow discounted product are following the same pattern over a range of time during the sale season. using hadoop programming for analysis of the retail data comparing the invoice of the one customer with invoice of another customer whether both the invoice have some common product. so based on that report retailer can provide some discount price on same product. responsibilities : designed the modules considering various factors like reusability , extensibility , modularity , scalability and flexibility. defined the best practices in writing the code in mapreduce developed the hive scripts and hive user defined functions. developed hbase code to store the output. defined the best practices in writing the code in pig tools and technologies : hadoop , mapreduce , hive , pig and hbase operating systems linux , solaris , windows languages c++ , java6 , j2ee databases oracle 9i , 10g , mysql high availability veritas cluster and redhat cluster web related html , jsp , servlets , jdbc scripting shell script , perl script , java script , knowledge on python frame work struts , hibernate , spring bigdata hadoop , hdfs , mapreduce , hbase , hive and pig xml related xml , xsd , jaxb skillsassociate with 5 year 9 month experience in consulting , analysis and implementation of java and big data solutions for various project assignments hands - on experience with the hadoop eco - system ( hive , hbase , hdfs , map reduce , pig ) expertise in solaris and linux hands on shell scripting high availability management veritas cluster and redhat cluster hands on in java j2ee and hadoop ecosystem developed a backend job using java and shell scripts to extract and process data from osvdb ( open source vulnerability database and push it into mysql developed an alerting system based on priority , using email and a sms api coding using core java xml , java api s .connectivity experience in working with myeclipse , eclipse , net beans creating srs , designing( class diagram ) and software programming strong experience of deployments and administration in linux unix environments trainings at : hadoop cluster training hadoop ecosystem( hive , hbase and pig ) educationmaster of science in computer science from university of madras - 2009 with 81 %',\n",
       " 'summarymore than 6+ years of relevant experience in data analysis , data mining , predictive modeling across retail domain and product based company rich experience on modeling with the help of hadoop , r , spark , hive , hbase , phoenix , nifi , kafka , and storm , azur. extensive experience to transform business data into relevant and actionable insights using good balance of math and business skills expert level experience in developing models like regression model , logistic model , market mix model , glm models rich experience in developing churn models attrition models across banking finance , retail , telecom and pharmaceutical and ecommerce business worked in cross sell and upsell models for banking and finance , technology and multi brand sports retailer worked as a developer analyst and consultant in a pure play analytics firm. worked on executive dash - board to capture various kpis related to marketing and sales activities experienceover 6+ years experience in complete software development life cycle specializing in big data guru and team lead with extensive experience in data integration and business analytics. developer with passion for technology , learning , and rising to new challenges. proactive problem - solver with excellent ability to context - switch and work on multiple projects. outstanding organizational skills , commitment to quality , and drive to consistently meet and exceed expectations and deliverable. current responsibilities : - working on big data platform to develop a scalable prediction model using java , hadoop , hive , hbase , pig , sqoop , spark , kafka , ooziee , zookeeper , r. i have worked on creating a hadoop like framework to enable parallelism between multiple r sessions - which will enable r to work in parallel way similar to hadoop , while being fault tolerant and scalable. working with massively distributed , high - performance computing. transforming large , unruly data sets into competitive advantages using cutting edge newbig data technologies using the latest data and analytics technologies including cloudtechnology , python , java , hadoop , mapreduce , hive , spark , and hbase organization : adobe system india private limited , bangalore , india duration : nov 2012 - till date designation : sr. big data and spark analytical developer at adobe area of work based on current business objective : real - time data collection from various data center. building etl pipe line for bigdata. developing mapreduce hive pig sqoop spark code from the scratch. integration of data from multiple data sources with bigdata. customer analytics md and p ( marketing data and personalization ) marketing data platform - ingestion and analytics campaign analytics and effectiveness personalization - triggered marketing events churn analyzer current project : - 1 mcdp ( marketing cloud and data platform ) product : key responsibility : worked as sr. hadoop big data analytics developer on customer data analysis of adobe.com customers which involved analyzing , extracting , transforming and modelling of various types of data related to adobe products download , usage and behavioral pattern. my involvement was mainly focused on the events tracking system , social media data as a source from where all the data collected are processed and analyzed by using big data tools and technology like java , hadoop , hive , pig , mapreduce , hbase , spark , niki , kafka and custom created tools ( collection agent ) , etc. based on business needs , the data collected was transformed using various analytical tools like r , spark , and mapreduce to deep dive into the data to discover the patterns and provide transform logic. technology used : shell script , java , hadoop , hive , hbase , pig , niki , kafka , spark , azur , aws , google storage , tablue , hana , sybase iq , docker , maven , sbit , jenkins etc. business gain : the output of this data was used in various business case modelling through live dashboards reporting that help in better understanding of customer usage , retention , attrition , recommendation based on various market segments , customer profile , etc. during this time , i was involved in analyzing data complexity and coming up with various approach design to implement the business cases. provided poc for such use cases before actual implementation. this was highly appreciated and we were able to solve many technical issues. ( like identity stitching , recommendation , free to paid customer conversion over real time data feed ) project - 2 : ( migration project from hana to hadoop ) organization : mu - sigma business solution private limited , bangalore , india duration : may 2011 oct 2012 designation : analytics engineer area of work : retail experience banking insurance responsibility : worked as modeling professional in developing a campaign effectiveness measurement tool by using six different technologies for the largest home improvement retailer of usa ; high appreciation from the clients about the functionalities of the so called state of art tool. store performance been measured with the help of test control methodology and panel regression analysis worked on marketing analytics for a technological retail client , where the aim is to evaluate the short term and long term impact of marketing and other business drivers on revenue developed a series of roi mmx models to study the halo - effects of media channels and quantify their effectiveness using mix modeling , glm methods developed cross and upsell models using binary logistic regression techniques to find out the list of customers likely to make transaction across different lobs. technology used : java , teradata , hadoop , hive , rabitmq , r , d3 chart. challenging task : a ) predict free to paid customer probability. b ) handling customer cancellation process using churn attrition model. c ) developing the robust segmentation algorithm to determine the free or paid customer. d ) recommendation based on the product and usage. e ) data latency from various source system. f ) real - time reporting ad - hoc reporting. g ) capture user who go into stopped or suspended status as another indicator for user dissatisfaction. organization designation duration adobe system india private limited sr. hadoop developer nov 2012 - till date mu sigma business solution pvt. ltd analytics engineer 2011 oct 2012 edhita tech trainee +developer nov 2008 2010 skillsjava , python , scala analytics tools : r , sas , spark , excel. big data skill : hadoop , hive , hbase , sqoop , map reduce , spark , mongo db , cassandra , pig , oozie , kafka , nifi , flume , phoenix , impala , tez relational database : mysql , oracle , hana ( basic ) , teradata. reporting tools : tableau , micro strategy. scripting language : shell script. building tools : apache - maven , sbt , gradle operating system : linux , ubuntu , centos 6.5 , windows. web server : tomcat , apache 2.0 application server : jboss , web logic 8.0 web services restful cloud amazon , google , azur big data platform cloudera , hortonwork , apache , azur hd inside other statistical modeling , mathematical modeling , operation research , distributed computing , snaplogic current work exposure on technology : big data , hadoop , map reduce , spark , hive , hbase , nifi , kafka , r , sas , pig , sql , hana , and tableau. also got knowledge on : python , ruby , php , jsp , servlet , spring , hibernate , ibatis , sqlserver , etl tools , ooziee , zookeeper , modeling , designing. decent analytical skill over complex things to do in easy way. an efficient communicator with excellent organizational , leadership , team building , motivational and project management skills. a result oriented professional with an analytical mind capable of delivering timely and desired results even in most challenging situations. ability to do work on risk environments with dead line comment. experience with web application performance and scalability issues. capacity to deliver complex projects on time. experience in debugging and problem - solving skills. a_aadobe kudos ( 2016 ) for smart work and commitment delivery. adobe talent context ( 2015 ) spot kudo spot award ( july 11 ) : received spot award for showing the right acumen in delivering the complex deliverables in time and automating the process( from mu - sigma ) certified on big data analytics from edupristine specializing in predictive modeling , decision science and machine learning. education2005 2008 fakir mohan university ( mca ) . 2002 2005 fakir mohan university ( b.sc. physics , math ) .',\n",
       " \"experiencecompany designation period hexaware technologies big data architect and developer dec2016 - till date ample7 infra soft pvt ltd bigdata consultant december 2012 - nov16 agewell technologies pvt ltd software engineer july 2011 to november 2012 levvo software technologies pvt ltd it - trainer february 2008 to june 2011 - preparations for civil cervices exam january 2005 to december 2007 projectquantum - sales - ib client project duration hexaware technologies dec 2016 to till date roles big data architect and developer project description this project deals with the sales and support team for the customer and product sales . in this project every day 50gb data is being generated for the sales . this project has been developed using ruby - rails and mysql . technologies used cdh5 , cassandra3.0( datastax ) , zookeeper3.4.6 , apache kafka , spark1.3.0 , scala2.10 , kafka - 2.10.0.9.0.1 , ruby - rails , rdh6.0 , mysql5.6 , cloudserver - 10node responsibilities design the cluster architecture using 10 node cluster setup 10 node cluster setup on cloud server . multi - node cluster setup for spark and cassandra , kafka design cassandra data model for importing the data setting scala , sbt and spark . integrate apache kafka with multinode using cloud server. implemenation of spark streaming using apache kafka to cassandra involved in testing for cluster data client interatction on the project manage 6 member team use spark mlib for machine learning algorithms use k - means , expectation - maximization , power iteration clustering algorithm . write higher order user defined function in scala . perform analyctics using spark use kafka for the live data streaming on hourly basis in the project adding recommendation listing understanding customer recommendation behavior perform analytics predict shipment delivery failures company project duration client : dhl roles : big data consultant duration : may 2014 to october 2015 client location enrich - it , dlf building , gachi bowli , hyderabad - 500019 project description problems concerning increased shipment re - deliveries and customers refusing the shipments due to additional charges had become concern for dhl. the redelivery of shipments not only added costs but also consumed extra storage space in facilities for the un - delivered goods. work was conducted with objective to , predict chances of customers refusing to be delivered shipments reduce the number of multiple last mile journeys provide insights into different paradigms of failed shipment delivery technologies used mapr2.0 , sqoop1.4.6 , hive0.13.0 , rhadoop , rmr2 , rhdfs , apachesolr3.6 , flume1.5.0 , jdk1.6 . responsibilities data ingestion with sqoop i transferred bulk data related to shipments , billing , customers , global references , tracing , claims , complaints , and checkpoints [ sensor data ] to hdfs and hive designed and developed hive tables using joins , partitioning and bucketing. made the data ready for performing future analysis. execution of hive was done within spark engine analysis and reports using r programming language i performed predictive analysis on the shipments data. applied decision trees and random forest supervised learning technique to predict failure of shipment delivery using qlickview i facilitated comprehensive customer relationship management dashboards which provided the insights into different paradigms of failed shipment delivery and customer problems email analysis high volumes of customer queries in the form of emails are received by dhl. managing emails , understanding customer queries , providing timely responses , tracing , searching and storing emails became a challenge and concern for dhl. work was undertaken with objective to , perform sentiment and text analysis on the emails , using sophisticated text - analysis to discover trends in customer problems. to categorize the emails and route them to right agent queue map correlations between the unstructured data in email and transaction data. integrate with backend systems , and customer data in single place increase contact centre productivity and improve overall customer service , reduce handle and wrap time for replying to the emails , enforce service levels provision for the csa ' s to search emails on different email fields , to , subject , from , date , subject , email contents , keywords provide management dashboards to the supervisors data ingestion configured flume for fetching the emails , morphlines for transformation and storing the emails into solr , hdfs designed and developed hive queries exported data from hdfs to transaction database using sqoop analysis using r programming language i performed text analysis and sentiment analysis on the emails. created word cloud , discovered topics using lda algorithm of topic modelling hierarchical and k - means clustering were performed to understand various segments of the customer queries exported hdfs data to the transactional database using sqoop clinical integration acceleration company project duration enrich - it august 13 - april 2014 client project duration optum health may 13 - april 2014 client location enrich - it , dlf building , gachi bowli , hyderabad - 500019 project description cdid is identified as one of the source for this clinical integration acceleration project. data will be extracted from xml files for this source and will be maintained in non - relational file hub layer after enhancements and enrichments. cdid is uhcs capability for in - taking hl7 transactions. cdid consumes hl7 from ocie and transforms them in the hl7 ccd v3 format implementation details : creating two workflows to handle historical rejects. cdid_historicalload 2. cdid_processhistoricalrejects. cdid_historicalload : this workflow will have a single step that will invoke a java class. historicalload : it will get all the rejected csv files( message id , reason .. ) from reject location and merge all the rejected message ids into one file. creates a hbase table on top of the consolidated rejected message ids. cdid_historicalload workflow : this workflow has 3 steps. 1. getfilesfrom archive : 2. filteringrejects : 3. movefilesto landingzonedir : once xml , xsd and control file are moved into the landing zone directory new incremental intake workflow will be triggered. technologies used mapr2.0 , jdk1.6 , , apachecassandra2.0 , pig0.12.0 , mapreduce , avro , hdfs , zookeeper , sqoop , spring , hibernate , mysql responsibilities design the solution architecture for cdid service using edraw. involve in writing java code to read the input xml file from datafabric d_hdfs optum optumhealth commondocumentintakedistribution dq_working 20140409t055107_4075_cdd_csp_hl7_subq_from_cdid_ifw_data_484_full.xml. write mapreduce application which process each line as a xml file from input file. design and development of data ingestion process services making changes only in the schema validation step for processing of the rejects. write mapreduce for new process for schema validation write a mapreduce job to read data from cassandra and write it to cassandra from hdfs . write pig user defined functions( udf ) to insert the data into hdfs and read the data from hdfs . write pig script to filter the data and insert to hdfs . build a script to join driverid , name , hours and miles logged involved into cdid_historicalload process and update ingestion history involved schema change detection using md5 algorithm design and develop application for the analysis of the health data clubrecommendation and analytics system( craas ) for business growth in e - commerce client project duration miac analytics , bangalore november 2015 to nov 2016 project description this project deals with clubbing many application for their recommendation and perform analytics as per market needs and current demand for the e - commerce domain . this project deals recommendation of the item using collab filtering algorithm and perform analaytics : this module recommend the item while shopping based on the user rating in the e - commerce portal . this will perform analytics for rated item only and show the saled of the item in the reporting and analytics tools . we are using bestbuy.com api for testing platform . recommendation of item to improve the business sales for e - commerce portal using sbta( structural balance theory algorithm ) . this modules also perform the analytics on the offline and online data to see the increase in the sales demand for unrating product also which is listed in the portal . recommendation of the product based on microbloging site like twitter and facebook etc. here we are using microbloging site like face book and twitter . this module makes set of the on given keywords using k - means clustering and hierarchical clustering algorithm . it stream all the online data as per cluster setup and remove the stop word . image recommendation and analytics of the online image for branding and model demand about the camera or mobile based on online e - commerce portal . implementation details for image recommendation and analytics : image dataset will be sourced from the flickr , sample datasets crawled from web hadoop image processing api will be used to process the image image dataset are downloaded and imported on hadoop . mapreduce jobs will be configured and run against the image data set.all the image would be store in hadoopimagebundle( *.hib ) format in hdfs. - run the mareduce using mesos to read metadata of the hib and store into hdfs using pig . standard benchmark mapreduce jobs distributed with apache hadoop will be used to measure the performance. apache hadoop installation will be applied mesos cpu resource aware schedular and will be configured accrodingly. hibecnh performance measuring tool for mapreduce will be rerun with same set of benchmark mapreduce jobs and datasets . recommendation and analytics would perform based on metadata of the images technologies used cdh5 , java , hdfs , mongodb3.2 , , pig0.13.0 , mapreduce , mesos1.0.1 , apachenutch1.10 , sqoop , zookeeper3.4.6 , spark1.3.0 , scala2.10 , kafka - 2.10.0.9.0.1 , spring , hibernate , mysql5.6 responsibilities design the solution architecture for club recommendation client interaction and understand the requirement of the recommendation engine . setting scala , sbt and spark . setting mongdb and apache kafka write function for frequent patterns matching , people to people correlation , customer review and ratings develop recommendation from other ' s views use spark mlib for machine learning algorithms use k - means , expectation - maximization , power iteration clustering algorithm perform association analysis using frequent pattern mining ( fpgrowth ) . population the e - commerce data set from bestbuy.com and amazon use content based recommendation algorithm in scala . write higher order user defined function in scala . perform analyctics using spark exported the result to mysql using sqoop configured apache nutch and write mapreduce program in spark for the webcrwalling of the image as data use kafka for the microbloging data in the project configure mesos with hadoop and write mapreduce program to crawl the image and store as hadoop image bundle file into hdfs. write mapreduce program and pig using udf to read image metadata and store into hdfs. adding recommendation listing understanding recommendation behavior perform analytics predict shipment delivery failures company project duration client : dhl roles : big data consultant duration : may 2014 to october 2015 client location enrich - it , dlf building , gachi bowli , hyderabad - 500019 project description problems concerning increased shipment re - deliveries and customers refusing the shipments due to additional charges had become concern for dhl. the redelivery of shipments not only added costs but also consumed extra storage space in facilities for the un - delivered goods. work was conducted with objective to , predict chances of customers refusing to be delivered shipments reduce the number of multiple last mile journeys provide insights into different paradigms of failed shipment delivery technologies used mapr2.0 , sqoop1.4.6 , hive0.13.0 , rhadoop , rmr2 , rhdfs , apachesolr3.6 , flume1.5.0 , jdk1.6 . responsibilities data ingestion with sqoop i transferred bulk data related to shipments , billing , customers , global references , tracing , claims , complaints , and checkpoints [ sensor data ] to hdfs and hive designed and developed hive tables using joins , partitioning and bucketing. made the data ready for performing future analysis. execution of hive was done within spark engine analysis and reports using r programming language i performed predictive analysis on the shipments data. applied decision trees and random forest supervised learning technique to predict failure of shipment delivery using qlickview i facilitated comprehensive customer relationship management dashboards which provided the insights into different paradigms of failed shipment delivery and customer problems email analysis high volumes of customer queries in the form of emails are received by dhl. managing emails , understanding customer queries , providing timely responses , tracing , searching and storing emails became a challenge and concern for dhl. work was undertaken with objective to , perform sentiment and text analysis on the emails , using sophisticated text - analysis to discover trends in customer problems. to categorize the emails and route them to right agent queue map correlations between the unstructured data in email and transaction data. integrate with backend systems , and customer data in single place increase contact centre productivity and improve overall customer service , reduce handle and wrap time for replying to the emails , enforce service levels provision for the csa ' s to search emails on different email fields , to , subject , from , date , subject , email contents , keywords provide management dashboards to the supervisors data ingestion configured flume for fetching the emails , morphlines for transformation and storing the emails into solr , hdfs designed and developed hive queries exported data from hdfs to transaction database using sqoop analysis using r programming language i performed text analysis and sentiment analysis on the emails. created word cloud , discovered topics using lda algorithm of topic modelling hierarchical and k - means clustering were performed to understand various segments of the customer queries exported hdfs data to the transactional database using sqoop clinical integration acceleration company project duration enrich - it august 13 - april 2014 client project duration optum health may 13 - april 2014 client location enrich - it , dlf building , gachi bowli , hyderabad - 500019 project description cdid is identified as one of the source for this clinical integration acceleration project. data will be extracted from xml files for this source and will be maintained in non - relational file hub layer after enhancements and enrichments. cdid is uhcs capability for in - taking hl7 transactions. cdid consumes hl7 from ocie and transforms them in the hl7 ccd v3 format implementation details : creating two workflows to handle historical rejects. cdid_historicalload 2. cdid_processhistoricalrejects. cdid_historicalload : this workflow will have a single step that will invoke a java class. historicalload : it will get all the rejected csv files( message id , reason .. ) from reject location and merge all the rejected message ids into one file. creates a hbase table on top of the consolidated rejected message ids. cdid_historicalload workflow : this workflow has 3 steps. 4. getfilesfrom archive : 5. filteringrejects : 6. movefilesto landingzonedir : once xml , xsd and control file are moved into the landing zone directory new incremental intake workflow will be triggered. technologies used mapr2.0 , jdk1.6 , , apachecassandra2.0 , pig0.12.0 , mapreduce , avro , hdfs , zookeeper , sqoop , spring , hibernate , mysql responsibilities design the solution architecture for cdid service using edraw. involve in writing java code to read the input xml file from datafabric d_hdfs optum optumhealth commondocumentintakedistribution dq_working 20140409t055107_4075_cdd_csp_hl7_subq_from_cdid_ifw_data_484_full.xml. write mapreduce application which process each line as a xml file from input file. design and development of data ingestion process services making changes only in the schema validation step for processing of the rejects. write mapreduce for new process for schema validation write a mapreduce job to read data from cassandra and write it to cassandra from hdfs . write pig user defined functions( udf ) to insert the data into hdfs and read the data from hdfs . write pig script to filter the data and insert to hdfs . build a script to join driverid , name , hours and miles logged involved into cdid_historicalload process and update ingestion history involved schema change detection using md5 algorithm design and develop application for the analysis of the health data enrich - gloabl payment gateway company project duration ezidebit - amexcard , australia december ' april 13 - july13 client location enrich - it , dlf building , gachi bowli , hyderabad - 500019 technologies used java , hive , sqoop responsibilities high level design project deals with payment settlement of the card holder for the bank like amex card holder . hadoop hive configuration and setup. hive schema design for oracle olap tables. design develop server side component. bank - fusion universal banking ( ub ) product support project duration july 2011 to december 2011 company agewell technologies pvt ltd client location ibm , manayata project description this project involved support to onsite implementation team for various modules. technologies used java , jboss 4.0 , bank fusion platform responsibilities discussion with client and understanding problem fixing bugs. performance fixing for batch processes. bank - fusion universal banking ( ub ) integration of universal banking project duration december 2011 to november 2012 company agewell technologies pvt ltd client location ibm , manyata project description this project involved interface integration of universal banking with other misys products such as opics , ti etc and intellect flow ( polaris product ) technologies used java , jms , hibernate , jboss 4.0 , ibm web sphere mq , bank fusion platform responsibilities requirement understanding for integration of universal banking with trade innovation. high level design for financial posting from ti to universal banking. ibm web sphere mq configuration as part of messaging layer. development of accounting entries from ti to universal banking. development of universal banking - ti meridian repository using misys message manager middle ware for message transformation and routing. design and development of ub with intellect flow( polaris product ) . levvo software technologies pvt ltd project duration february 2008 to june 2011 company levvo software technologies pvt ltd roles it - trainer technologies c , c++ , core java , j2ee , struts , oracle 9i 10g , android , python responsibilities i used to train their frehser employee at the company premises i used to support the development team for the any programming issue in java i used to write plsql code in the development team i used to visit client location from company side as trainer for providing corporate training i used to take some set of workshop on technologies like java collection , plsql to the engineering college in bangalore i was guest trainer for ssi , niit , manipal academy , engineering colleges from my company . skillsadv. analytics tools statistical modeling tools r - studio , python , machine learning r , mllib , spark ml machine learning algorithm experience prediction analytics business analysis i have used the algorithm and machine learning technique 1. classification algorithm 2. svm 2. knn - query algorithm 3.k - means clustering algorithm 4.collab filtering algorithm 5.association algorithm 6.regression algorithm 7. decision tree algorithm 8.neural network algorithm experience with machine learning tools , data mining tools , spark - mlib python - scikit - learn r - cran mahut big data competencies big data technologies hortonworks , cloudera , apache hadoop real time analytic apache spark streaming , apache kafka , flink batch data processing spark , hive , pig , mapreduce data collection sqoop , flume , springxd , impala nosql technologies hbase , mongodb , cassandra , neo4j messaging framework kafka , rabbit - mq programming language python , scala and r , json cloud platform and tech. amazon aws , jelastic cloud server programming languages c , c++ , java , androidsdk3.2 rdbms oracle8i 9i , mysql5.6 mobile technologies android web technologies servlet , jsp , xml , xsl , xslt , xpath , webservices , json design tools staruml , edraw web server apache tomcat , jboss , websphere testing tools junit version control git , tortoise project management tools maven build tools ant , log4j other tools eclipse , android studio a_aproject of the quarter award for first quarter of year 2016 spot award for excellent performance for the bank fusion module. received several client appreciations for well management and delivering the project module well before the deadline. educationhighest degree year of passing board university master of science ( msc.it ) 2004 sikkim manipal university b.sc. 1999 magadh university , bodhgaya\",\n",
       " 'summarytotal 6+ years of training and teaching including 10+ months ( hadoop ) of experience .. having hands on experience in usinghadooptechnologies such as hdfs , hive , hbase , pig , sqoop , flume. having experience on importing and exporting data from different systems tohadoopfile system using sqoop , databases like mysql , oracle to hadoop. having experience on creating databases , tables and views in hiveql , and pig latin. strong knowledge of hadoop and hives analytical function. implemented different bigdata analytic tools , migration from different databases like mysql , oracle to hadoop. having experience on storage and processing in hue covering allhadoopecosystem components. load and transform large sets of structured , semi - structured and unstructured data usinghadoop ecosystem components. handle multiple files like text , tsv , csv etc. , having good experience in all flavors of hadoop ( cloudera , hortoworks ) . good knowledge of single node and multi node cluster setup good knowledge on oops concepts of core java. good knowledge in using linux commands highly motivated , subject oriented , has ability to work independently and as a part of the team with excellent technical , analytical and communication skills. good knowledge of working with tableau https : public.tableau.com profile gopal.n.biradar# ! kle bba and me : roles and contribution : implemented the sims ( student information management system ) with testing and contribution of development. installation and configuration of open source firewall clearos configuration of moodle for entrance test of applicants. guided the market research projects of pg and ug students with various industries with ibm spss as tool coordination of all college activities including it support , placements , counselling of students. responsible for processing of admission , attendance , students performance analysis including creating dashboards. organized national conference on smes in global economy as programming committee chair. organized national conference on wine industries in india , opportunities and challenges as programming committee chair. organized wine exhibition promoted by karnataka wine board. automated the process the students performance analysis using ms excel and ms word. responsible for organizing major college events and events in associations govt departments. experienceworking with getin it solutions , bangalore as a hadoop trainer from apr - 2016 worked with bba college , hubli as a senior lecturer from dec - 2009 to nov - 2015 worked with aptech computer education from dec - 1999 to till jan - 2006 a_aawarded as best paper at international conference on management of micro , small and medium enterprises( msmecon - 2011 ) at imt nagpur , titled sme scenario in goa an empirical study ( http : www.imtnagpur.ac.in news - msmecon.html ) international conference on \" soft skills \" at himalayan institutes , kala amb , hp ( 9th , 10th april , 2010 ) titled strategic solution : quality of work life international conference on \" soft skills \" at himalayan institutes , kala amb , hp ( 9th , 10th april , 2010 ) titled meetings without yawns , the best practices for effectiveness national conference challenges of depreciating indian rupee on higher trajectory of growth organised by karnatak university economics forum and kle gh college , haveri karnataka. educationbigdata training from getin it solutions , bangalore. mba ( masters in business administration ) 67 % ( 2008 ) bsc ( bachelor of science ) from kud with 51 % . ( 1999 ) diploma in computers from aptech with a+ grade strengths : passion to learn new technology quickly , try to adopt the same to our practical life energetic , hardworking , and capable to perform responsibilities under extreme pressure and time constraints. flexible , good interpersonal skill and confident , straight forward , silent observer , good follower , result oriented.',\n",
       " \"summaryaround 6 years of experience in software development and life cycle. 4 years of hands - on experience of coding and bug - solving in hadoop , pig , oozie , hive , mapreduce , python , java , mysql , kafka , hue , avro , parquet , json , cloudera solr search and elastic search. around 2 years experience in java server side mobile applications. experience working on cloudera and apache distributions. thorough understanding of the software lifecycle and services. experience on agile projects. worked in co - ordination with client and cross geographical teams. experience of mentoring juniors. good in documentation. experiencepossess around 5.10 years ( includes 7 months internship ) experience in java and big data development. may 2016 till date with saama technologies , pune as a consultant. march 2015 to may 2016 with allstate solutions private limited , pune as senior it consultant. jan 2011 to march 2015 with persistent systems , pune as senior software engineer. 4 years as hadoop big data developer. projectfae - fluid analytics engine ( saama technologies ) description : working as a consultant for r and d engineering as part of fae ( fluid analytics engine ) team. fae is a product or solution from saama technologies involving various tools and technologies from hadoop or bigdata stack and has modules for visualization , data sciences and data ingestion and transformation. scope : design , develop , maintain and document code for new features added to the product. solution design for new enhancements. implementation of scope through coding and documentation. present it to stakeholders. responsibility : implement the scope defined for the enhancements to be added. solution designing , development , maintain and documentation of the enhancements. working in close co - ordination with other team members. test cases review and unit testing of code. duration : may 2016 till date technologies : java 1.7 , cloudera distributed hadoop 5.4.4 , hdfs , json , rest webservices , shell , elastic search , svn and jira. product : fluid analytics engine eds hadoop centre of excellence ( allstate solutions private limited ) description : defining standards for new hadoop projects , technology vetting for new hadoop technologies , support hadoop projects onboarding , operationalizing hadoop environment , solution designing and supporting hadoop application teams. individual contributor on a project involving cloudera search for competitive analytics team. scope : technology vetting of new tools or technologies. design , develop , maintain and document code for writing tools to support internal environment. solution design for new projects. implementation of scope of projects through coding and documentation. present it to interested parties. responsibility : setting up offshore hadoop team for allstate. technology vetting on various technologies or tools. defining project standards for hadoop projects. individual contributor on various tools to support internal environment needs. design , develop , maintain and documentation for these tools. writing test scripts to ensure smooth operationalization of environment after upgrades. co ordination and support cross - regional teams. mentoring new team members. duration : march 2015 may 2016 technologies : java 1.7 , cloudera distributed hadoop 5.4.4 , hdfs , pig , oozie , mapreduce , json , hive , avro , parquet , perl , shell , python , kafka , cloudera search , svn and jira. client : allstate insurance corporation ( usa ) yahoo ! direct ( yahoo ! usa ) description : yahoo ! direct is a project that generates an inventory of users who are to be targeted for email marketing mails and are likely to respond to the mails. the users are targeted for various marketing mails based on their interests and preferences. scope : generate an inventory of potential users to be targeted. generate a list of users who have updated their newsletter subscriptions. generate various feeds and reports using the response data. responsibility : to write pig scripts and udf ' s to implement the above scope using hdfs , hadoop and oozie. individual contributor on a component. generate various data feed pipelines needed in the generation of inventory. modify the code as per enhancements requested by client for fixing the bugs. migration of various feeds from one cluster to another. co ordination and general follow up with client. mentoring junior team members. duration : june 2013 march 2015. technologies : java 1.6 , hadoop , hdfs , pig , oozie , mapreduce , mysql , hive , perl , shell , python , rest webservices , svn , bugzilla and rhel 5.5 client : yahoo ! ( usa ) grid bootstrap process ( yahoo ! usa ) description : bootstrap purpose is to generate dumps that are used to create mdbms ( memory based database manager ) .each extract goes through all or some of following stages : format , join , partition , split and merge. these stages are used for filtering , applying business logic , output formatting etc. scope : migration of rollup scripts from perl to oozie workflow using hdfs apis and mapreduce. auditing the data for validation of data. responsibility : to decode fairly complex perl logic and implement the same in java. write oozie workflows using hdfs apis , mapreduce , hadoop and oozie. auditing the data generated to ensure data validity. duration : october 2012 to june 2013 technologies : java 1.6 , eclipse , linux , hadoop , hdfs , mapreduce , oozie , perl , svn and bugzilla. product name : powrollup client : yahoo ! ( usa ) oneclick ( sprint usa ) description : oneclick is sprints new user interface. the objective is to provide a ui that allows access to as many features as possible in as few key presses as possible. the ui provides customizable up to date information on a variety of topics such as news , weather and stocks. oneclick consists of a client which is preinstalled on the handset. responsibility : simulate , analyze , fix , test and follow up for the issues identified at production site. release the enhanced code patched to client with proper documentation. modify the code as per enhancements requested by client for fixing the bugs. co ordination and general follow up with client. duration : june 2011 to october 2012 technologies : java 1.5 , xml , eclipse , memcached 1.2.5 , tomcat 5.5.23 , apache2.2.8 and perforce. product name : oneclick xui client : sprint ( usa ) mpm - mobile portal manager ( sprint - nextel usa ) description : mpm is a suite of applications that renders homepages for mobile handsets dynamically. nextel content manager creates a default home deck that is available to all subscribers based on the services that they have purchased. mpm application provides the following functionality to its end - users : * homepages : the homepages are rendered in device specific language. * bookmarks : subscriber can store , edit and view bookmarks from their handsets. responsibility : simulate , analyze , fix , test and follow up for the issues identified at production site. release the enhanced code patched to client with proper documentation. modify the code as per enhancements requested by client for fixing the bugs. co ordination and general follow up with client. duration : june 2011 to october 2012. technologies : java 1.5 , eclipse , rhel , perforce , webservices , oracle 9i , struts 2 , servlets and jdbc. product name : sprint - nextel mpm client : sprint ( usa ) project #7 email mx ( openwave systems usa ) description : mailing system provides an aws s3 interface to cassandra. the idea was to create an interface for aws s3 ( amazon web services simple storage service ) . it provided the apis to support backend operations such as creation of mailbox. responsibility : implementation of above scope. coding , documentation and support. modify the code as per enhancements requested by client for fixing the bugs. co ordination and general follow up with client. duration : jan 2011 to july 2011 technologies : java 1.5 , cassandra , aws s3 , jsp , servlets , html , tomcat 5.5.23. product name : emailmx client : openwave systems inc.( usa ) skillshadoop , hdfs , pig , hive , avro , parquet , oozie , mapreduce , sqoop , kafka , elastic search , solr cloudera search and hue. mysql and oracle 9i. basic perl , shell and python. core java , servlets , struts 2 , java webservices and jdbc. windows and linux rhel. jenkins , perforce , svn and git. tomcat 5.5 and tomcat 6 bugzilla and jira. a_abravo ! from client in recognition of perseverance on pow rollup task in grid bootstrap bootstrap project. you made a difference award twice for yahoo ! direct recognizing the contributions made towards multiple components. educationmaster in computer applications ( mca ) with 64.80 % from indira institute of management ( 2011 ) . bachelor of science ( computer science ) with 63 % from symbiosis college ( 2008 ) . ssc with 72.26 % from erin nosh nagarwalla day school ( 2003 ) . oracle certified java professional ( ocjp java se6 ) . cloudera trained hadoop professional.\",\n",
       " 'summaryaround 5 years of real - time experience on developing the enterprise applications using java j2ee and scala apis. around 3 years of experience in bigdata [ hdfs , mapreduce , hive , pig , sqoop and spark ] . extensive knowledge on aws cloud [ emr , ec2 and s3 bucket ] services. experience in performing data storage operations on aws redshift and cassandra. good knowledge on messaging system like jms and kafka [ direct streaming and batch processing ] . experience in implementing spring , rest , hibernate and clear2pay opf frameworks. experience in involving the software development life cycle ( sdlc ) including design , development , documentation , testing , deployment , version control and production support. ability to learn and adapt quickly to the emerging new technologies. experienceemployer designation duration securonix sr hadoop developer may 2016 till date wipro tech sr. software engineer july 2015 may 2016 infosys ltd technology analyst aug 2014 june 2015 hcl technologies software engineer jul 2011 aug 2014 wipro tech , bangalore , india oct 2015 till project : integration keys [ ik ] , capital one financial corporation role : module lead description : development of ik migration is to deliver the data analysis on payment history file for the daily basis using spark. the spark ik results will be stored in hive table and s3 bucket for analysis. responsibilities : requirement gathering and analysis. implemented receiver base spark streaming for the ingested file. implemented the transformation and actions for the ingested file. with the spark migration , speed has increased 30x faster. implemented spark sql with data frames to process join operation on 2 input files. integrated with aws s3 and redshift bucket to store the end results. configuration of emr cluster for spark. skill : scala , spark [ sql and streaming ] , aws [ redshift , s3 bucket and emr ] . wipro tech , bangalore , india july 2015 sep 2015 project : file broker agent , capital one financial corporation role : module lead description : development of dde file movement is to transfer the files from dde servers to hdfs and aws s3 bucket. to trace the file movement integrated kafka to send the transfer status. responsibilities : implemented dde file movement to hdfs using hadoop java api. developed utility for hdfs file movement to s3 bucket using amazon api. implemented kerberos authentication. developed kafka producer and spark consumer service. implemented ui analysis for the for the file transfer status. written j - unit test cases for the module testing. skill : java , hdfs , kafka , amazon cloud [ ec2 , s3 , redshift and sqs ] , kerberos , spark streaming , angularjs , spring mvc. infosys ltd , bangalore , india aug 2014 june 2015 project : apple watch , apple inc. role : senior developer description : development of mr job analysis for the several apple watch models based on the customer reviews from the multiple sources. responsibilities : implemented the apple watch review analysis using spark framework. developed utility to move result to hive using spark - sql. implemented the spring and rest framework for the displaying results in ui. implemented j - unit framework to test the mr app. involvement in production support. skill : java , hdfs , mapreduce , spark , hive , spring , json and rest. hcl technologies , bangalore , india jan 2014 - aug 2014 project : singapore g3 - bulk payments , cimb group role : senior developer description : development of g3 bulk payments system of singapore aims at enabling bulk funds transfer for credit transactions between member banks participating in g3 in an stp mode. there is no limit for credit transfer bulk transaction. responsibilities : developed the different layers in order to process the core payments using ejb , jms and hibernate. optimized the code and hibernate queries for maximum performance. involved in requirements gathering and preparing software requirement specification. prepared the class and sequence diagrams. developed the scripts for the configuration of queues in ibm mq. developed the quartz jobs for the monitoring batch payment files to process. prepared the lld for the module. developed utility for the importing data from oracle to hdfs using sqoop. worked on the poc for the data processing using mapreduce. worked on hive for the storing and analysis for the structured data. skill : java , ejb , rest , hibernate , clear2pay opf , jms , quartz jobs , sqoop , mapreduce and hive. hcl technologies , bangalore , india aug 2011 - dec 2013 project : bwa clearing and settlement , cba australia role : java developer description : development of phbwa is to serves the bank of western australia customers online payments. since bank of western australia has been acquired by cba , all the transactions going out and coming to bwa are to be routed through cba. eventually actual settlements happen inside cba. responsibilities : developed the service provider framework ( spf ) to reuse the payment services. implemented the internal mq utility to send the produce consume the messages. developed payment service with clear2pay opf. implemented the exception mechanism to handle the different layers exceptions. optimized the java code and hibernate queries for maximum performance. prepared the class and sequence diagrams. developed the scripts for the configuration of queues in ibm mq. developed the quartz jobs for the monitoring batch payment files to process. have good sql coding knowledge. writing junit test cases for testing the application. skill : java , ejb , hibernate , clear2pay opf , jms , ibm wid , ibm mq explorer and quartz jobs. skillsbfsi payments , cards , retail programming scripting languages java , scala and python big data hdfs , mapreduce , hive , sqoop , solr , spark [ streaming and sql ] and kafka cloud aws [ ec2 , s3 , redshift , emr and sqs ] nosql cassandra jee j2ee technologies jsp , ejb , jms , spring , hibernate jax - rs and jax - ws servers ibm web sphere and tomcat 7 build maven , ant sdlc agile ide ibm wid and eclipse educationbachelor of technology from scsvmv university. 2011 with 8.7 cgpa.',\n",
       " \"summarya java j2ee developer with a total 6 years of corporate experience. have experience of working in product from scratch. have 10 years b1 us visa. have worked on various projects in supply chain domain and healthcare domain. having hands on experience in handling complex business requirement. good technical and functional knowledge of development and customization. development and delivery of formal team training sessions implemented new functionalities and providing fixes for critical and high priority change requests on time and with minimal defects. excellent communication , presentation , client interaction skills and involving client calls to gather requirements and weekly status calls and meetings. comfort working with both waterfall and agile methodologies. determine to develop the best product functionalities and create solutions that satisfy and exceed client expectations. reputation for providing superior user support and going the extra mile to resolve difficult problems. fast learner known for delivering results quickly. experiencerole : sr. software engineer company : impetus infotech ( india ) pvt. ltd duration : from august 2014 till date role : it developer company : hewlett packard globalsoft pvt ltd duration : from july 2010 to july 2014 projectdocasap - ihub company : impetus june 15 to till date designation role : sr. software engineer platform and skills : eclipse luna , active mq , mysql , java , j2ee , spring framework , apace tomcat 8 webserver , hapi library , hibernate , jenkins. brief : docasap integration hub system facilitates association of docasap appointment system with 3rd party electronic patient management ( epm ) platforms like allscripts , athenahealth , ge cenctricity and nextgen etc. following are the features required for the integration hub system : docasap system integration with 3rd party applications scalability and expandability to support approx. 1400 third party systems integration with three 3rd party applications as allscripts , athenahealth and nextgen. categories of third party systems api based ( push and pull ) messaging ( hl7 ) direct db access baseline ( schedule and appointment data ) synchronization appointment synchronization scalable and highly available solution my key responsibilities were : understand the requirements ( both functional and non - functional ) by going through the specifications and with inputs from business in order to participate efficiently in the design , development and testing phases of the project. understanding the system domain and architecture web architecture design and core development development of web framework component for data validation and processing. understanding of existing messages format like hl7 messages and communication between different modules converting them in json setting up workspace , softwares for all the modules and setting up interwar communication between them working on new modules following best practices of coding , process fixing functional and performance defects 8x8 - streamanalytix company : impetus april 15 to may 15 designation role : sr. software engineer platform and skills : eclipse luna , kafka , storm , rabbit mq , couchbase , postgress , mysql , java , j2ee , spring mvc , apace tomcat 8 webserver brief : 8x8 is a customize application for telecom domain built on stream analytix , which is a product of impetus. 8x8 analytix is an application to collect the data in near time and identify the following information from log data : end to end call flow by collecting events of a call. calculate multiple filters on near time and provided search options. show the top dominant and ivr path. my key responsibilities were : understanding the system domain and architecture applying filters on data. working on new modules following best practices of coding , process fixing functional and performance defects big data practice company : impetus infotech ( india ) pvt. ltd. jan 15 to march ' 15 designation role : sr. software engineer platform and skills : java , j2ee , hadoop , hive , nosql , datastax enterprise , kafka , spark brief : it involves researching optimum solutions for a given big data problem using hadoop , zookeeper , datastax enterprise , hive , spark and other related technologies. the key objective is to arrive at a solution that is efficient and within the expected sla. cloud - trial company : impetus dec 14 to jan 15 designation role : sr. software engineer platform and skills : eclipse luna , rest easy , derby 10.11 , apace tomcat 8 webserver , java 1.7 , jaxb brief : cloud - trial is an add - on feature of stream analytix , which is a product of impetus. it allows user to get a free trial for a particular period of time in order to support marketing and customer service activities. it will also give a user directory on ftp where can upload their data and use it for further analysis. my key responsibilities were : web architecture design and core development. development of web framework component for provisioning of stream analytix components on cloud for user trial. participating in design discussions , analyzing the problem statement to understand the key points developed the code and thoroughly tested it. unit tested the application. optum stingray company : impetus sept 14 to nov 14 designation role : sr. software engineer platform and skills : java 1.7 , rest easy services , derby 10.11 , jboss data virtualization , hive , jaxb. brief : stingray is focused at developing consistent processing for data quality that can drive quality execution downstream. the data from multiple sources needs to be standardized and made consistent before being meaningful information can be derived out of it ; stingray provides a platform for data standardization and for applying data validation and filtering checks in a consistent format. my key responsibilities were : understand the requirements ( both functional and non - functional ) by going through the specifications and with inputs from business in order to participate efficiently in the design , development and testing phases of the project. development of web framework component for data validation and processing. developed the code and thoroughly tested it. ldap authentication. worked on defect fixing. supplier market place company : hewlett packard oct 13 to jul 14 designation role : it developer platform and skills : java 6.0 j2ee ( struts 1.3 , hibernate 3.0 orm apis ) , ms sql server 2012 , hpit cloud paas ( tomcat 6.0 , apache webserver ) brief : the sarbanes - oxley module enables companies to comply with the requirements of the sarbanesoxley act of 2002. with this application a company can define control activities , distribute action plans to employees for completion , and track the completion of control activities. my key responsibilities were : understand the requirements ( both functional and non - functional ) by going through the specifications and with inputs from business in order to participate efficiently in the design , development and testing phases of the project. prepared , executed and documented the unit test cases and technical design documents. worked on documenting the interfaces used in our application. creating unit test plans in order to develop validate maintain the application as per the requirements. worked on defect and bug fixing. smart buy application company : hewlett packard jul 13 to feb 14 designation role : it developer platform and skills : core java 5.0 j2ee ( ariba framework , jdbc ) , oracle database 10g brief : smart buy is hps standard procurement application for ordering non - production goods and services. it automates the requisition , approval , purchase and payment processes and enablesemployees to easilypurchase the items they need through anintuitive and efficientuser interface. smart buy is one component of the gp requisition - to - payment ( r2p ) platform which strengthens compliance and controls , thereby decreasing procurement costs. i was point of contact for russia deployment team to identify and provide queries for data extraction solution. my key responsibilities were : worked on bug fixes and sdr items. worked on the business requirements for new interfaces. worked on tech refresh. prepared , executed and documented the unit test cases and technical design documents. analyzing and fixing the defects raised by the users. providing maintenance and development support for the application. providing different customization on the basis of user needs. estimating the time lines for new enhancements and defects. smart labor application company : hewlett packard. jul10to jul13 designation role : it developer platform and skills : java , j2ee ( emptoris framework , jdbc ) , oracle database 10g brief : smart labor streamlines the process ofagency contractor labor through the use of a single on - line tool which handles all tasks throughout the lifecycle of labor engagements. the smart labor system also provides visibility to all contractor engagements , ensuring they are managed in accordance with negotiated contract rates and terms as well as hp labor policies. my key responsibilities are : understand the requirements ( both functional and non - functional ) by going through the specifications and with inputs from business in order to participate efficiently in the design , development and testing phases of the project. worked on documenting the interfaces used in our application. creating unit test plans in order to develop validate maintain the application as per the requirements. worked on defect and bug fixing. worked on production issues. employee engagement activities conducted and volunteered in many events organized in company. planned and organized team building activities requested by teams in company. hosted hp global social innovation day event in hp. anchored engagement activities during quarterly all employee coffee talks. volunteered in event hp hackathon event which won 1st price in hp social initiatives. skillsprogramming core java , sql , html , javascript , spring core , spring mvc , jaxb , jms , maven , shell scripting , ant , hadoop , hive , kafka , spark streaming , active mq rdbms : rdbms ( mysql , oracle ) , nosql ( cassandra ) database connectivity : jdbc , hibernate operating system : windows , unix , ubuntu ide : eclipse , toad , derby experience in analysing the business requirements and functional requirements to formulate and implement design. involved in creation of functional specifications , technical specification , business requirements analysis , flowcharts and process maps pertaining to my project. hands on experience in requirements gathering , analysis , detailed requirements design ( use cases ) , workflow design , functional ( application ) testing. strong analytical and documentation skills problem solving and communication skills strong client interaction skills a_areceived client appreciation on implementing hl7 interface for message communications. received appreciation for implementing ihubdata independently for docasap project. secured grade a in kalaratan award. 2002 participated in festival of asian childrens art. 1997 - 99 captain of school kho - kho team and leads the team to ist spot in inter school competition of kho - kho. 2002 successfully performed the role of event coordinator for the pack - rat in intercollege fest elan - 07 at sdbct , indore. 2007 participated in various painting competitions educationbachelor degree of engineering in electronics and communication with an aggregate of 73.62 % ( 2005 - 2009 ) senior secondary with 72.00 % ( 2004 - 2005 ) . higher secondary with 67.2 % ( 2002 - 2003 ) .\",\n",
       " \"summaryover 6+ years of experience in the field of it including 2.6+years of experience in hadoop eco - system environment and java and j2ee applications. experience in core java , spring , hibernate , jpa , hadoop hdfs , mapreduce , hive , pig , oozie , hbase , ibm bluemix , flume and mongodb zookeeper. experience in web services , servlets , jsp , struts , junit , log4j and maven , sqoop scripts having good knowledge of spark with scala , kafka and cloudentdb experience in web technologies using core java , j2ee , servlets , jsp , jdbc , java beans , and design patterns. experience in mvc technologies struts , spring mvc , hibernate . experience in developing j2ee applications using various persistence frame work ( hibernate ) and implementing jpa ( java persistence api ) . experience in load and transform large sets of structured , semi structured and unstructured data using hadoop eco - systems hive , pig and hbase. experience with oozie work flow engine in running work flow jobs with actions that run hadoop map reduce and pig jobs good analytical , debugging and testing skills. ability to adapt new technologies quickly. working as application developer for ibm india pvt ltd from dec 2014 to till date. worked as sr. software engineer for collabera from nov 2012 to dec2014. worked as software engineer for bhilwarainfo from jun 2012 to nov 2012. worked as software engineer for source one from aug 2011 to jan 2012. worked as software engineer for harjaicomputers from dec 2009 to july 2011. projectrole : software engineer environment : jsp , core java , spring , ibaties , hdfs sqoop scripts and mapreduce. database : db2 , mysql tools used : my eclipse and svn client : sears ( u.s ) duration : jun 2012 to nov 2012 description : the application is maintained the prices of items of sears , kmart physical and online stores by dynamically. the application maintains the promotions of the offers of particular intervals. the blocked items by store model in addition to the current ability to block items by format. this will allow the business more granular control of price recommendations for given items across the geographic dimension. price team uploads a spreadsheet indicating adds or deletes to the block item repository. the items can only be blocked at the format level. the new change to current ability to block at format , the pricing team will also be able to block at the model level. price hub application change home screen with suggestion functionality , which will allow the user to view the suggestions accepts or rejects them. dynamic pricing ( dp ) system comp shop user upload enhancements competitors pricing obtained from third party into dp.the business units driving this enhancement are automotive , food , and drug etc. responsibilities : worked on analyzing hadoop cluster using different big data analytic tools including hive and mapreduce , pig and oozie. involved in loading data from linux file system to hdfs. implemented partitioning , dynamic partitions , buckets in hive. involved in scheduling oozie workflow engine to run multiple hive and pig jobs. exported the result set from hive to mysql using shell scripts. involved in developing pig udfs for the needed functionality that is not out of the box available from apache pig. involved in processing ingested raw data using mapreduce , apache pig and hive. importing and exporting of data from rdbms to hdfs and vice versa using sqoop. analyzed the data using pig and written pig scripts by grouping , joining and sorting the data. load and transform large sets of structured , semi structured and unstructured data. developed pig latin scripts to extract data from the web server output files to load into hdfs. responsible for creating hive tables , loading data and writing hive queries. handled importing data from various data sources , performed transformations using hive , map reduce , and loaded data into hdfs. extracted the data from db2 to hdfs using sqoop exported the patterns analyzed back to db2 using sqoop project 5 online merchant system ( oms ) role : sr. software engineer environment : jsp , js , core java , spring , struts , stored procedures. database : db2 tools used : rad , apache maven , and svn client : american express ( u.s ) duration : aug 2011 to jan 2012. description : oms intl : migration of oms intl application from was3.5 to was5 application server ( including migration from frontservlet to jpf spring based design ) . japa disputes : upgrade of disputes and homepage look and feel for japa and add functionalities like case updates and chargeback response similar to domestic and emea. lac disputes : upgrade of disputes and homepage look and feel for lac and add functionalities like case updates and chargeback response similar to domestic and emea. global disputes : add disputes functionality in following three markets : singapore , hong kong , thailand , completely revamp disputes functionality in mexico and argentina markets and add similar functionality to australia and new zealand markets. responsibilities developed spring beans , worker classes and configuration files. developed the jsps for developing the screens. extensively involved in writing the stored procedures. understanding existing business model and customer requirements. responsible for doing the code changes in the development and production environment. checking the test environment ( build , database changes and links ) before forwarding the build to system integration testing and uat. interact with the sit and uat team to reproduce and solve the bug. skillslanguages : core java , sql. web development interfaces : servlets , jsp , struts , hibernate and spring ide and analytical tool : eclipse , splunk , junit , cvs , svn and toad. databases : oracle 9i , mysql and db2 database connectivity interfaces : jdbc , ibaties , jpa , jndi. web server : apache tomcat 5.0. application server : web logic 10.3 , web sphere6.1 , jboss4.1 operating systems : windows nt 2k and unix. cloud computing : big data hadoop , hdfs , hive , pig , zookeeper , oozie , hbase , flume , spark and ibm bluemix. project 1 : regional mobile banking taiwan role : application developer environment : jsp , core java , spring , hibernate with jpa and bluemix. hdfs , hive , pig , zookeeper3.3 , oozie and spark database : oracle11g , db2 and mongodb tools used : eclipse , soapui3.6.1 and svn application server : ibm bluemix , websphere 6 client : development bank of singapore ( dbs ) duration : feb 2015 to till date . description : development bank of singapore ( dbs ) is one of the major clients of ibm corporation under banking and financial sector. dbs bank has the largest network of branches in singapore and branch networks in the key asian markets of hong kong , indonesia , china , india and many others. regional mobile banking dbs ( taiwan ) : rmbtw is a regional mobile banking app for taiwan dbs customers wherein with the top notch services available in pre - login and post - login , its a highly secured mobile app. pre - login services includes atm branch locator , market watch , asian insights etc. and post login services includes account summary , transaction history , unit trust summary and history , fund transfer , purchase and redeem unit trust and other major banking functionality just a mobile click away. this app is developed for android , iphone , ipad rich clients as well as for android thin client. admin module enables the user to manage and maintain the various services provided by the mobile banking system. this project provides access control to mobile banking admin module users based on the needs of mobile banking system. admin module users can create , modify , approve , reject and remove the options services provided by mobile banking system to mobile banking users. liquid : liquid is dbs - ibm internal web based application ( poc ) for solution proposal system where ibm developers will provide solutions for dbs change requests ( crs ) and dbs reviewers will review the solutions provided by ibm and provides rating for the multiple solutions. liquid consists 4 modules as 1 ) dbs admin 2 ) ibm admin.3 ) ideas and solutions.4 ) review and rating. ibm survey : ibm survey is a bluemix based web application ( poc ) for taking the feedback on the services provided by dbs from its customers. responsibilities : responsible for running hadoop streaming jobs to process terabytes of xml ' s data. load and transform large sets of structured , semi structured and unstructured data using hadoop big data concepts. involved in loading data from unix file system to hdfs. responsible for creating hive tables , loading data and writing hive queries. handled importing data from various data sources , performed transformations using hive , map reduce , and loaded data into hdfs. extracted the data from db2 into hdfs using the sqoop. exported the patterns analyzed back to db2 using sqoop. installed oozie workflow engine to run multiple hive and pig jobs which run independently with time and data availability. involved in development of liquid and ibm survey web applications for ibm. deployment of liquid application on spark with scala on ibm bluemix cloud environment. developed the jsps for developing the screens of ibm survey application. involved in creating mongo database for json data and implemented dao layer for ibm survey application. as responsible for doing the development and deployment activity for internal app ' s liquid ibm survey on cloud environment on bluemix and mongo db. developed multiple mapreduce jobs in java for data cleaning and preprocessing. assisted with data capacity planning and node forecasting. collaborated with the infrastructure , network , database , application and bi teams to ensure data quality and availability. monitored workload , job performance and capacity planning using cloud era manager. installed and configured flume , hive , pig , sqoop and oozie on the hadoop cluster. used flume to collect , aggregate , and store the web log data from different sources web , unix servers , mobile and network devices and pushed to hdfs. project 2 : integrated virtual labor market ( ivlm ) role : sr. software engineer environment : jsp , js , core java , spring ioc , hibernate and jap , web services database : oracle11g tools used : eclipse , soapui3.6.1 and svn application server : apache - tomcat - 7.0.34 client : accenture internal ( h and ps ) duration : jun 2013 to dec 2014 description : the accenture ivlm asset offers a fully integrated solution for individuals , employers and caseworker.ivlm asset implements market - leading job matching technology , allows multi - channel access , and efficiently. ivlm asset supports employment service and recruitment processes as well. ivlm asset caseworker facilitator helps for jobseekers ( candidate looking for a better job ) and employers ( companies looking for right candidate ) . caseworker act on behalf of jobseeker upon request. ivlm asset has 4 type of user to the application individual unregistered ( guest ) registered ( jobseeker ) employer case worker admin virtual labor market : 1. one - stop government solution with complete overview of the labor market 2. highly efficient business processes in job placement , job counseling , re - up - skilling and recruiting 3. fast and precise job placement reduces duration of unemployment and fast - tracks recruiting 4. the ivlm solution can act as a common self - service platform for all actors of the labor market , and accelerate job market balancing through increased flexibility and transparency responsibilities : analysis of new business requirements and change requests. developed contract wsdl for soap web service. generating stubs using apache cxf - web services. implemented service and dao interface layer and configuration files. developed spring beans , utill classes and properties files. integration with data services ( wsdl ) entity services ( wsdl ) involved with sit and uat team to reproduce and solve the bug. creation of technical detail design , unit test plans and executing them. project 3 accenture health benefits exchange ( ahbx ) role : sr software engineer environment : jsp , js , core java , spring mvc port let , hibernate , database : db2 , mysql tools used : my eclipse and svn client : u.s state government duration : nov2012 to may2013 description : health insurance exchange is an information technology system for organizing the health insurance marketplace to help consumers and small businesses shop for coverage in a way that permits easy comparison of available plan options based on price , benefits and services , and quality. exchanges will employ a single , streamlined eligibility process that will serve as the central point of access for individuals and small groups shopping on the exchange , including individuals who receive advance premium tax credits , and for individuals receiving medicaid.accenture health benefits exchange ( ahbx ) , an aca compliant , cots solution , based on soa based apsp platform is accentures solution for health insurance exchange implementation. ahbx solution has been divided into the following functional areas shop employer , employee individual , administration , financial management and support services. responsibilities : developed spring controller , service , dao classes and configuration files. developed spring beans , utill classes and properties files. developed the jsps for developing the screens. involved in writing functional design documents and technical deign documents. interact with the sit and uat team to reproduce and solve the bug. responsible for doing the code changes in the development and production environment. analysis of new business requirements and change requests. educationmaster of computer application ( mca ) from kakatiya university.\",\n",
       " 'summaryi believe that you are always a student , never a master. i believe in learning and love for exploring new avenues. i am very passionate about working in team to solve problems. i have a good track record of solving problems and finding optimized technical solutions. my greatest asset is my flexibility and \" can - do \" attitude. i am multi skilled in several areas of oracle , unix , informatica , sql. i am oracle certified associate with strong knowledge of plsql. i have performed self - learning and gained good understanding of mongodb , with basics of hadoop , python , sqoop , mongodb and more data analysis technologies like mapreduce , hbase , hive , pig , spark , impala etc. i am result - oriented , analytical and articulate senior software engineer who is focused on \" end - to - end \" problem solving attitude with strong design and integration using gathered knowledge. i am a good team player , with good analytical skills , a very enthusiastic type of person , reliable and organize ; with excellent customer service and strong communication skills. persistent in motivating and helping colleagues ; focused efforts to achieve the goals of the team and accomplish task in a fast - paced environment. experiencesr. software engineer may 2014 to present accenture services pvt ltd 1. client : bank of italy ( september , 2015 - present ) ( accenture ) role description : plsql developer ( db poc india ) worked as an only oracle plsql developer resource for this client from the day i joined project. this is an banking services project which has dual combination of java ( dojo ) and oracle plsql. complete backend works on plsql. this application is used in banks of italy named for daily work of bank and routines of bank. key points : heavy code generation for banking client of italy , with various code reviews , co - ordination with peers and onshore teams to get zero code defects in development. exposure to management role of assigning the tasks , hours allocation and maintaining the budget for team. delivered almost 9 packages and more than 25 stored procedures and function successfully with 5 % of defect ratio. working as a critical resource for this project. nominated and successfully cracked the business continuity management assessments for our development unit being a critical resource. successfully working as a shared resource in another project for plsql development in same du. successfully delivered a sb module of banking in italy with less then 5 % of defects in plsql coding. 2. client : tia - zfs ( may 2014 june 2015 ) ( accenture ) role description : plsql developer. worked as a core plsql developer in an insurance domain for latin america client like brazil , argentina and venezuela. project was of tia application which is an insurance application used in sale or renew or modify various policies of health and motor. this was designed in oracle forms and backend was plsql and other multiple technologies. with the help of plsql packages the actual code of policy creation , premium calculation , claims , financial and multiple tariff related activities were designed. key points : understanding the change request received from client and relationship managers. scheduling transition calls with business analyst to understand the requirement if any confusion ; raising queries if any issues. maintain the code versioning in tools like clearcase. raising a clearquest request for deployment build teams. co coordinating with testers to check the running functionality. worked in code versioning tools like clearcase , clearquest to check in and checkout the codes. worked on informatica for data and pdf file generation. good understanding of all test phases like ut , st , sit , uat. cross trained my team in technology like docpath. also cross trained maintenance with the code and functionality , which was developed by me. i was the whole and soul of brazil development for the technology called docpath which was newly learned by me in short time. successfully delivered more than 10 crs and two go live activities. before accenture i had worked for 3 years in allscripts healthcare solution as technical analyst for database migration projects. 3. client : us medical practitioners. ( may 2011 april 2014 ) ( allscripts ) role description : plsql developer. worked as a sql developer or db migration analyst in a healthcare domain for us clients. there are around 150k client for whom our team use to work. design of migration tool was developed where all the plsql and sql packages were executed to migrate the data of clients from one db to other. key points : performing check of database tables for various parameters related to allscripts application. development of database migration wizard and incorporating various procedures in the wizard so as to perform migration on database server. retrieving data from informix database , oracle database and loading into sql server 2008 and oracle as per requirement criteria of clients. analyze the error if reported by consulted team and resolving the issues with database wizard and etl packages. generating various report from database for checking data quality. working as l2 ( level 2 ) role for current project. interacting with resource consultants in u.s regarding various issues faced during project lifecycle. skillsexcellent proficiency in oracle sql plsql 9i , 10g , 11g , 12c( introductory ) , informatica 9.1 rich knowledge of relational database design concepts , coding and debugging plsql functions , triggers , procedures , packages and have good hands - on in the same. have rich experience in hands on oracle development , producing high quality plsql code on oracle 11g databases. responsible for qa and uat testing for development work participates in on - call rotation and provide first line after hours support as needed. excellent understanding of development for banking application. good understanding in combination of java and oracle plsql for banking domain. extensively used oracle built in dbms packages like utl_file , utl_mail , dbms_schedular etc. command on database designing ( referential integrity primary key , foreign key , constraints etc. ) . worked on composite variables , reference cursors , multiple insert , functions , procedures , packages and triggers in pl sql to implement business logic. developed map reduce programs for data cleaning and preprocessing responsible for creating hive tables and implementing adhoc queries using apache hive. responsible for implementing pig scripts for filtering , aggregating , sampling and processing the data experience in working with different data sources like flat files , xml files and databases. responsible for handling data flow source data store to hdfs done various performance optimizations like using distributed cache for small datasets , partition , bucketing in hive have very good understanding with hdfs and hands on experience on hadoop architecture and various components such as hdfs , job tracker , task tracker , name node , data node and map - reduce programming paradigm good knowledge on hadoop cluster architecture and monitoring the cluster. good understanding of unix commands ( grep , top , ps , cut etc ) . worked on reference cursors , gtt , utl_file , utl_mail , dbms_schedular etc. good understanding of hadoop , mongodb , python , sqoop , pig , hive and have confidence of working in these technologies. gaining master level proficiency in writing complex codes in java which are required for big data hadoop platform. good knowledge of docpath which is a tool for pdf generation and widely used in latin american countries. big data ecosystem hadoop , mapreduce , hdfs , hive , pig , sqoop. operating system linux ( ubuntu , redhat ) , windows nt , windows 95 98 2000 xp 7. programming languages java , map reduce , hive , pig , unix shell scripting , pl sql , sql. databases and tools oracle , sql server , mysql , nosql ( hbase , mongodb ) other tools sql * plus , sql developer , eclipse , winscp , putty , clearcase , clearquest.',\n",
       " 'summaryhaving total 8 years of experience in design , analysis , coding and implementation of enterprise applications. extensively worked on projects involving java j2ee , node 4. having 2 years of experience in node js. having 3 years of experience in big data and good knowledge in apache storm - 0.9.1 , apache hadoop , apache cassandra , apache pig , apache solr , quartz scheduler , spam assassin and hector client api. worked on web based application using core java , servlets , jsp , struts , hibernate , spring 3.0 , mvc architecture and j2ee design patterns. quick learner and excellent team player having ability to meet tight deadlines and work under pressure. strong in communication and have good interpersonal skills. highly motivated and committed to the work assigned. ability to work as a team leader and confident in serving the best results experiencecurrently working as a software engineer with quadrant4 software solutions pvt ltd , chennai since mar 21 2012 to till date. worked as a software programmer with zoniac software pvt ltd , chennai since feb 09 to feb 12. quadrant4 software pvt ltd , as software engineer. #1. project name : adchain.io client : fusion seven duration nov 15 to till date team size : 14 environment : linux , windows xp scripting : nodejs analytical : apache hadoop 2.2 , storm .9.0.1 , unix sripts deployment tool : heroku deployment for development nosql database : hbase , mongodb ide used : atom , netbeans 8.x responsibilities : requirement analysis , development and unit testing. bug fixing and customization works based on customer needs. epitome : adchain is a product , fusionseven provides an end - to - end saas solution that solves for the ever growing financial complexities in the media buying process. the platform covers all aspects of a media transaction including a ) contract management from all vendors b ) order ingestion c ) in - flight optimizations d ) clearing e ) settlement f ) real - time revenue recognition g ) billing and reporting currently for this product we are using nodejs for backend programming ; database is hbase and mongodb , front end using single page application react js and nvd dashboards. #2. end - to - end customer ticketing system ( bigdata project ) client : cablevision systems. duration oct 13 to nov 15 team size : 4 environment : linux , windows xp web server : apache tomcat 6.0 nosql database : apache cassandra java client : hector api analytical : apache hadoop 2.2 , apache pig , hbase - 96.16 , storm .9.0.1 , unix sripts monitoring tool : zookeeper 3.4.5 framework : spring 3.x position : senior java developer ide used : netbeans 7.x responsibilities : requirement analysis , development and unit testing. bug fixing and customization works based on customer needs. epitome : the end - to - end customer ticketing system will bring disparate information together and provide methods of tracking and sharing the journey related to a customer issue , by opening , managing and closing customer case records. the system will enable employees throughout the customer service organization to view and understand customer interactions across all touch points and provide consistent expansion of interaction knowledge as each action occurs and is recorded in the case. information related to plant and product delivery systems status will be merged with interactions data. the combination of data will allow customer service employees to act more quickly and be responsive to customers needs and provide the best service possible and significantly improved customer service experience. educationbe - electronics and communication and engineering ( first class ) under anna university , chennai.',\n",
       " \"summaryhaving 6.8 years of experience in informatica. experience in analysis design , development and implementation of business applications using informatica power center. strong design and development skills. working experience on informatica power center tools designer , workflow manager and workflow monitor. excellent knowledge of data warehouse. significant experience in etl process in data warehouse lifecycle. developed different mappings mapplets using different transformations. possess excellent learning ability and practical implementation of the same. significant experience in preparing the unit test cases and design document. experience in performance tuning the mappings. experience on oracle sql. 3 years experience in teradata ( multiset and volatile table , fastload , multiload , tpt , tpump , daigonastic stats , joiner index , loader connection , teradata sql assistant , stats , skew factor , dbc tables , export , import , release mload , error table , view point , etc ) experience on sql. performance tuning in sql and informatica. hands - on experience procedure , function , triggers. error handling in informatica. worked on all command of unix. good exposure of shell scripting in unix. reviewing the code and documents prepared by the team. can adapt to new technologies quickly capacity to work meet deadlines excellent logical , analytical and debugging skills. have good communication skills , interpersonal relations , hardworking and result oriented as an individual and in a team. a self - starter with clear understanding of the business needs and having radical and tactical problem solving approaches and possess excellent leadership and motivation skills. client interaction experience. good domain knowledge in banking and financial services ( bfs ) . experienceinformatica admin , deployment migration , code release , tool release , configuration , setup , repository backup restore , types of connection in workflow manager etc. having good experience in cloudera big data hadoop , hdfs , lfs , yarn , hadoop eco - friendly system , pig , hive , sqoop , flume , hbase , oozie , zookeeper , map reduce. connectivity of hadoop with informatica to load data from lfs( hdfs , hdfs( lfs , hdfs( hdfs. loading data from hdfs to hive , lfs to hive using informatica. technology : operating systems unix , windows databases oracle , teradata , sql server , ingress technologies power center , informatica , idq , hive , hdfs , pig tools ide informatica , toad , putty , winscp , sql developer , hpqc framework bigdata hadoop dates organization designation aug 2016 present schlumberger software engineer may 2016 aug 2016 jp morgan chase associate dec 2012 may 2016 capgemini sogeti , india consultant july 2010 dec 2012 tata consultancy services systems engineer projectthe details of the various assignments that i have handled are listed here , in chronological order. 1. schlumberger sims customer schlumberger role data integration developer period aug 2016 to present description data lake and data pond google cloud platform : - data ingestion project. we are loading data to hive from different sources. we create generic mapping ( template ) and call that mapping from the application ( which we create into informatica developer tool ) .we pass parameter to the application ( informatica developer tool ) to call all the objects and values of parameter which loads the data in hive. there is two flow after hive. one is for master data load and other is file extraction which sent to gcp ( google cloud platform ) both stream is loading for full and incremental load. we maintain two different zone in hive named as landing zone and discovery zone. landing zone having client required data whereas discovery zone has dump data. role ( creating template in informatica bde( big data edition ) ( loading both full and incremental data to the both lake and pond. ( generation for parameter file as per template ( analysis and review application which is created using above template. ( data validation at both the places data lake and pond ( gcp ) environment unix tools informatica bde v10.1.1 , sql - devloper , putty , idq , hive , hdfs , sql server , sql workbench for hive 2. jp morgan chase reconciliation customer jp morgan chase role team member period may 2016 to aug 2016 description development of informatica mapping. creating a target file which act as source of the other tool ( tlm - - transaction life cycle management ) . which get load into the tlm db ( oracle ) after reconcile. role development of informatica mapping. creating a target file which act as source of the other tool ( tlm - - transaction life cycle management ) . which get load into the tlm db ( oracle ) after reconcile. environment unix tools informatica 9.6 , sql - devloper , winscp , putty , tlm 3. capgemini : - sims customer schlumberger , houston , texas , united states period july 2015 to till date role team lead description there are two releases in queue. release r1 : - we have source system sap ms sql server.bi extractor comes in picture when source system is sap. for ms sql server source system we create mapping and call procedure. pulled data has been directly loaded into oracle db ( in work table ) .work table to staging table loading done after implementing business requirement. staging to target is one to one mapping adding some business suggested column. we maintain history table for work table and target table. once data is ready in target table ( oracle db ) , one team called reported team use that table as source. we are maintaining audit framework in all tables e.g work table( staging table( target table( history table currently r1 is in qa and its going to live on may 1st. release r2 : - in release r2 we have hadoop with informatica. currently we are busy in different activities : 1.hadoop connectivity with informatica 2.hdfs connection test 3.loading data from informatica source file system to hdfs , hdfs to informatcia target file 3. loading data to hive from local file system. 4. read the data from sqoop( pigs( hive( hdfs( load the data to informatica target. 5.done the other poc such as : a ) counting a particular word in an unstructured file using pig. b ) hands on hbase. strong grip on : hive , pigs , hbase , sqoop , flume , oozie , zookeeper. role client owner and responsible for keeping all the track and follow up with business for my clients. developed codes by interacting with the business to understand the requirement and implemented the etl logic. implemented cross - reference( xref ) logic. expertise in developing the mappings by using suitable transformation as per requirement. involved in performance tuning. understanding the whole process and gave suggestion to change whenever required. involved in analysis before acceptance. provides guidance training to the new joinees as and when required. providing effective technical solutions among the team. enhanced the existing etl mappings in informatica to meet the new requirements. also part of the deployment , good knowledge if informatica administrator ( implemented below scenarios as well being shared resource : - mapping variable and mapping parameter global parameter power center utilities ( pmcmd command , pmrep command ) global parameter target load plan indirect load event wait dynamic parameter file performance tuning : ( pushdown optimization ( key range partition ( pass through partition ( hash auto key partition environment unix tools informatica 9.6.1 , sql - devloper , winscp , putty , dvo banking solution customer sns bank , netherlands role team lead period december 2012 to june 2015 description the project is about all type of banking model. like net banking , loan , mortgage , risk management etc. creating new models , changing on existing code by cr or fixing issues on incident request. role client owner and responsible for keeping all the track and follow up with business for my clients. developed codes by interacting with the business to understand the requirement and implemented the etl logic expertise in developing the mappings by using suitable transformation as per requirement. involved in performance tuning. understanding the whole process and gave suggestion to change whenever required. involved in analysis before acceptance. provides guidance training to the new joinees as and when required. providing effective technical solutions among the team. enhanced the existing etl mappings in informatica to meet the new requirements. mapping variable and mapping parameter global parameter power center utilities ( pmcmd command , pmrep command ) global parameter target load plan indirect load event wait dynamic parameter file performance tuning : ( pushdown optimization ( key range partition ( pass through partition ( hash auto key partition environment unix tools informatica 8.6 , sql - devloper , winscp , putty , teradata , morphues 4. tata consultancy services vehicle upload customer ge capital , france role team member period july , 2012 december , 2012 description ge has purchased lots of vehicle in europe. all vehicle needs to classify on the basis of types and models. if model changed , then we need to discontinue the vehicle. on the basis return indicator decide which vehicle will go for servicing. loading the data from flat file source systems to the revel staging layer and then target involving all the calculation at transformation level. getting source file from customer on business days. roles and responsibilities developing and debugging the informatica mappings to resolve bugs , and identify the causes of failures. user interaction to identify the issues with the data loaded through the application. developed different mappings mapplets using different transformations. providing effective technical solutions among the team. responsible for handling change. responsible for code check in. participated in the design documentation. enhanced the existing etl mappings in informatica to meet the new requirements. reviewing the code and documents prepared by other developers. extensively worked on performance tuning. maintaining all the trackers. taking project level initiatives. mapping variable and parameter incremental load mapping variable and mapping parameter global parameter target load plan indirect load tools informatica 8.6 , unix , ingress db environment unix nike customer british petroleum ( bp ) , uk role team member period march , 2011 june , 2012 description the nike application was using heterogeneous sources from different countries.that sources is invoice of oil sale on everyday in that particular country.we use different transformation as per business requirements.finally we populate data in target table( oracle db ) .so business generate report ( invoice ) in their standard format. roles and responsibilities developing and debugging the informatica mappings to resolve bugs , and identify the causes of failures. providing effective technical solutions among the team. worked on informatica tool source analyzer , target designer , mapping designer , mapplet designer and transformation developer. created the partition for newly inserted records. participated in the design documentation. the mappings were unit tested to check for the expected results. study and analysis of the mapping document indicating the source tables , columns , data types , transformations required , business rules to be applied , target tables , columns and data types. loaded data into the data warehouse and there were two kinds of data loading processes ( daily and monthly ) depending on the frequency of source data. reviewing the code and documents prepared by the team. tools informatica 8.6 , toad , putty environment unix bulk data extracts customer pnc , us role team member period sept 2010 feb 2011 description pnc financial services group , headquartered in pittsburgh , pa , is one of the nation ' s largest financial holding companies. the company operates through an extensive banking network primarily in ohio , illinois , indiana , kentucky , michigan , missouri and pennsylvania , and also serves customers in selected markets nationally. bulk data extract : extraction of zipped files and place the files in the directory from where informatica can pick up the files and perform the desired transformations for loading purpose roles and responsibilities understanding the requirements and preparing the low level design doc based on understanding. involved in designing the mappings between sources and targets developing and debugging the informatica mappings to resolve bugs , and identify the causes of failures. involved in preparing the unit test cases. tools informatica 8.6 , toad environment unix passport details : name as on passport relationship passport number place of issue quaish abuzer self j4168356 patna , bihar educationdegree and date institute year of passing master of computer application( mca ) birla institute of technology( bit ) , mesra , ranchi 2007 - 2010\",\n",
       " 'summaryalmost 7 years of professional work experience in java and hadoop ecosystem. scjp certified and 4+years of work experience in core java , design pattern. over 3 years of strong experience designing and implementing big data solution architecture from ground up using cutting edge big data technologies. strong hands - on experience implementing big data solutions using technology stack including hadoop mapreduce , hive , pig , hdfs , hbase , yarn , tez , spark rdd , sqoop , and oozie. buillt distributed , scalable and reliable data pipelines that ingest and process data. experience in multiple big data distributions , cloudera 5.x , hortonworks 2.5. an excellent team player with extraordinary problem solving and trouble - shooting. capabilities and ability to work under pressure with minimum or no supervision. good knowledge of apache spark and apache kafka. good experience in telecom( oss bss ) domain includes various areas like service provisioning ( service activation ) , telecom billing ( bscs ) , charging system. experienceworking as an sr. hadoop developer in ibm india pvt ltd , bangalore from june 2016 to till date. worked as sr. software engineer in ericsson global india service pvt ltd , bangalore from january 2012 to mar 2016. worked as an application developer in ibm india pvt ltd , bangalore from june 2010 to january 2012. projecttitle : at and t( usa leading telecom provider ) company : ibm designation : hadoop consultant. technology : jdk 1.7.0 , java , hdfs , yarn , tez , hbase 1.0 , 2.6.0 - cdh 5.4.7 , hive 0.13 , sqoop 1.4 , eclipse description : the datalake for att is a storage platform to capture and process vast amounts of multi - structured data that typically has been cost prohibitive to store and analyze. it is a cross platform collaboration environment that is highly scalable with low - latency performance. crediting is the process by which the media system determines viewing of content from a distribution source , or that viewing is occurring from a signal source that has the capability to carry such content. normally , such crediting is rendered as a distribution source identifier , and a broadcast time at which the content was carried by the distribution source. roles and responsibilities : responsible for requirement gathering and analysis for the project and put together possible ideas for the model. developed multiple mapreduce jobs in java for data cleaning and preprocessing. extracted data is loaded to datalake platform from multiple sources like teradata , oracle using sqoop. built oozie workflow to import cdc using sqoop and hive. debugging hadoop cluster configuration and maintenance , troubleshooting of mr jobs. used hortonworks ambari to monitor the hadoop echo system. used maven for working on mapreduce program development. knowledge of shell scripting and wrote many shell scripts for project use. creating and updating the technical document like low level design. title : bigdata hadoop company : ericsson designation : senior software engg environment : jdk 1.6.0 , java , linux , cdh 4.5.0 , hdfs , yarn , core java , hive 0.11 , pig 0.11 , eclipse description : this project aims to collect , aggregate , process , and report the web logs. web server logs contain the information about user clicks , user demographics and products information. we used this data to derive some insights like which products are doing well , which pages are popular , which users are active , which users are inactive , page clicks count , time spent by each user ( per month , per week , per day , per year ) etc. this poc objectiveis to use the open distributed computing platform like apache hadoop and hadoop ecosystem to ingest data , tostore data on commodity hardware , to process data in parallel and finally to report the processed data. responsibilities : we collected some few giga bytes log data from other application like adobe omniture click stream via ftp. we stored various types of file ( txt , csv , compressed , json ) into hadoop distributed file system. we wrote map - reduce programs for processing. analyzing the data with hiveql and writing hive queries for data analysis. we reported this processed data to sas systems and other downstreams. worked on hive queries creation , hive partitions , hive joins and deployment of them on hadoop production. involved in transfer of data from oracle and teradate tables into hdfs and hive tables using sqoop. company : ericsson global india pvt ltd. designation : software engineer. technology : core java , soap web services , xml , xslt , linux ( redhat ) , spring framework duration : january 2012 to march 2015. project description : ericsson multi activation is a product of ericsson oss which is used by the service provider in telecom industry globally to activate the services in different network element like vms , hlr , auc , vlr , smsc , mmsc , etc. we had to customize the ema product as the customer requirement and integrate with the ordercare and make the service activation possible with the towards the vendor specific network element. responsibilities : implementation of the solution , writing source code review. involved in complete sdlc from requirement gathering to deployment , which includes activities like interaction with clients. impact analysis , review and enhancement for the new cr involved in troubleshooting and bug fixing. documentation of the project. developed java code for different modules. testing of code and functionality along with roll back plans. company : ibm india pvt ltd. client : vodafone italy designation : application developer. technology : core java , struts , jsp , servlet environment : web sphere application server duration : june 2010 to january 2012 project description : vodafone italy steady state is a project for maintaining and enhancing the internal applications of vodafone italy , across different modules like sales , billing , dealer station , 190web and provisioning. responsibilities : involved in enhancement and bug fixing. documentation of the project. involved in complete sdlc for any changes. writing java code as per requirement , designing jsp pages impact analysis and enhancement for the new crs. skillshadoop ecosystem hdfs , map reduce , yarn , tez , hive , sqoop , pig , oozie monitoring tools cloudera manager , ambari programming language java , shell scripting ide eclipse 3.3.2 , jboss developer studio database oracle and mysql source control tools maven , svn , tortoisesvn , git , gerrit operating system linux , ubuntu , centos and windows educationb.e ( electrical engg ) from , indian institute of engineering science and technology , shibpur in 2010.',\n",
       " 'summaryhave over 6 years of focused experience in it industry in mainframe and oracle application design and development , business analysis and management. possess 2 years of good experience in hadoop framework by developing big data applications using hadoop ecosystem and other open source tools. experienced with ingestion , storage , querying , processing and analysis of big data using hadoop ecosystem including map reduce yarn , hdfs , hive , pig , sqoop have a very good experience in analyzing the data using hiveql queries , pig latin scripts. experience in importing and exporting data from different database systems ( oracle , mysql ) to hadoop file system using sqoop and automating the process using oozie coordinator. a good knowledge in various hadoop components like flume , hbase , spark - sql. good command over language while interacting with clients. exposure in industry leading hadoop distributions like cloudera , hortonworks and apache hadoop. good knowledge on relational databases like mysql and oracle and mainframe extensive experience with sql and database concepts. a team player with excellent communication , analytical , verbal and writing skills. excellent communication skills , problem solving skills , leadership qualities and an attitude to learn new cutting edge technologies. academic laurel : b.e ( electronics and communication ) from p.b. college of engineering , first class ( affiliated to anna university ) , chennai , [ 2010 ] hsc 1st class from govt. boys hr. secondary school , chromepet , chennai , [ 2008 ] ssc 1st class from govt. boys hr. secondary school , chromepet , chennai , [ 2006 ] experienceassociate @ cognizant technology solutions , india ( p ) ltd , chennai from march 2011 till date. skillsprogramming languages hql , pig latin script , sql , spark - sql hadoop ecosystem hdfs , mapreduce yarn , hive , pig , sqoop rdbms sql db , mysql , oracle , db2 mainframe nosql operating systems hbase windows , linux project title : feed analysis ( april 2016 till date ) role : business developer description : our client is a us based insurance company , deals with many investment brokers and so forth who sends the feed information on a daily basis that consists of thousands of records of every customers account and transaction details. part of our job is to collect all the data from rdbms and store it in hdfs to make it available for further analysis and push back the data to rdbms for ui purpose. responsibilities : interact with the customer to get requirement and develop a prototype. provide a design plan on the complete workflow. write sqoop script to ingest data from rdbms and export back. managed hive tables and created child tables based on partitions write pig script and hive query to transform data for analysis. project title : kpatient bda system ( march 2015 april 2016 ) role : application developer kaiser permanente is an integrated managed care consortium , based in oakland , california , united states. kaiser permanente is made up of three distinct groups of entities : the kaiser foundation health plan and its regional operating subsidiaries ; kaiser foundation hospitals ; and the autonomous regional permanente medical groups. as of 2014 , kaiser permanente operates in eight states and the district of columbia , and is the largest managed care organization in the united states. in this project we decided to migrate kaiser cdw application to new hadoop platform , because etl processes which loads data into data mart , had been consuming a lot more cpu cycles and processing time than planned or designated , which was on oracle exadata platform. data ingested into hdfs using sqoop. data transformation logic with analytical functions are carried out using pig latin and hive query language. responsibilities : handled importing of data from various data sources , performed transformations using hive , pig , mapreduce , and loaded data into hdfs. participated in design , pseudo code , data modeling. project title : claims ( march 2011 jan 2015 ) role : business developer environment : endeavour , remedy , ispf , alm , file master , sql developer technology : cobol , jcl , db2 , oracle11g project description : pos point of sale , is on - line pharmacy claim adjudication system. medcos point - of - sale ( pos ) electronic claims processing system , is also referred to as telepaid. when a member of a paid card or mail plan has a prescription filled at participating pharmacy , the pharmacy transmits the claim information to the telepaid pos system in a standard ncpdp format. claim is processed in pos system. pos validates and verifies member dependent eligibility , group and drug coverage plan , determines pharmacy participation in pharmacy network ppo , checks claims for duplicates , performs drug utilization review dur , prices prescription , calculates deductible oop , saves claims on pos , customer service data base and iw , transmits claim response back to the pharmacy. responsibilities : ensured the re - usability , traceability and efficiency in the new modules gave resolution on any issues related to the project involved in unit testing , regression testing , system integration testing performed and participated in requirement gathering , pr ( project requirement ) review and test case reviews to ensure all requirements are addressed clearly worked with the business users for generating intended advices to build all the possible scenarios in real time environment raised clarifications wherever necessary to ensure all the functionalities are right. be available and resolve give resolution quickly on any issues related to the project certification and recognition completed certification in retail ( l0 ) from cognizant technology solutions. hadoop bigdata certified from jpa solutions chennai. cognizant certified professional in itil v3.0 cognizant certified professional in six sigma ( yellow belt ) ccp ( cognizant certified professional ) certification in db2 udb v8.1 , cobol and jcl',\n",
       " \"summaryto obtain a creative and challenging position that enables me to gain valuable commercial experience and improve the software design , development and problem solving skills. total 6.5+ years of working experience in it industry in the healthcare domain with india document solution pvt. ltd. , noida , ( jan 2010 to till date ) ( www.dsg - us.com ) during working expiring having 4+ year experience of team lead position. over 2.6 years of experience in hadoop ecosystem , java , python. hadoop and big data certification from edvancer eduventures expertise in hadoop echo systems hdfs , map reduce , pig , sqoop and hive for scalability , distributed computing and high performance computing well versed in core java. hands on experience working in agile serum methodology based on project. design , development and testing of applications in java , j2ee and flex. having good knowledge on single node and multi - node cluster configurations. self - motivated and ready to learn new technologies. as trial developer ecrf design coding unit testing as per edit specification related to module followed sdtm , cdisc standard. experienceproject title ( http : dev2013 leo leo pharma environment hadoop , apache pig , hive , sqoop , hbase and nosql ( mongo db ) project description leo pharma is a visionary , leading - edge company that develops and markets innovative solutions for important dermatology challenges and issues. the purpose of the project is to perform the analysis on the effectiveness and validity of controls and to store terabytes of log information generated by the source providers as part of the analysis and extract meaningful information out of it. the solution is based on the open source big data software hadoop. the data will be stored in hadoop file system and processed using map reduce jobs , which intern includes getting the raw data , process the data to obtain controls and redesign change history information , extract various reports out of the controls history and export the information for further processing. responsibilities involved in design and development of technical specifications using hadoop technology involved in moving all log files generated from various sources to hdfs for further processing. written the apache pig scripts to process the hdfs data. created hive tables to store the processed results in a tabular format. monitoring hadoop scripts which take the input from hdfs and load the data into hive. created external tables in hive. project title http : dev2011.dsg - us.com spectrum , spectrum hadoop framework : hdfs , python , hive , hbase , sqoop , pig , spark , aws , mysql project role hadoop developer project description maintaining the patient ( site subject ) details and rewards points transaction are very difficult in terms of storage and processing. patient loyalty management system is replacing the existing reward management system which is developed as a web service provider with the help of database sharing with fda. aim of this system is to reduce the response time of web service. this system is designed with hbase storage handler and later planning to remove some bi reports generation using hive. the solution is based on the open source big data s w hadoop. the data will be stored in hadoop file system and processed using map reduce jobs. scropting python , unix responsibilities application installation of hadoop , hive , mapreduce and sqoop hdfs support and maintenance and adding removing a node , data rebalancing. developed mapreduce application using hadoop , mapreduce programming and hbase with spark streaming. involved in developing the pig scripts involved in developing the hive reports. project title http : www.enobia.com , enobia pharma , us environment hadoop , apache pig , python , hive , sqoop , linux , mysql role hadoop developer hardware virtual machines , linux. project description the purpose of the project is to store terabytes of log information generated by the dr. clinical specialist during trial phase on website and extract meaning information out of it. the solution is based on the open source bigdata s w hadoop .the data will be stored in hadoop file system and processed using map reduce jobs. which intern includes getting the raw html data from the websites , process the html to obtain product and pricing information , extract various reports out of the product pricing information and export the information for further processing. this project is mainly for the replatforming of the current existing system which is running on webharvest a third party jar and in mysql db to a new cloud solution technology called hadoop which can able to process large data sets ( i.e. tera bytes and peta bytes of data ) in order to meet the client requirements with the increasing completion from his retailers. responsibilities involved in developing the hive reports. involved in developing the pig scripts developed the sqoop scripts in order to make the interaction between pig and mysql database. involved in developing the controller , service and dao layers of spring framework for developingdashboard for walmart project hadoop mr jobs can be done depending upon the data set size using this class time taken to execute the jobs by defining combiner class or without combiner without combiner with combiner involved in resolving the jiras based on hadoop. moved all crawl data flat files generated from various retailers to hdfs for further processing. developed the linux shell scripts for creating the reports from hive data. completely involved in the requirement analysis phase analytics model with r used for group expressions , box plots and used to read files usingread.table ( ) functions and scan ( ) functions. r with hdfs and r with hbase. raw html data extracted from code repositors using pig regular expressions and used in hive externaltable using dynamic path ( ` extern ' ) . using sqoop exported hive external output processed data into mysql with role of clinical trial developer project title web development http : www.promiuspharma.com , promius , us environment ecaselink , dsg designer tool and sql 2008 along with java script , remote machine. project description promius pharma is a visionary , leading - edge company that develops and markets innovative solutions for important dermatology challenges and issues. we are focused on better answers for today ' s skin care needs with an eye to the future to reinvent and redefine therapeutic options in dermatology. responsibilities currently building ecrf using sdtm guidelines for http : www.promiuspharma.com using. ecaselink , dsg designer tool , sql2008 , vss and tfs. project title web development ( http : dev2011 abbottvascular abort , india environment ecaselink tool and sql 2008 along with java script , remote machine. project description developed systems responsible for serving critical medical challenges take careful planning , relentless resilience , the best scientific minds , and rigorous clinical trials across web properties. responsibilities currently building ecrf using sdtm guidelines for www.hgsi.com using .ecaselink , dsg designer tool , sql2008. skillsclinical domain skills : cdisc , sdtm , gcp , ich , sdtmig databases : ms sql server2000 2005 2008 , mysql , nosql ( mongo db ) operating system : windows 2000 xp development tools : eclipse , ecaselink , dsg designer scripting language : java script , html , xml others : java , python , spark , sqoop , linux , hadoop , claudera ( cdh3 , cdh5 ) hive , pig , spark , jsp , scala , flume , linux shell scripting ( ubuntu ) , aws , winscp role and responsibility : as a hadoop developer involved in design and development of technical specifications using hadoop technology. also developed mapreduce application using hadoop , mapreduce programming and hbase. involved in developing the pig scripts performance testing tuning experience on cloudera ( hive ) involved in developing the hive reports. ecrf design coding unit testing as per edit specification related to module. as a team lead administering trial developer tests trial during the interview phase. training new hires and the existing employees in my group. making sure employees complete their tasks on time and meet the deadlines. provide regular feedback to manager ( fang wei ) .\",\n",
       " 'experiencebig data hadoop professional with overall 9 years it experience in analytics , software development , software testing , application maintenance and support. good at analyzing big data problems and providing innovative solutions. good knowledge and understanding of the practical applications of hadoop ecosystems hdfs , sqoop , flume , pig and hive. worked as team lead , so experience in managing team. strong functional and technical knowledge of the corporate client on - boarding process. ability to quickly pick up new technologies and concepts. big data hadoop experience over 2.6 year of experience in hadoop technologies , worked in cdh5 cloudera machine. good knowledge in hadoop echo systems hdfs ( storage ) pig ( etl business logic for structured and semi - structured ) hive ( data science ) sqoop ( data import export ) flume ( data import export for semi - structured and unstructured ) hue ( gui for applications ) + yarn ( framework ) basic knowledge map reduce nosql - hbase ( scalability , distributed computing and high performance computing ) oozie spark scala 1. big data hadoop 2. business analyst 3. sap business object business intelligence l and t infotech dec 2012 till date team lead developer projecthonda environment cloudera distribution for hadoop ( cdh - 5 ) , hadoop2.5 , apache pig , hive , oozie , sqoop , unix , mysql , oracle duration : june 2015 till date team size : 4 description : the purpose of the project is to perform the analysis on the effectiveness and validity of controls and to store terabytes of log information generated by the source providers as part of the analysis and extract meaningful information out of it. the solution is based on the open source big data software hadoop. the data will be stored in hadoop file system and processed using map reduce jobs , which intern includes getting the raw data , process the data to obtain controls and redesign change history information , extract various reports out of the controls history and export the information for further processing. responsibility : 1. analysis of the specifications provided by the clients. 2. preparation of analytical requirement use cases documents. 3. import data from oracle to hdfs using sqoop 4. write hive queries. 5. involved in design and development of technical specifications using hadoop technology. 6. involved in moving all log files generated from various sources to hdfs for further processing using pig. 7. written the apache pig scripts to process the hdfs data. 8. created hive tables to store the processed results in a tabular format. 9. monitoring hadoop scripts which take the input from hdfs and load the data into hive. 10. created external tables in hive. citi environment cloudera distribution for hadoop , hdfs , mapreduce , pig , hive , flume , sqoop duration : sept 2014 june 2015 team size : 5 description : citi is a american multinational banking and financial services company. it is a universal bank with operations in retail , wholesale and investment banking , as well as wealth management , mortgage lending and credit cards. it has operations in over 50 countries and territories and has around 48 million customers. l and t infotech is providing innovative big data solutions to citi to get insights on the various existing data sources. l and t infotech has developed , supported and maintained big data applications for citi responsibility : 1. analysis of the specifications provided by the clients. 2. preparation of analytical requirement use cases documents. 3. transfer all log files into hdfs location 4. import data into hdfs using sqoop 5. write mapreduce code that process for text files. 6. write hive queries 7. generate reports using bigsheets with generated output. honda lotus notes environment lotus notes r7 , r8.5 type : application development , support platform : windows role : team lead duration : dec 2012 august 2014 team size : 15 description : working as an team lead application developer in current project. my job profile includes requirement gathering , client interaction , documentation , design , coding and r and d. i am into development and maintenance of various applications for honda. we are supporting client based as well as web based application maintenance support. responsibility : 1. developing new applications. 2. providing functional support maintenance to all the applications. 3. interaction with client for requirement gathering. 4. detailed analysis of the requirements gathered from the client. 5. impact analysis of the change request prior to approval. 6. code creation for change requests. 7. support activity. 8. documentation and testing. 9. ensuring review of each task. ibm india pvt.ltd. jan 2008 nov 2012 ibm pvt.ltd. is a cmm level 5 company with an onsite presence at key locations globally , the company is headquartered in usa. my job profile consists of working as a application developer. dupont type : development , maintenance and support. platform : windows 7 server , lotus notes r8 x server software : lotus notes r8 x clients client : dupont wilmington , usa role : developer team size : 20 languages lotus script , formula language , x - pages and java script description : dupont puts science to work by creating sustainable solutions essential to a better , safer , healthier life for people everywhere. operating in more than 70 countries , dupont offers a wide range of innovative products and services for markets including agriculture , nutrition , electronics , communications , safety and protection , home and construction , transportation and apparel employees : 67 , 000 worldwide global : operating in more than 90 countries worldwide r and d : more than 75 research and development and customer service labs in 12 countries around the world. we support many lotus notes databases from regions emea , apac , uk , america. responsibility : 1. developing new applications 2. providing functional support maintenance to all the applications 3. interaction with osc for requirement gathering. 4. detailed analysis of the requirements gathered from the osc or the client. 5. impact analysis of the change request prior to approval. 6. code creation for change requests. 7. support activity. 8. documentation and testing 9. pql activities responsible for all quality related documents for the project 10. ensuring review of each task skillsoperating systems linux unix , windows 7 , windows xp big data technologies apache hadoop , hdfs , mapreduce , hive , sqoop , hbase , oozie , zookeeper , languages java , shell script , scala , javascript , lotusscript , formula language , excel programming , sql databases sql , oracle , no sql hbase , lotus notes groupware lotus notes domino r7 , r8 , r8.5 , lei , xpages internet technologies html , json , xml concepts and processes big - data , hadoop , sdlc , agile methodology requirement analysis , design , coding , documentation , testing , functional support defining escalation response resolution time for reported problems on the basis of criticality , mentoring team and handling deployment of tasks. client interaction , end to end query management , project management , business quality processes. international certifications 1. itil v3 foundation 2. ibm lotus notes a_areceived valuable contribution award best team performance for development team as well as support team educationmca pune university 2006 bcs s.r.t.m.university , nanded 2003',\n",
       " 'summary6.8 years of experience in big data technologies , sql server development , oracle , tableau reporting and business intelligence including ( integrating services and reporting services ) in development , sit , uat and production environments. 2.9 years of exclusive experience big data technologies. good experience in apache pig , sqoop , hive , oozie and impala. 3.2 years of experience in sql server development and business intelligence ( sql server integration services ) . expert in pig scripting and hive ql. 1 year experience in reporting services like tableau and ssrs. good knowledge in nosql like mongodb and hbase. good in spark with scala. experienced in transforming huge volume of data in and out of hadoop usingsqoop. good in shell scripting ( development and deployment in production environment ) . knowledge in flume for pulling log data. experience in cloudera hadoop distribution ( cdh4 , cdh3 ) . in depth and extensive knowledge of hadoop architecture and various components. worked as an onsite co - coordinator in client location. worked in proof - of - concepts ( pocs ) in designing cluster and development in big data using hdfs , hive , pig , sqoop , hbase , cloudera 3.x , 4.x and 5.x. knowledge on installation , configuration and managing hadoop clusters. good experience in database designing. good experience of working with microsoft visio , sql sentry plan explorer and db schema. experience in system and data modeling , database design , development , implementation , testing , documentation and maintenance of applications in client server and web environment. extensive experience in writing complex sql queries , stored procedures , views and triggers. strong experience with sql server 2012 , 2008r2 and 2005. good experience in t - sql and query optimization. having knowledge on healthcare , telecom , banking domains. experience of working with defect tracking tools and project management like bugzilla , rally and jira. experience on maintaining production servers which are in remote location. strong in backup restores for various rdbms. strong and quick understanding of client requirements. strong analytical , problem troubleshooting solving skills. experience in creating the reports as per clients requirement in the form table , matrix and charts. experience of working on agile and sprint development model. excellent verbal , written communication and presentation skills , problem solving and conflict resolution skills , and detail oriented. working as i.t analyst in tata consultancy services , hyderabad from march 2014 to till date. worked as database engineer in prime ki software solutions pvt ltd , hyderabad from jun 2010 to feb 2014. roles undertaken tcs - ericsson trusted person , key point of contact to both customer ( ericsson ) and users ( tcs associates ) . lead the team to develop many process improvement projects by implementing sql scripts , performance tuning which decreased lot of manual efforts. spoc of fun and connect activities at tcs. certifications , awards and recognitions bigdata and hadoop certified developer with top 15 percentile conducted by international knowledge measurement ( tcs sponsor ) . awarded on the spot and star of the month awards for the development and query performance tuning. awarded on the spot award for the deliverable of an adapter in very short span as per customer request. projectclient : development bank of singapore , singapore hyderabad , india and singapore role : developer project duration : oct 2014 to till date team size : 8 dbs is a leading financial services group in asia. we are headquartered in singapore , with a growing presence in greater china , southeast asia and south asia. dbs is fully committed to effective governance to protect the interests of all our stakeholders and to promote the long - term sustainability of the group. lrr module is related to financial consolidation. the main objective of this is to replace the existing legacy based system with hadoop. impala is used in etl layer for processing. in this module data is from hive tables to be taken as input for etl enrichment layer. both intermediate as well as final results of etl are again stored in hive or sent to downstream systems for further processing and report generation. responsibilities gathering and analyzing the requirements provided by business and convert it into technical aspects. created hive table schema to store the processed results. understanding the requirement and develop the data model. using sqoop ( import export ) ingest data from oracle and mysql data into hadoop. writing etl logic using impala and sql. scheduled the daily job scripts using tws. environment : cdh5.5.x , hive v1.1 , impala v2.3 , sqoop v1.4.x , orcale 11g and mysql 5. client : bank of america , usa project : fraud analytics , bank of america hyderabad , india role : developer project duration : june 2014 to oct 2014 team size : 8 this project aims at finding if a particular card usage was suspect or not , based on pre - defined rules. here in this project , we had to identify the usage patterns of the customer. here we have used r as our analytics tool. algorithms such as c5.0 and c4.5 were evaluated. for this particular case c5.0 had been chosen based on more accurate values for specificity , sensitivity. the final predicted data is stored into hive. reports are generated using tableau. responsibilities created hive tables to store the processed results in a tabular format. implemented hive tables and hql queries for the reports. involved in preparation of different report sheets in tableau , and also to prepare dashboards. involved in preparation of effort estimation for the activity based on the requirement and design document analyzing the business requirements and system specifications to understand the application. responsible to manage data coming from different sources. environment : r analytical tool , hive - 0.7.1 - cdh3u3 , tableau 8.1 and tableau server client : ericsson , sweden hyderabad , india role : developer project duration : may 2014 to june 2014 team size : 6 big data is used for optimizing the etl and the stored procedure. scenario 1 : this scenario aims to remove the last two columns of the input file with one new column. the content of the new column was some session id which should be same for all the rows. each time the job was run , that id should be incremented by 1. there was around 10gb of client data in csv format with 188 columns. the files were initially loaded into my sql. then using sqoop , data was loaded into hdfs. then map - reduce job was run in the cluster for all the processing. then the output data was loaded from hdfs back to my sql using sqoop. scenario 2 : this scenario aims in joining three different data files based on some conditions which was there in a sql file. client has provided three data files which were in csv format along with a sql file. the sizes of the files were 1tb , 80 gb and 1kb respectively. the files were processed and loaded in hdfs using pentaho. map reduce program was implemented for joining those files. pig script was also used for joining the files. using hive : finally the three tables were loaded in three hive tables. then we developed the hive query to join the three files. hive along with impala was used to optimize the performance. responsibilities understand the architecture of the big data ecosystem and its components. responsible for handling data flow from source data store. implemented a script to transmit tables from mysql to hive using sqoop. implemented hive tables and hql queries to define schema for given data set. implemented pig latin scripts ( joins ) according business rules. implemented etl solution with pig and hive. responsible for preparing technical specs , analyzing functional specs , development and maintenance of code. coding , unit testing and regression testing of the modules. interacting with onsite coordinators and clients. environment : hive - 0.7.1 - cdh3u3 , pig - 0.8.1 - cdh3u3 , map - reduce , my sql 5.0 , pentaho report designer - 3.8.2 , tableau 8.1 and tableau server and oracle 9i client : ericsson , sweden hyderabad , india role : module lead project duration : mar 2014 to may 2014 team size : 10 ericsson is running a worldwide it operation supporting business operation in more than 180 countries. ericsson it application landscape has been growing in business processes areas as well as in regions with new capabilities , increased performance and availability. the growing it landscape portfolio is resulting in increased costs of operation and maintenance. the cost development trend is not sustainable. in order for ericsson to stay agile and responsive to customer needs , its required to enforce consolidation and simplification of the it landscape. in this manner , ericsson will reduce costs for operation and maintenance , free up resources for future development needs and hence be able to respond more swiftly to business needs. simplification of the landscape shall also result in reduced complexity and hence lower operational risk and improved stability. responsibilities analyzing the business requirements. involved in database designing. implemented and tests database design and functionality and made modifications to increase the performance. worked on complex stored procedures to implement business logic. worked extensively on scripts creation ( create , drop , insert , update , alter and delete ) . data analysis - pulled data from different sources and analyzed information per report requirement. wrote sql queries to pull data from source databases and queries from database based on the report requirement. worked on creating dynamic reports using ssrs. involved in writing several etl packages using ssis. involved in requirements gathering with onsite team. attended daily sprint calls as part of agile methodology. environment : microsoft sql server 2012 , java se 7 , windows 8. client : scriptsave , arizona hyderabad , india role : sr software developer project duration : mar 2013 to mar 2014 team size : 8 scriptsave specializes in designing prescription drug benefits , which provide the most value to customers with no drug benefits , limited drug benefits , or prohibitively costly drug benefits. scriptsave discount engine we will reconfigure the switch to pass pharmacy claims to the scriptsave intermediary rather than the processor. this will cover the adjudication development that includes changes to the client portal and admin and back office interface and generating reports by fetching the claims and network data for warehouse. responsibilities analyzing the business requirements. involved in high level database designing. implemented and tests database design and functionality and makes modifications to increase the performance. worked on complex stored procedures to implement business logic. worked extensively on scripts creation ( create , drop , insert , update , alter and delete ) . data analysis - pulled data from different sources and analyzed information per report requirement. wrote sql queries to pull data from source databases and queries from database. importing and exporting pharmacy data. creating new custom reports based on requests from the client. involved in requirements gathering with onsite team. attended daily sprint calls as part of agile methodology. environment : sql server 2008r2 , windows xp. client : sunrx , usa project : virtual inventory ( www.340b.sunrx.com ) hyderabad , india role : software developer project duration : aug 2010 to mar 2013 team size : 12 skillsdatabases : sql server 2005 , 2008 r2 and 2012 ; oracle 9i and no sql ( mongo db , hbase ) big data ecosystem : hadoop , hdfs , mapreduce , apache pig , sqoop , hive , hbase , oozie , flume , impala , apache spark with scala. database tools : sql enterprise manager , sql server management studio , sql query analyzer and sql business intelligence development studio , sql profiler operating systems : windows server 2003 , xp , windows 7 and linux version control tools : tortoise svn educationm.c.a. with an aggregate of 76.50 % from jnt university in 2010.',\n",
       " 'summaryhaving 9+ years experience in data warehousing and data analytics. including requirements analysis , development , application maintenance , enhancement , data structring , data cleaning - cleansing using dw , data analytics tools.in - depth skills of data wharehousing and data analytics tools. industry : it - software software services functional area : it software - system programming role : team lead tech lead experiencesenior software engineer dec 2011 - till date cognizant technologies solutions sr.analytic engineer job type : permanent us work status : not mentioned preferred location : chennai skillsskill name version last used experience cognos bi 8.x , 10.x 2016 6 year( s ) 6 month( s ) msbi 2012 2016 9 year( s ) 0 month( s ) cognos bi 8.x , 10.x 2016 7 year( s ) 6 month( s ) hadoop 2.7 , 2.8 2017 3 year( s ) hive 2.x 2017 3 year( s ) apache pig 0.13 , 0.14 2017 2 year( s ) 9 month( s ) hbase 17 2017 1 year( s ) 6 month( s ) r programming 3.0 2017 2 year( s ) 9 month( s ) educationp.g. mca ( computers ) 2007 bharathidasan university , trichy ibm cognos 10 bi designer microsoft certified tech. specialist for bi tools. hadoop , hive certified developer.',\n",
       " 'summaryhaving around 6.10 years of it experience in design , application development , and implementation using emerging technologies like apache spark , scala , hadoop , nosql , and core java. around 2+ years of strong working experience with big data and hadoop ecosystems. capable of processing large sets of structured , semi - structured and unstructured data and supporting systems using hadoop architecture. well versed with hadoop ecosystems including map reduce , hdfs , pig , hive , hbase , sqoop , spark , scala , flume and oozie. excellent understanding knowledge of hadoop architecture and various components such as hdfs , job tracker , task tracker , name node , data node , yarn and mapreduce programming paradigm. good knowledge on machine learning , analytics and graph processing using spark. proficient in big data ingestion and streaming tools like flume , sqoop and kafka capable of creating real time data streaming solutions and batch style large scale distributed computing applications using apache spark , spark streaming , spark sql and no sql databases like hbase and cassandra. utilized apache kafka for tracking data ingestion to hadoop cluster good knowledge on akka actor for concurrent systems processing. insight into importing and exporting data using sqoop from hdfs to relational database systems and vice - versa. understanding in apache flume for efficiently collecting , aggregating , and moving large amounts of log data. proficient in writing the mapreduce jobs on hadoop ecosystem including hive and pig in cloudera. proficient in job workflow scheduling and monitoring tools like oozie. worked on creating the rdds , dfs for the required input data and performed the data transformations usingspark - core. good knowledge on apache mesos cluster manager. experienced working with different file formats - avro , parquet , orc and json. experiencecurrently working as a senior developer in capgemini , chennai , may 2016 to till date. worked as a senior consultant in polaris software labs , chennai , jan 2010 to may 2016. projecttitle : re - hosting of web intelligence may 16 to till date client : standard chartered bank( scb ) , uk platform : apache hadoop , cloudera , mapreduce , hive , sqoop , apache spark , zookeeper , scala , kafka , hbase , java , oozie , cassandra and unix shellscripting description : scb is the one of the largest bank in uk by assets and also by market capitalization and it provides clients with access to premier financial products and services. all the customer and accounting information are stored in rdbms and also collecting large amount of log data from distributed sources. volume of data is huge and growing in rdbms and it has high maintenance cost , scb started to move the data to hdfs as a part of rdbms to hadoop migration. hadoop helps to implement business rules , predictive analytics on risks or fraud , compliance and marketing. extract various reports out of the products and pricing information and export the information for further processing to external interface. role and responsibilities : moved data flat files generated from various corporate clients to hdfs for further processing created hive tables to store the processed results and used compression codec managing and scheduling jobs on a hadoop cluster involved in writing the hive scriptsfor processing and querying over the data in hdfs experience in implementing advanced procedures like text analytics using mapreduce , hive and processing using the in - memory computing capabilities like apachesparkwritten in scala. experience in importing data from various data sources like sql and oracle using sqoop and performed transformations using hive andsparkand loaded data into hdfs. performed transformations , cleaning and filtering on imported data using hive , map reduce and also loading data into hdfs using sqoop. involved in creating hive tables , loading with data and writing hive queries to process the data. worked on partitioning and used bucketing in hive tables and setting tuning parameters to improve the performance. experience in oozie workflow scheduler template to manage various jobs like sqoop and hive. experience in usingsparkapi over hadoop yarn to perform analytics on data. experienced in implementingsparkrdd transformations , actions to implement business analysis. extensively worked onsparkdata frames , sparkdata sources , sparksql and streaming using scala. configured streaming data to perform real time analytics usingsparkstreaming and kafka ( producer and consumers ) and persists the analyzed data to nosql db cassandra. involved in documenting best practices of the project. title : stocks purchase analytics platform jan 15 to april 16 client : morgan stanley( ms ) , new jersey , usa platform : hive , oozie , unix shell scripting , map reduce and flume description : stocks purchase platform is a web application where data is imported from different sources like rdbms , log files and web browsers etc. to a central repository. manipulations are performed before and after storing the data so that analytics can be performed over it. the application uses various big data technologies like flume , sqoop for importing various types of data. hive to store and apply tranformations over it and oozie as a workflow engine. role and responsibilities : developed framework related components and hive analytical queries to extract business critical information as per the business requirements developed the custom record reader to handle specific inputs developed the hive scripts for processing the logs to identify the critical user information like no. of shares purchased created hive queries to determine the sell to cover information scheduled the workflows using the oozie workflow scheduler with map - reduce , focused on finding frequency of stocks options each year improved the hive queries performance by implementing partitioning and clustering importing and exporting data from web servers to hdfs , hive used flume to collect , aggregate and store the web log data from different sources like web servers and pushed to hdfs title : test harness application jan 13 to dec 14 client : citi bank , usa platform : core java , jdbc , sql , unix , eclipse description : test harness application is designed to feed in canned input data into calculator and verify the output generated at three different stages( hedge engine output , optimizer engine output and calculator engine output ) against the canned expected data. the test harness is wrapped with intuitive gui , where the end user can input edit the canned data. user can select the tests that are required for a given run and trigger the test execution. post execution of selected tests , results will be populated back into gui and reports are generated. role and responsibilities : involved in the full software development life cycle of the project from analysis and design to testing and deployment. participated in technical design reviews and actively involved in functional design. created uml diagrams to capture architecture and application design. involved in preparing use - case diagrams , sequence diagrams and class diagrams using rational rose , uml. extensive use of core java collections , generics , exception handling , and design patterns for functionality , such as portfolio summary and user information. design relationship among different database tables , creation tables with low coupling. involved in writing complex sql queries , stored procedures , triggers to access the data from relational database. implemented user input validations using javascript and jquery developed test cases and performed unit test using junit framework. used agile methodology for the development of the project. build code using eclipse and deploy it using apache tomcat. skillsoperating systems : unix , windows , linux hadoop distribution : cloudera ( cdh5 ) big data hadoop : hive , map reduce , pig , sqoop , shell scripting , yarn , spark , apache mesos , zookeeper and akka actor database : db2 , sql no sql database : hbase , cassandra job scheduler : oozie build tool : maven version controller : git hub languages : core java , scala messaging brokers : kafka , flume ide : eclipse industries : banking and insurance educationb.tech in may 2006 with 70 % , ece , affiliated to jnt university , hyderabad.',\n",
       " \"summarylooking for bigdata jobs in chennai. looking for informatica tech lead role jobs in chennai. experience9+ years of it experience worked in the implementation of different application systems using bigdata hadoop and informatica . good experience in the loading of different applications system data to hadoop hive tables using unix shell scripting and hive. good experience in the analysis , design , development , testing and implementation of business intelligence solutions using data warehousing and etl concepts. etl experience using informatica 9.1 powercenter client tools - mapping designer , workflow manager monitor good understanding of informatica architecture and having exposure of sdlc. work technically with the requirements gathering and the development of etl mappings workflows and performance tuning of etl jobs. hands - on experience across all the stages of sdlc life cycle including build , unit testing , systems integration and user acceptance testing. hands - on experience with scd 1 and 2 mappings creation , push down optimization concepts to expedite the etl job runs , indirect list file concepts to process multiple files of the same format. using informatica data transformation studio to process semi - structured or unstructured data. extensive testing etl experience using informatica 9.1( power center - designer , workflow manager , workflow monitor. involved in handling defects issues raised by qa team within the required timeframe. experience in unix shell scripting , scheduling of jobs in control m , automation of etl jobs using unix shell scripting. creating unix shell scripts to check the availability of the files , file size limit , to attach the generated output file of informatica through mail , to send error messages if file is not processed , capturing source file names in the target , adding header to the file and enormous file concepts. involved in working with relational databases such as oracle 11g 10g 9i 8x. good understanding with stored procedures , functions , views , triggers and oracle pl sql. > worked as associate in cognizant technology solutions from oct 2007. > working as senior consultant in capgemini from jan 2016. projectscope : - scope international the wholly owned subsidiary of standard chartered bank , uk , began operations in 2001.in less than seven years scope has grown from 6500 employees and services over of the 70 countries .scope has move ahead form being a cost effective venture for the group to becoming an integral part of its operations. it deals with banking and financial services. responsibilities : worked with the loading of different source systems to hadoop hive tables with unix shell scripting. the following source systems implementation of data load to hadoop hive tables have been done. sci , razr , feds , acb , otp , nbp , dotopal - phase 2 , scstar. created tsd , technical mapping , aig , sop , implementation plan documentation for different sources that i have worked on. worked on sit testing bug fixes and provide sit support. worked on the cr checklist which is the list of items necessary to present the source in the cab call for production implementation. worked on prod implementation support for the above sources , which is production deployment. worked on uvt testing which is to validate the data in production layer. king on big data hadoop with hive and hadoop hdfs operations for dealing with big data sets. ihub : - innovation hub responsibilities : > worked on informatica power center tools - designer , repository manager , workflow manager , and workflow monitor. > parsed high - level design specification to simple etl coding and mapping standards. > involved in building the etl architecture and source to target mapping to load data into data warehouse. > created mapping documents to outline data flow from sources to targets. > extracted the data from the flat files and other rdbms databases into staging area and populated onto data warehouse. > used various transformations like filter , expression , sequence generator , update strategy , joiner , stored procedure , and union to develop robust mappings in the informatica designer. > developed mapping parameters and variables to support sql override. > used existing etl standards to develop these mappings. > prepared mapping standard documents to maintain standards in the etl mappings. > worked on different tasks in workflows like sessions , events raise , event wait , decision , e - mail , command , worklets , assignment , timer and scheduling of the workflow. > created sessions , configured workflows to extract data from various sources , transformed data , and loading into data warehouse. > used type 1 scd and type 2 scd mappings to update slowly changing dimension tables. > used debugger to test the mappings and fixed the bugs. > prepared migration document to move the mappings from development to testing and then to production repositories. > involving in status calls with higher management guiding and mentoring new joiners in the team. > providing daily updates to the onsite team in status calls. > discussing time lines for future enhancements between offshore management and onsite. > created reusable sessions to manage control and balancing for auditing : - this is to validate the source counts to the intermediate staging area and from the intermediate staging tables to the target layer. environment : informatica power center 8.6 oracle 9i role : etl developer duration : 2 years. got ' associate of the quarter award ' when worked in ihub. bcbsm : - blue cross blue shield of michigan : bcbsm is a global health insurance company offering valuable coverage , help the customers to make the right choices , and help you to get quality health care. the company ' s aim is to provide affordable innovative products that improve their care and health. responsibilities : > responsible for developing , support and maintenance for the etl ( extract , transform and load ) processes using informatica power center 8.1. > experience in integration of heterogeneous data sources like oracle , db2 and flat files ( fixed and delimited ) into staging area. > wrote sql - overrides and used filter conditions in source qualifier thereby improving the performance of the mapping. > designed and developed mappings using source qualifier , expression , lookup , router , aggregator , filter , sequence generator , stored procedure , update strategy , joiner and rank transformations. > managed the metadata associated with the etl processes used to populate the data warehouse. > used session parameters , mapping variable parameters and created parameter files for imparting flexible runs of workflows based on changing variable values. > improved session performance by enabling property incremental aggregation to load incremental data into target table. > used pmcmd command to automate the power center sessions and workflows through unix. environment : informatica power center 8.6 oracle 9i role : etl developer duration : 2 years. esi : esi corporation reponsibilities : > gathered business requirements from business analyst. > prepared hld lld ( high level low level ) design documents after requirements gathering to facilitate the development. > designed and implemented appropriate etl mappings to extract and transform data from various sources to meet requirements. > designed and developed informatica etl mappings to extract master and transactional data from heterogeneous data feeds and load. > installed and configured the informatica client tools. > created the environment for staging area , loading the staging area with data from multiple sources. > used workflow manager for session management , database connection management and scheduling of jobs. > created unix shell scripts for informatica etl tool to automate sessions. > monitored sessions using the workflow monitor , which were scheduled , running , completed or failed. debugged mappings for failed sessions. > created mapplets worklets to maintain reusability of mappings sessions. environment : informatica power center 8.6 oracle 9i role : etl developer duration : 2 years. d and b : dun and bradstreet d and b is the leading provider of international business information helps to provide commercial data to business which stimulates business - to - business sales and marketing , lead scoring and supply chain management and social identity matching. responsibilities : > gathering requirements form the onsite counterparts. > preparing design documents from the requirement collection. > creating informatica mappings workflows through power center designer workflow manager. > debugging and testing mappings workflows. environment : informatica power center 8.1 oracle 8i role : etl developer duration : 2 years. skills &#124; technology &#124; informatica 9.1 , big data hadoop &#124; &#124; databases and tools &#124; oracle 11g &#124; &#124; shell scripting &#124; unix &#124; a_agot informatica specialisation certificate from informatica academy. completed informatica 8 mapping designer certification completed oracle 8i cetrification. educationfirst class in bachelor of engineering from anna university - 2007 batch.\",\n",
       " 'experiencetechnologies used : spark , hadoop , mapreduce , pig , hive , hbase , oozie , flume , kafka , java , sql analytics tool : aster teradata studio , tableau hadoop security : implemented kerberos for hadoop echo system uml and architecture design tools : rational rhapsody , rational rose , rpe , doors configuration and version control tool : rational clear case , rational clearquest company name : wipro technsology from : nov 2016 to till date duration : 6 months designation : hadoop developer project 1 description : risl ( rajcomp info service ltd. ) rajsthan government client risl ( rajcomp info service ltd. ) rajsthan goverment roles development and data analytics , module project implementation visualization and development tools aster teradata studio and hadoop bigdata language hadoop , hive , pig , sql , java , cron_tab , hbase , sql_developer , unix etc. role and responsibilities : involved in development and data analysis. developed use cases email - logs , reality - check , sentimental - analsis , call - center etc. we are maintaining various application of government of rajasthan like transport department , bhamashah , citizen 360 etc. here we are developing various issue solution in bigdata hadoop which is facing by the rajasthan citizen. generate the visualize report for every issue with the help of analytics tools. also responsible for end to end implementation for use cases which has been alpproved by the client after analysis of data. handled the clients interaction. reporting with use of data analytics tool. company name : pinaka aerospace solution pvt ltd from : july 2011 to oct 2016 projectname : - cacc duration : - ( jan 2016 - oct 2016 ) designation : - hadoop developer client indian air force roles development and data analytics , module project implementation visualization and development tools tableau and hadoop bigdata language map reduce , hadoop , hive and pig , zookeeper , oozie , flume , sql project description : this project belongs to indian air force for controlling the air traffic for own and enemy aircraft. for providing proper atc clearance for avoiding collision. role and responsibilities : involved in development , designing and data analysis. developing project flow diagram with the help of uml tool after discussion with module expert. handled the clients interaction. name : - battle field surveillance center duration : - ( oct 2012 - jan2016 ) designation : - java and hadoop developer client indian army roles designing , coding and testimg , module project implementation uml and development tools rational rose , rhapsody , qt creator , net beans language kerberos for hadoop echo system , java , hadoop big data , hive , pig , sql project 2 description : battlefield surveillance system ( bss ) for indian army is to collect data from a variety of sensors , which include radars , sound ranging systems , electro - optical sensors and aerial sensors. the raw data received from these sensors shall include text messages providing attributes of targets detected , images , freeze frames and video. the sensors connected to bss system are distributed over a wide terrain. the data from ground sensors shall be transmitted via radio link for processing at surveillance node. the data video from aerial sensors shall be transmitted via optical radio link for processing at surveillance center. the sensor data shall be processed and fused using multi sensor data fusion ( msdf ) techniques to build a tactical picture. this tactical picture shall be displayed for the operator under gis environment. it shall allow the operators to carry out image analysis , artillery analysis , target analysis and supervisory functions. the bss system shall allow the operator at sensor end to edit annotate video images and data coming from sensors. role and responsibilities : involved in development , designing and data analysis. developing project flow diagram with the help of uml tool after discussion with module expert and testing the project. handled the clients interaction. project name : ladc ate duration : july 2011 to oct 2012 duration : 1 yeae 4 months designation : developer client drdo roles designing , coding and testing , module project implementation uml and development tools rational rose , net beans ide language core java , c , c++ , html project 1 description : the ladc ( levcon air data computer ) is intended to be used in thenavy version of light combat aircraft. the ladc is a quadruplex air data computer and important hardware configuration item ( hwci ) of the lca air data system. the ladc shall use primary csci to process the air data and outputs signals on rs - 422 serial data link to the digital flight control computer ( dfcc ) . each channel of ladc shall also accept data on rs - 422 serial link , which shall be spliced guh link in the lca air data system for further necessary processing and control of the aircraft the features are : - lower costs. no wait lines - receive immediate service. advanced technology and facilities. role and responsibilities : involved in analysis and designing. project flow diagram developing with the help of uml tool after discussion with module expert and testing the project. handled the client interaction. professional qualification : - b.e computer science with first class in 2011 from pune university',\n",
       " 'summaryapprox 7 years of overall it experience in application development in java j2ee and big data technologies like hadoop and apache spark. experience in analysis , design , enterprise application development , integration of web based application using core java( 1.7 ) , j2ee with spring framework 3.0.5 , tibco ems and bw designer 5.11. good experience in design and development of business critical software related to equities middle office technologies. good understanding of trade life cycle from fo - - >mo - >bo and fix protocol. implemented various business critical and regulatory related projects. understands and analyze the brd and functional view of the requirement given by business. understand , design and implement how to use market data , order data , allocation data , reference data and cloud service data. 2 year of experience in spark 2.2 , hadoop and its components like hdfs , spark , map reduce , hive , sqoop etc. implemented web server log analysis for trade generation equity project with map reduce and hive. implemented risk regulatory reporting with apache hive , sqoop and apache spark rdd. completed twitter case study through flume , kafka and spark streaming. data warehousing management with hive. query optimization with partitioning. knowledge of integrating pig and hive with hcatalog and hive and hbase with hbasestorage structured data ingestion from rdbms( sqlserver ) in to hadoop( hdfs and hive ) using sqoop( rdbms < - - > hadoop ) . implemented incremental inserts with sqoop. experience in developing and deployment of web applications in tomcat 5.5 , 6 and websphere 6. experience in project management in agile scrum and waterfall delivery models and also good understanding of engineering standards and principles. good understanding of databases like sql server and business workflow like tibco business work. quick learner , good communication and interpersonal skills , analytical skills , and strong ability to perform as part of team. have interacted with clients and attended business meetings , prepared software requirement specification documents. basic knowledge on no - sql databases like ( hbase , mongodb ) , r , scala and pig. experiencecurrently working as java hadoop developer in citi bank , pune from sep 2014 - till now worked as senior developer in hsbc , pune from aug 2012 - sep 2014 worked as java developer in tcs , mumbai from aug 2010 - aug 2012. skillslanguages : core java , j2ee big data technologies : mapreduce , hive , spark , sqoop , flume , kafka sql databases : sql server 2012 nosql databases : hbase , mongodb j2ee technologies : jsp , servlets , spring core , orm like hibernate methodology : agile , scrum , waterfall development tools : eclipse , ibm rad 8 , bitbucket git , tibco bw designer , junit , ibm jrules servers and build tools : apache tomcat , ibm websphere application server 7 , ant , maven operating system : windows , linux currently reading : r , phython , scala other tools : hp qwalitycentre , svn educationcleared developer certification of spark databricks( 1.x - 0805 ) sun certified java developer b.tech. in information technology from ymca institute of engg( 2010 ) . , faridabad( haryana ) - 73.54 % higher secondary from saraswati vidya mandir( 2006 ) , yamunanagar ( haryana ) - 86.00 % high school from saraswati vidya mandir( 2004 ) , yamunanagar ( haryana ) 81.30 % peoject details 1.primo( platform reengineering in mo ) primo is global application which is used for trade generation , risk management and reporting. it comes under middle office in trade life cycle. in primo we got order details which executed in the market , allocation details which is related to account and execution details which means order executed in exchange or broker. through all above parameters primo generated real time trades and send to back office for settlement. risk regulatory reporting us fed mandates all financial institutions to submit ccar 14q , ccar 14m and fdic summary which can be traced back to each loan , mortgage or card level transaction. the whole history should be preserved about how the fed report is being formulated. the project required us to analyse each card , loan and mortgage related monthly data across all geographies and apply ccar and fdic mandated rules on them and then aggregate them at much higher level for fed reporting. spark rdd and hdfs is being used to process and store all such granular data and oracle is used for final reporting. technical experience : core java , j2ee using spring mvc framework , tibco designer , tibco ems , bitbucket git , ibm jrules , spark 1.6 java rdd , hadoop 2.7 , cloudera 5.7 , sqoop , oracle , maven , junit , spark dataframe team size : 7 duration of the project : 2.9 yrs methodology : agile scrum role : developer roles and responsibilities : understands and analyze the brd and functional view of the requirement given by business. understand , design and implement how to use market data , order data , allocation data , reference data. implements regulatory , market mandatory changes related to exchange needed or any country. have interacted with clients and attended business meetings , prepared software requirement specification documents. in depth knowledge and experience with object oriented design , design patterns and sdlc followed as agile scrum model. resolved the production and uat issue on daily basis. resolve dev query and help to global team to understand the requirement. good experience in big event like brexit , us election and msci rebalance and how to monitor volumes and resolve issue quickly. understands and analyze the brd and functional view of the report requirement given by business. understand and design data set related to customer 360 degree for reporting. ingestion of data to hive tables through sqoop and once processed store back in to oracle database. to migrate the computation tool from hive to spark. model the data and store it in an easy to access manner using spark core. resolved the production and uat issue. 2.web server log analysis our application generates more than 100 gb of logs on daily basis for orders and executions which is tough to analyze. we did poc to analyze the logs. technical experience : hadoop , hive , hdfs , sqoop team size : 2 roles and responsibilities : understands and analyze the logs. design hive tables to store log data. understand and develop error code analysis script. no of times gc is running with timely manner. 4.gsir( group strategic individual review ) gsir is a web based application , developed for call centre and branch hsbc employees. with this application call centre and branch employees can get the individual review about hsbc customer and prospect customer. technical experience : core java , collections , j2ee using spring mvc framework , ibm rapid application developer , mq service call , dojo 1.8.0.1 , maven , svn , qc team size : 9 duration of the project : 1 year methodology : agile scrum role : developer roles and responsibilities : understands and analyze the brd and functional view of the requirement designed the four section of the web screen which populates all the data of the application. implements the session timeout logic using dojo. implements the business logic for reference data call , prepopulation , server side validation and mapping of data. improve the code coverage performance by applying sonar tool and resolve the critical and minor issues. prepare the release document of the application , . 5.global staff assisted 6.cac( currency administrative cell ) 7. dac ( data archival center )',\n",
       " \"experiencea competent professional with 7+ years of work experience in database and application sw development in telecommunication and banking domain with web based , etl and analytics based development. currently working with ericsson global india services pvt ltd. ericsson is a swedish multinational provider of communications technology and services. experience in data modeling ( logical and physical design of databases ) , normalization and building integrity constraints , experience in query optimization and performance tuning expertise in all aspect of database design and implementation , experience in data warehousing experience in dimensional modeling , er modeling , star schema snowflake schema , fact and dimensional tables and operational data store ( ods ) . in - depth knowledge and experience with advanced oracle pl sql sql server ( design and development ) , performance tuning and appropriate partitioning methods. in - depth knowledge and experience in sql server development and administration. excellent communication , interpersonal , analytical skills and strong ability to perform as part of a team. delivering and implementing the project as per scheduled milestones. ability to work in teams and independently with minimal supervision to meet deadlines. experience in the full - cycle software development including requirements gathering , prototyping , and proof of concept , design , documentation , implementation , testing , maintenance and production support. current assignment : currently associated with ericsson global india services pvt ltd as senior engineer. organization designation duration ericsson global india services pvt ltd senior engineer ( 01 2015 ) ( continued ) aircom international pvt. ltd senior software engineer ( 03 2013 ) ( 12 2014 ) saksoft ltd pl sql consultant ( 11 2010 ) ( 3 2013 ) indiabulls technology software engineer ( 03 2008 ) ( 11 2010 ) annexure ( projects handled ) company ericsson global india services pvt ltd projectwfm fso analytics role lead senior engineer duration from ( jan , 2015 ) to till date team size 50 client rwc( mbn , telefonika , vgc ) , rcom , vodacom environment db sql server tools bigdata , hadoop , spark , hive , unix , html5 , java script description fso analytics is designed based on various business logic and algorithms and it is further divided into various use cases , and supporting multi - vendor and multi technology. use case 1 : - first time right analysis duplicate wo analysis wrong assignments incorrect svd for wo multiple svds due to site accessibility issues multiple svds due to infra issue quality issues with field engineers quality issues related to incorrect closure of wo quality issues related to re - occurrence of issue use case 2 : - incoming work order quality improvement duplicate wos incomplete information in wo out of scope wo wo to be managed remotely correlated alarm patterns auto ceasing and no fault found alarm patterns site - alarm correlation frequent wo handling patterns - site - alarm - solution report correlation key responsibilities : analyzed the business requirements and functional specifications data acquisition , cleansing , preparing , machine learning mapping client ' s requirements and providing them best solutions , evaluating and defining scope and finalizing the large scale requirements. translating the client ' s requirements into specific systems , applications or process designs for complex solutions and enabling platforms. handling the end - to - end life cycle including design , development , testing , implementation , and integration and support functions. handled the responsibilities of leading team through the technical risks throughout the project sql program logic designing and development. company aircom international pvt ltd. i - view live plan role pl sql programmer duration mar ' 13 dec14 team size 60 client swisscom( switzerland ) , u.s. cellular ( usa ) , mtn( south africa ) environment db oracle 11g r2 tools oracle 11g , pl sql , c#.net silverlight , wcf , unix description i - view live plan is a fully automated solution for all technology like 2g 3g and lte which updates aircoms assett radio network planning tool with the latest live network configuration and parameter information. this allows network expansion plans and optimization changes to be performed using the actual live network configuration to ensure maximum integrity of the design. coverage plots also map much more closely to reality. all this helps to ensure a better network for your customers. the automated approach to updating the planning environment has a number of advantages : it removes the need to manually update asset with network optimization changes allowing engineers to spend their time on more important tasks. it prevents human error leading to inaccurate updates and ensures no network changes are forgotten about. the accurate and up - to - date data allows engineers to make better decisions leading to a better network design. key responsibilities : extensive use ofpl sqlto generate stored procedures triggers and database functions to meet user requirement. co ordination in release and performing sanity test used bulk collections for better performance and easy retrieval of data , by reducing context switching betweensql andpl sqlengines involved in coding and co ordination the testing efforts with the client to ensure user acceptance of the system generated. coordinated with the front end design team to provide them with the necessary stored procedures and packages and the necessary insight into the data. version controlling and bug analysis through tortoise svn and jira. providesl3 support on production issues. company saksoft ltd. data warehouse rainbow role pl sql programmer duration from ( march , 2011 ) to march 2013 team size 350 client citi financial ltd ( singapore china tiwan austrelia ) environment db oracle 11g r2 tools teradata sql assistant , informatica power centre 8.6 , unix description covers the following subject area in term of data models , credit risk , scoring analysis , sales analysis , customer service , marketing retention , authorization and fraud detection , collection policy , chanel analysis , rewards , credit regulatory ( basel ii ) rainbow developed to generalize both cards and banks related transactions and information about the customers , history data analysis for decision management. its scope to deployed all over countries based on region ( seap nap ceemea ) there are some country specific and some base requirement which is common for all country. reports generated from lower level to higher management level credit risk basel ii output reports generated for risk analysis. key responsibilities : requirement analysis and data modeling. mapping the functional and business requirement in pl sql programs. development of stored procedure , function , database cursor , triggers and custom programs in pl sql. enhancement of existing system , sanity verification between source and target. creation of backend code in pl sql for data loading development of pl sql object to populate the bi reporting table based on mapping business document. involved in the continuous enhancements and fixing of production problems company saksoft ltd. adm ( advance data mart ) sbi credit card role pl sql programmer duration from ( nov , 2010 ) to march 2011 team size 10 client ge money environment db oracle 10g tools business object , oracle warehouse builder , unix description adm is designed to help the management for decision making , it is a credit card data mart which is applicable for customer portfolio management and analysis based on the customer transaction , various fact and dimension tables has been designed to fulfill the requirement. and it also includes the information for ( bttc ) best time to call a customer which is only based on the historical data analysis. data migration has been done based on historical data to get the desired management level report. key responsibilities : development of functions procedures packages development of database objects like tables , views , packages , trigger , and indexes etc. enhancement of existing application. overseeing the quality procedures related to the project. bo reporting table designing based on business logic. tuning of query and program. company ib technology ltd. focus role pl sql programmer duration from ( march , 2008 ) to ( nov , 2010 ) team size 10 client indiabulls financial ltd. environment db oracle 9i tools form 6i , report 6i , toad , pl sql developer description focus has several modules and sub modules. it is client server based application. it handles all back office activities of indiabulls like when user gives request , and after performing some activities the request is updated through respective department through focus. for example suppose branch punch a new kit in iboss ( kit id is generated ) to open an account and send physical kit for franking , and after franking kit is received in back office , the back office check the received kit through focus and after verification client id is generated. there are different types of trading like trading in cash segment , trading in future and option segment , margin trading , commodities trading etc. there are different options for all related operations. one very important thing is security and privileges and focus provides privileges to users on focus operations. only authenticated users allowed operating s w. various levels of securities has maintained main modules are as below. admin , cash segment , future and option , master , audit trial , report generation key responsibilities : development of functions procedures packages development of database objects like tables , views , packages , trigger , and indexes etc. enhancement of existing application. overseeing the quality procedures related to the project. bo reporting table designing based on business logic. tuning of query and program. skillsdatabases oracle 8i , 9i , 10g , 11g microsoft access , sql server 2008 2012 2014 database tools toad , pl sql developer , oracle , sql developer etl tools teradata , teradata sql assistant big data hadoop , hive database languages sql , pl sql reporting tool report 6i , crystal report quality testing tools hp quality centre 11 , jira operating system windows nt , dos windows 95 98 2000 xp other tools tortoise svn , hp quality centre 11 , jira , unix client servicing interacting with the client for system study , requirements gathering , analysis and scoping. code debugging and troubleshooting the application. extending post go - live and application maintenance support to the client. daily defect call with overseas clients and providing resolution. a_arock start ace award : q4 , year 2016 in ercisson global india service pvt ltd. starawardin q3 , 2014 as best employee in aircom international india pvt ltd. educationmaster of computer application ( mca ) from iimt engineering college with 77 % from up technical university , lucknow.\",\n",
       " 'experienceworking as a senior software engineer at accenture from june 2014 to till date. worked as a software engineer at source one management services pvt ltd for client accenture from june 2013 to june 2014. worked at leiyo informatics pvt ltd from june 2010 to june 2013. highest qualification attained bachelor of technology in electronics and communications , 2006 - 2010. references please feel free to contact me for any more details to my email id or mobile number. current project project title : ncoc ( north caspian operating company ) . client : ncoc , kazakhstan duration : nov 15 to till date. description : input data was provided in excel , csv. data was filtered based on the criteria : location of area , type of document and location of unit. unique data has been maintained and sorted out based on plant. output exported from hdfs to mongodb using sqoop. finally , pig output has been stored in hive external table. responsibilities : gathered the business requirements from the business partners and subject matter experts technical assistance to the team setup the mapreduce configuration in the project involved in mongo db analysis and designing the hive table schema implemented pig and hive scripts for the fast retrieval of the data royal dutch shell project title : shell ncoc ( north caspian operating company ) . client : shell duration : feb 15 to oct 15. description : this project is all about the rehosting of their ( target ) current existing project into hadoop platform. previously target was using oracle db for storing their competitors retailers information [ the crawled web data ] . but as and when the competitor retailers are increasing the data generated out of their web crawling is also increased massively and which cannot be accommodable in a oracle kind of data box with the same reason target wants to move it hadoop , where exactly we can handle massive amount of data by means of its cluster nodes and also to satisfy the scaling needs of the target business operation. responsibilities : proactively involved and contributed in every stage implemented code optimization techniques and best practices gained good exposure to different basic components of hadoop project title : shell managed services team size : 150 environment : sharepoint , intergraph smartplant tools duration : september 14 to jan 15 description : accenture supports and maintain applications such as oracle primavera , intergraph smartplant , sharepoint and osisofts pi system for shell. tickets ( problems issues ) solving sharepoint portals 2010 and 2013 versions related queries and spf , vtl using service manager9 ( sm9 ) tool to shell world wide employees responsibilities : resolving issues raised by shell data and where loaded , integrated and transfer into operations using sharepoint and spf spo deploying new releases and enhancements to production. maintaining the windows servers and oracle database used for spf , vtl in full health worked as a tester for intergraph smartplant tools like sppid , spo and spi. responsible for the server - side administration of sharepoint that includes sharepoint software updates , sharepoint service maintenance , and sharepoint custom solution deployment. perform typical administrative activities such as backup , restore , and site creation. providing day - to - day administration and support of sharepoint environment including troubleshooting , monitoring , patch management , and problem resolution solid teamwork , interpersonal , and customer service skills ; ability to work well in cross - functional teams and foster team commitment to tasks. ability to interact effectively with all level of management , staff and members encountered in the courseof work. ability to work well under intensive deadlines. project title : shell integrated engineering environment ( iee ) . client : shell team size : ten environment : sharepoint 2010 , intergraph smartplant tools duration : april 14 to september 14 description : shell iee involves in testing and defect management of smartplant applications ported and sharepoint applications on cloud environment. responsibilities : provide technical expertise in identifying , evaluating , and developing effective procedures and systems requirements that meet business requirements. communicate conceptual and detail designs to client and development team. preparing test cases and test case execution worked as a tester for intergraph smartplant tools like sppid , spo , managing the defect management process. tester and configurator for sharepoint2010 web applications transferring knowledge successfully to users through formal presentations and informal training. projectproject title : spf - sharepoint bidirectional integration client : team size : five environment : sharepoint 2010 , sql server 2008 , intergraph smartplant applications sharepoint designer 2010. duration : june 13 to till date description : integrating sharepoint with intergraph smartplant owner operator ( spo ) applications , to be able to bi - directional synchronization the data between spo and sharepoint. responsibilities : preparing integration plan. developed custom sharepoint web services developed custom sharepoint server apis developing deploying the custom workflows. developing deploying the custom timerjobs. documenting the configuration created new custom master pages designed workflows using sharepoint designer 2010. installing third party webparts. migrated the application from sharepoint 2010 to sharepoint 2013. project title : migration from moss 2007 to sharepoint 2010 client : ing.com. team size : three environment : sharepoint 2010 , sql server 2008. duration : october 10 to june 13 description : worked on migration of sharepoint sites from moss 2007 to sharepoint 2010 using database migration approach. steps to follow in data base migration : - install sharepoint 2010 - backup the previous version of database - restore the backup copy to the new farm. - add the databases to the web applications - review the upgrade log files for any issues - repeat the restore and add database procedures for remaining databases responsibilities : preparing migration plan for portal. analysis of current portal ( infrastructure , database , sites , usage ) installing sharepoint 2010 developing deploying the custom web parts. installing activating 3rd party web parts. unused sites master pages clean up. created new custom master pages. co ordinate with business user for poc testing. post migration steps. documenting installation , migration and post migration steps. skillshadoop ecosystem mapreduce , hdfs , pig , hive , hbase , sqoop. databases sql server 2008 2012. oracle 10g big data platform cloudera hadoop distribution( cdh ) sharepoint technologies microsoft sharepoint server 2010 2013 , web parts , workflows spd 2010 2013 , bcs , caml , infopath 2010 , 2013 , fast search server 2010. nosql databases hbase , mongodb os windows , ubuntu , windows server 2008 2008r2 2012 languages sql , hiveql , pig latin , java , c#.net. tools eclipse , netbeans , sql developer web technologies html 5.0 , xslt , xml , soap , javascript , ajax , css 3.0 , jquery. ticketing tools shell service manager 9 , topdesk. scripting languages unix basic commands and shell scripting',\n",
       " 'experience5. l and t infotech : - august 29th 2016 - present type : pre - sales , product development and research specialties : big data , hadoop , spark , spark streaming , hbase , hive , sqoop , kafka , spark sql , spark ml , java springs , splice machine. duration : 6 months working as a technical lead for hadoop and spark projects. currently working as a product architect and also as technical presales consultant. i am part of mosaic.ds product engineering team where i work on cutting edge technologies like hadoop , spark and other big data frameworks to solve complex business problems which enables the companies to take accurate decision with the help of advanced analytics. 4. citiustech pvt ltd : - dec 10th 2014 - 12th aug 2016 type : product development and research specialties : big data , apache hadoop , map reduce , hbase , hive , sqoop , kafka , storm , solr , spark sql , java springs , core java , shell scripting , oozie. duration : 17 months working as a technical lead ( big data hadoop engineer at citiustech. installing and administration of cloudera 5.2 and hortonworks hdp 2.4 hadoop platform with its ecosystem projects hadoop performance tuning management tool - ambari tool , cloudera manager implementing kerberos security on the hadoop clusters commissioning and decommissioning of nodes of hadoop cluster deep expertise with hdfs for developing a healthcare data lake developed a framework for ingesting healthcare data into data lake using apache kafka and storm for high throughput developing error free map reduce code to perform processing of structured and semi - structured data which is ingested in data lake involved in performance tuning of mapreduce jobs developing an oozie workflow and coordinator for automated scheduling of job worked on spark sql to support real time analytics for customers developing automated and integrated test case using testng framework well - versed with agile and scrum methodologies writing junit classes for unit testing leading a team size of 4 junior engineers. design and deliver training programs on hadoop to team mates project3. vistaar systems pvt ltd , mumbai 9th april 2012 - november 2014 type : application development software : core java , java collection , hadoop , map reduce , sqoop jan o workedconecpts such as olap , entity modelling , workflow. avascript , rhino script , shell scripting. database : oracle 11g duration : april 2012 to november 2014 working as a senior application developer. in customer facing role. implementation of the solution as per customer requirements using product features. involved in requirement analysis which included having frequent calls and meetings with customers. involved in designing , coding , testing and documentation and code review. understanding customer requirements and delivering the same before time. perform uat for customer. also working as an acting team leader and heading a team of 3 engineers. giving them technical and functional guidance. also worked on defects , enhancements and performance related issues reported by customers. having knowledge of price analytics domain. working on concepts such as olap , entity modelling , workflow. also worked on hadoop framework for parallel processing. 2. amdocs pvt ltd : - june 13th 2011 - 6th april 2012 type : product development and maintenance software : core java , java collection , java hibernate. database : oracle 10g duration : 10 months worked as a subject matter expert. involved in optimizing code and modules. analyzing the bugs reported by customer , fix it and deliver it to customer on time. testing and fixing bugs. responsible for creating defect prevention plans , implementing and reviewing the plan. responsible for documentation. here , i am working as a subject matter expert for their telecom billing product. the work profile here consists of both technical as well as functional aspect of telecom billing product. the responsibilities here consist of analysis , coding , testing , bug - fixing and documentation. the work over here includes development and maintenance in core java and hibernate framework. the database used is oracle 10g. the modules in which i worked were amdocs rater and enterprise product catalog. this modules involved concepts such as multi - threading , file i o , hibernate framework , collections , exception handling. this rater module actually rates for all the services used by the subscriber including prepaid and postpaid. the product catalog module is the module in which all the implementation of the logic of rating the services is written and dependent on this module the rating is done. 1. vistaar systems pvt ltd , mumbai april 2010 to 10th june 2011 vistaar systems is a product company and has developed a software in pricing analytics domain for sales , marketing and finance. this software is useful for price setting , price execution and price strategy. vistaar works with the best companies in the world to transform pricing operations into a strategic advantage. for these companies , vistaar pricing software drives measurable price and margin improvements that deliver profitable growth and maximum shareholder value. worked as a software engineer. in customer facing role. worked on crs involving coding , testing , documentation. analyzing , coding , testing and documentation of solution related defects and enhancements reported by the customer. constantly monitoring and taking care of the customers hardware system where software is deployed. key projects handled at vistaar : - support and maintenance project( hitachi data systems and beam global spirits ) : - type : application development. platform : linux software : core java , javascript , shell script. database : oracle client : hitachi ( u.s ) , beam duration : 5 months here i worked as an application consultant. i was supposed to understand the customer specific implementation and address any defects or enhancements related to the solution. the work on this project included giving dedicated 24*7 support to the customer. also working on any defects related to product and also implementation issues. also worked on a tool called report builder. here i was included in developing the report and customizing the report. the work here included designing the report in vba. the task was to prepare a new reporting tool for the demo to customer so that customer can just analyze all the business measures with just click of button. implementation project( ford motor company ) : - type : application development. platform : linux software : java , javascript , vba , shell script. database : oracle client : ford motor company ( europe ) duration : 9 months here , i worked on a new project starting from scratch. after gaining the knowledge of the product and implementation i was assigned this project. the key responsibilities here were making and enhancing reporting workbooks which included vba designing , development and testing. the project also demanded me to do coding , testing in java and javascript. skillscertification : sun certified java programmer ( scjp 5 ) , cloudera certified developer for apache hadoop. programming languages : core java , java collection , javascript , rhino script , hadoop framework , map reduce , hive , impala , sqoop , kafka , spark core , spark sql , spark streaming , sparl ml , splice machine , apache zeppelin , java springs , spring jdbc , hibernate , shell scripting. databases : oracle 10g , oracle 11g , mysql , hive. tools : microsoft office , eclipse for java , maven. scala ide. job functions requirement analysis , design , coding , testing , documentation , review , bug fixing , maintenance and re - engineering. 1. certified sun programmer [ scjp 5.0 ] . 2. cloudera certified for apache hadoop [ ccd - 410 ] . 3. multi - platform experience in software design and development. 4. excellent in object oriented analysis and designing [ ooad ] . 5. excellent debugging and bug - solving skills. 6. excellent communication and presentation skills. seminars , workshops and extra curricular activities : 1. art of living course from yes ! +. 2. playing cricket for brothers cricket club from past 6 yrs. educationb.e ( computer engineering ) courses board university pass out marks obtained b.e ( computers ) mumbai university shah and anchor engineering college may - 2009 58 % hsc mumbai university somaiya college feb - 2005 71.17 % ssc mumbai university smt. vidyaben d gardi high school mar - 2003 80.66 % total experience : - 6 years 11 months( 83 months ) 1. l and t infotech ( september 16 to present ) 2. citiustech pvt ltd ( december 14 to august 16 ) 3. vistaar systems pvt ltd ( april 2012 to november 14 ) 4. amdocs pvt ltd ( june 2011 - april 2012 ) 5. vistaar systems pvt ltd ( april 10 - june 2011 )',\n",
       " 'experiencecurrent employer : aug 2014 to till date at ibm. organization : ibm india pvt ltd , bangalore my position : application developer previous organization details : july 2010 july 2014 organization : axa technologies shared services , bangalore my position : senior associate in document management solution projecttitle iard , msc_mrh client axa group solutions , paris project description : calligo is an enterprise software platform that complements core business systems by managing simple to complex , event - driven document. developed insurance document ( majorly letters to the clients ) in calligo , xml file for testing. software : calligo role : developer duration : november 2010 to jan 2011 team size : 2 roles and responsibilities analyzed and discussed the business requirements with the onsite business analysts team. did estimation for the project and later for change request( s ) . designed the template ( letters to axa clients ) , xml file. generated and verified all the possible cases of the output with using xml followed by bug - fixing ( if required ) . done the estimation for the change request. provided daily status updates of the project development to the onsite lead daily through mails and teleconference. title calligo migration client axa group solutions , paris project description : this project involves the replacement of the calligo document generation product with dialogue. dialogue , a hewlett packard product , is the enterprise output platform. in editique ( printing department of axa insurance ) there are 100 calligo documents ( letters ) which were are migrated migrating to hpd. software : dialogue v8.1 ( hp exstream ) and live editor role : developer duration : jan 2011 to nov 2011. team size : 3 roles and responsibilities analyze and discuss the business requirements with the ags team in paris. take the production output of calligo templates replicate the objects and conditions in hpd create the xml for hpd report , maintain the same structure as used for calligo report. create the variables , map the data , and write the rules and functions as per calligo report. create components , reference files for images , footers. use live to create the check boxes and make the editable paragraphs for users. generate outputs for all possible test cases and verify using calligo production report. provide maintenance support as per the sla , fix the defects and work on cr. title family protect client axa group solutions , france , italy , belgium , spain. project description : this project involves the document generation product with dialogue for different country. dialogue , a hewlett packard product , is the enterprise output platform. this project is different documents for different country and language. software : dialogue v8.1 ( hp exstream ) and live editor role : developer. duration : dec 2011 to february 2013. team size : 2 roles and responsibilities analyze and discuss the business requirements with the ags team in paris. based on the functional requirements created low level design document ( technical doc ) . used language layers. create components , reference files for images , footers. use live to create the check boxes and make the editable paragraphs for users. provide maintenance support as per the sla , fix the defects and work on cr. study the requirements , analyze and create document. page design and optimization. designed the form and table. coding the logics and crating relationship between tables. tracking of issues related with client reviews. estimating the change requests and regression testing. designed the pages for various letters with different paper types. done all the tracking of various issues related with the production run. xml data file mapping and formula creation to implement clients business rules. proposed graphical charter to clients. data matrix and qr code implementation. dse creation. title bau client axa group solutions and axa france. project description : this project involves the working on new change requests , a hewlett packard product and calligo , is the enterprise output platform and got opportunity to work on different project. software : dialogue v8.1 live editor ( hp exstream ) and calligo role : developer. duration : jan 2013 to july 2014. team size : 3 roles and responsibilities analyze and discuss the new change request with business analyst of ags team in paris. based on the functional requirements created low level design document ( technical doc ) . create components , reference files for images , footers. use live to create the check boxes and make the editable paragraphs for users. provide maintenance support as per the sla , fix the defects and work on cr. study the requirements , analyze and create document. designed the form and table. coding the logics and crating relationship between tables. tracking of issues related with client reviews. estimating the change requests and regression testing. done all the tracking of various issues related with the production run. xml data file mapping and formula creation to implement clients business rules. integration test and worked on the e - validation application. title solaries - iard client axa group solutions and axa france. project description : this project involves the working on new change requests , a hewlett packard product is the enterprise output platform and got opportunity to work on different templates. software : dialogue v8.1 live editor ( hp exstream ) role : developer. duration : jan 2014 to july 2014. team size : 1 roles and responsibilities analyze and discuss the new change request with business analyst of ags team in paris. based on the functional requirements created low level design document ( technical doc ) . create components , reference files for images , footers. use live to create the check boxes and make the editable paragraphs for users. provide maintenance support as per the sla , fix the defects and work on cr. study the requirements , analyze and create document. designed the form and table. coding the logics and crating relationship between tables. estimating the change requests and regression testing. xml data file mapping and formula creation to implement clients business rules. title fom - factuur mobiel client kpn telecoms. project description : this project involves the creating new invoice and call detail records , a hewlett packard product is the enterprise output platform and got opportunity to work on different templates. software : dialogue v9.0.104 ( hp exstream ) role : developer. duration : aug 2014 to jan 2016. team size : 2 roles and responsibilities analyzed the business requirements( br ) and based on the that created functional design document. proposed new design ( graphical charter ) and got approved from the kpn top management. based on the functional requirements created low level design document ( technical doc ) . create components , reference files for images , footers and initialization file for payment messages. study the requirements , analyze and create documents for including vat and excluding vat. used data sections in the customer driver file ( flat file ) from informatica team , complex tables are designed with data sections. coding the logics and crating relationship between tables. tracking of issues related with client reviews. estimating the change requests and regression testing. done all the tracking of various issues related with the production run. xml data file mapping and formula creation to implement clients business rules. invoice generated report files are created with functions. deployed all the files to ecm. title sic - cta claims client sic insurance. project description : the california teachers association ( cta ) web based application is currently using ibm forms server to generate pdf documents. sic is looking to convert the form processing from ibm forms to hp exstream to consolidate to one technology. software : dialogue v8.6.106( hp exstream ) , soapui 5.0.0 , ews 2.0 role : ba , developer. duration : feb 2016 to sept 2016. team size : 1 roles and responsibilities analyzed ibm forms , based on that created cta claims design document and got approval from clients. proposed new design ( graphical charter ) and got approved from the sic top management. based on the functional requirements created low level design document ( technical doc ) . created. fna file ( reportfile ) tracking of issues related with client reviews. estimating the change requests and regression testing. done all the tracking of various issues related with the production run. xml data file mapping and formula creation to implement clients business rules. invoice generated report files are created with functions. this is on - demand application , set up of end to end process has been taken care. hpe web services is be used to generate the claim forms. title sic - ams client sic insurance. project description : this project involves the working on new change requests and production support , a hewlett packard product , got opportunity to work on many application including on - demand and batch. software : dialogue v8.6.106( hp exstream ) , soapui 5.0.0 , ews 2.0 role : ba , developer. duration : sept 2016 to till date. team size : 2 roles and responsibilities analyze and discuss the new change request with business analyst of sic team. based on the functional requirements created low level design document ( technical doc ) . create components , reference files for images , footers. study the requirements , analyze and create document. designed the form and table. coding the logics and crating relationship between tables. tracking of issues related with client reviews. estimating the change requests and regression testing. done all the tracking of various issues related with the production run. xml data file mapping and formula creation to implement clients business rules. worked on production support and available on time when its needed. skillsarea name level operating system : windows 95 98 2000 xp expert tool : hp dialogue v8.0 , v9.0 live editor , soup , calligo , evalidation , clearcase and clearquest expert bigdata : infosphere streams , spl intermediate database : db2 , sql , intermediate : insurance , telecom intermediate a_awent onsite to paris 2 times for transition of project work to india. won prize for cultural events. nominated for the best employee of the year 2012 from our team attended infosphere stream training at ibm project details. educationb.e.computer science , east west engineering collage ( vtu ) , bangalore , karnataka. trained on exstream dialogue application development from hp - usa. puc science - sspu college davangere , karnataka. sslc - mmghs holalkere , karnataka.',\n",
       " 'experiencebig data hadoop professional with overall 8.3 years it experience in analytics , software development , software testing , application maintenance and support. currently associated with ibm india pvt ltd pune as bigdata hadoop developer. artificial intelligence and analysis of big data. good knowledge and understanding of the practical applications of hadoop ecosystems hdfs , map reduce , hive , sqoop , hbase , oozie , zookeeper , bigsql experience in the programming language java. good experience on linux operating system. proven experience in all the phases of software development life cycle ( sdlc ) such as requirements and analysis , design and construction , testing and support , basic admin task. adapt in analyzing information system needs , evaluating end - user requirements , troubleshooting and debugging of application related issues. proficient in handling projects efficiently for delivering the required applications in systematic way. excellent communication , interpersonal and analytical skills with proven abilities in resolving the complex software issues. ability to work well in both team and individual environment. manual testing , test case preparation , monthly code review document ( mcr ) worked as team lead , so experience in managing team. ibm india pvt.ltd. sept 2010 till date ibm pvt.ltd. is a cmm level 5 company with an onsite presence at key locations globally , the company is headquartered in usa. platform : linux software : cloudera vm ware duration : april 2014 april 2014 team size : 20 description : 5 week full time mobilization program conducted by ibm. responsibility : attended 5 weeks classroom training on big data hadoop , hdfs , mapreduce , hive , pig , sqoop , flume , hbase , oozie. completed all exercises given in training. projectidea cellular environment big data hadoop , hdfs , map reduce , hive , hbase bigsql , sqoop , bigsheets language : java duration : july 2015 till date team size : 9 description : idea cellular , commonly referred to as idea , is an indian mobile network operator based in mumbai , india. idea is a pan - india integrated gsm operator offering 2g and 3g mobile services. idea is indias third largest mobile operator by subscriber base. our teams responsibility is to do analysis on all sort of data using hadoop biginsight. and convert vigilance applications backend connectivity from oracle to hbase. responsibility : 1. analysis of the specifications provided by the clients. 2. preparation of analytical requirement use cases documents. 3. import data from oracle to hdfs using sqoop 4. write mapreduce code programs 5. write hive queries. 6. run queries on bigsql tool 7. generate reports using bigsheets , cognos with generated output. 8. import data to apache solr environment 9. change backend connectivity from oracle to hbase 10. convert all oracle queries to suitable format of hbase bigsql tables vodafone environment big data hadoop , hdfs , map reduce , hive , bigsql , sqoop , bigsheets language : java duration : june 2014 july 2015 team size : 5 description : idea cellular , commonly referred to as idea , is an indian mobile network operator based in mumbai , india. idea is a pan - india integrated gsm operator offering 2g and 3g mobile services. idea is indias third largest mobile operator by subscriber base. our teams responsibility is to do analysis on all sort of data using hadoop biginsight. and convert vigilance applications backend connectivity from oracle to hbase. responsibility : 1. analysis of the specifications provided by the clients. 2. preparation of analytical requirement use cases documents. 3. import data from oracle to hdfs using sqoop 4. write mapreduce code programs 5. write hive queries. ford motors environment big data hadoop , hdfs , map reduce , apache pig , hive , sqoop language : java role : hadoop developer duration : may 2014 june 2014 team size : 3 description : ford is the second - largest u.s. - based automaker. we get input data to do analysis and give report based on output. responsibility : 1. analysis of the specifications provided by the clients. 2. preparation of analytical requirement use cases documents. 3. transfer all log files into hdfs location 4. import data into hdfs using sqoop 5. write mapreduce code that process for text files. 6. write hive queries 7. generate reports using bigsheets with generated output. big data hadoop mobilization program environment big data hadoop , hdfs , map reduce , apache pig , hive , sqoop , flume , oozie. type : danisco type : development , maintenance and support. platform : windows 7 server , apache tomcat 5.x , websphere 7.0 software : eclipse client : danisco , sweden role : java application developer duration : july 2012 march 2014 team size : 5 languages java description : danisco a sis adanishbio - based company with activities infood production , enzymesand otherbio - productsas well as a wide variety of pharmaceutical gradeexcipients. we support many lotus notes databases from regions emea , apac , uk , america. worked on application used in the laboratory advanced analysis for managing analysis tasks. this include some group who receive a sample from other laboratory application group customer and perform a number of analysis on these. it may include fingerprinting , spectroscopy and other predefine identification analysis. this application uses web services to connect to the eln software in which folder corresponding to multiple analysis get created automatically upon the completion of all analysis. this application uses ldap and oracle authentication. responsibility : a. gathering business requirements , worked on design , configuration and preparation of functional specification documents. b. creating blueprint documents and worked on to model , design , develop , test , and implement new functionality. c. post production support. dupont type : development , maintenance and support. platform : windows 7 server , lotus notes r8 x server software : lotus notes r8 x clients client : dupont wilmington , usa role : team lead , developer duration : oct 2010 july 2012 team size : 20 languages lotus script , formula language , x - pages and java script description : dupont puts science to work by creating sustainable solutions essential to a better , safer , healthier life for people everywhere. operating in more than 70 countries , dupont offers a wide range of innovative products and services for markets including agriculture , nutrition , electronics , communications , safety and protection , home and construction , transportation and apparel employees : 67 , 000 worldwide global : operating in more than 90 countries worldwide r and d : more than 75 research and development and customer service labs in 12 countries around the world. we support many lotus notes databases from regions emea , apac , uk , america. responsibility : 1. developing new applications 2. providing functional support maintenance to all the applications 3. interaction with osc for requirement gathering. 4. detailed analysis of the requirements gathered from the osc or the client. 5. impact analysis of the change request prior to approval. 6. code creation for change requests. 7. support activity. 8. documentation and testing 9. pql activities responsible for all quality related documents for the project 10. ensuring review of each task capgemini pvt.ltd. july 2007 sept 2010 capgemini pvt.ltd. is a cmm level 5 company with an onsite presence at key locations globally , the company is headquartered in paris. my job profile consists of working as a consultant. huntsman lotus notes ams type : maintenance and support. platform : windows nt 4.0 server , lotus notes r6.5.x server software : lotus notes r6.5 x clients client : huntsman international llc , houston , usa role : developer duration : april 2008 sept 2010 team size : 6 languages lotus script , formula language , java script description : huntsman is a global manufacturer and marketer of differentiated chemicals. huntsman has many business units all over the world. we support many lotus notes databases from regions emea , apac , uk , america. responsibility : 1. providing functional support maintenance to all the applications 2. interaction with osc for requirement gathering. 3. detailed analysis of the requirements gathered from the osc or the client. 4. impact analysis of the change request prior to approval. 5. code creation for change requests. 6. documentation and testing 7. pql activities responsible for all quality related documents for the project 8. ensuring review of each task spectra energy type : development , maintenance and support. platform : windows software : .net , team foundation server client : spectra energy , usa role : manual testing duration : july 2007 april 2008 team size : 4 languages .net description : spectra energy transmission , llc ( spectra ) , formerly known as duke energy gas transmission , llc have been using lotus notes domino hundreds of applications over the last several years. responsibility : 1. analyzing the use cases and business requirements. 2. test case creation and review process. 3. test case execution and defect tracking ( ui , functional , security ) for the assigned applications. 4. defect prevention measures by complying with high degree of testing , reviews and regression testing so as to minimize the defects in the work product. 5. support for user queries skillsoperating systems linux unix , windows 7 , windows xp big data technologies apache hadoop , hdfs , mapreduce , hive , sqoop , hbase , oozie , zookeeper , bigsql and ibm biginsight languages java databases sql , oracle , no sql hbase concepts and processes big - data , hadoop , sdlc , agile methodology requirement analysis , design , coding , documentation , testing , functional support defining escalation response resolution time for reported problems on the basis of criticality , mentoring team and handling deployment of tasks. client interaction , end to end query management , project management , business quality processes. a_ad. received best employee award e. participated in sharenet 2011 program. education1. ibm big data fundamentals technical mastery test v1 ( 000 - n32 ) 2. ibm lotus notes 7 3. international standard testing quality board( istqb ) b.e information technology s.r.t.m.university , nanded 66.13 % h.s.c. latur divisional board 71.17 % s.s.c. latur divisional board 76.8 %',\n",
       " 'summary having around 6.3 years of experience in software development in c# , java , sql server and hadoop development related work. hands on experience bigdata hadoop - mapreduce , pig , hive , hbase , sqoop and flume , hadoop - yarn , basic knowledge of cassandra and java api to work on it. hands on to connect and read data from hive and mysql using prestodb with php client. experienced in installing , configuring , and administrating hadoop cluster of major hadoop distributions ( cloudera ) importing the data from sql server to hdfs and exporting too using sqoop. streaming the unstructured data from web sources using flume. good exposure on usage of nosql database. having java development skills using spring , mvc with rdbms mysql. have hands on experience in writing mapreduce jobs in java and optimizing performance. having experience in crystal reporting tool from scratch. excellent analytical , problem solving , communication and interpersonal skills with ability to interact with individuals and can work as part of a team as well as independently. ability to perform at a high level , meet deadlines , and adaptable to ever changing priorities. experiencerole : software engineer - c# and sql server developer project description : system design of erp for all kinds of manufacturing industry. the project includes multiple modules like order acceptance , sub contract purchase order , work order cycle , purchase order cycle. responsibilities : modification rectification of new project. taking approval for new projects from customers. new developing a project as per requirement of customer. solving a problem in project hadoop developer profile environment : c# , sql server , crystal report projectproject description : report development responsibilities : modification rectification of new project. developed new report as client requirement in ssrs crystal. wiring stored procedures , cursors function to fetch and process data as per requirement. taking approval for new projects from customers. new developing a project as per requirement of customer. solving a problem in project hadoop developer profile project : yardi voyager 7s - java developer project description : commercial property management software product and integrated accounting system for office , retail , and industrial properties ( real estate ) . yardi voyager is a fully - integrated , web - based , mobile enabled enterprise management system designed for property owners , managers , and investors in global real estate markets. there is a voyager software solution designed to meet the specific requirements of every real estate market , including commercial ( office , retail , industrial ) , automating processes with role - based dashboards , leasing work flows , critical date notifications , and analytics. enabling faster and more efficient execution of property operation strategies , voyager helps drive improved analysis and decision - making and delivers maximum transparency across your business. responsibilities : gaining product knowledge and understanding financial and property management domain.7 consultation with client and account managers to identify key requirements and issues faced by client. development and modification of standard and custom reports using ssrs , crystal and yardis reporting tools. troubleshooting and providing permanent fix to the bugs faced by end users. debugging the code to understand the application flow and logic. environment : java1.6 , spring 3.0 , maven , eclipse , my sql sai application integrator s pvt ltd pune. project description : this is a desktop and web based application , is designed for maintaining patient health records with easy to use and interactive view. it provides easy access to doctor and nurse , so they can check patient past record. with payment records and multiple facilities like report generation , mailing , bill generation , pathology record and consultation management. responsibilities : modification rectification of new project. taking approval for new projects from customers. new developing a project as per requirement of customer. solving a problem in project environment : c# , sql server , crystal report project description : this is a desktop application , is designed for maintaining customer record with easy to use and interactive view. it provides easy access to admin and all employee from the automotive , in this system we maintain the stock of each item from store and also we stored record in db and it will give advanced suggestion for servicing for specific customer depending on date. in that system also generates invoice and receipts for customer. responsibilities : requirement gathered and designed project flow modification rectification of new project. taking approval for new projects from users .. new developing a project as per requirement of product. solving a problem in project environment : c# , sql server , crystal report hadoop developer profile skillsbig data : mapreduce , hdfs , hive , hbase , pig , sqoop , and flume , nosqlcassandra , prestodb , mongodb , languages : java , sql , c# data bases : oracle 10g 11g , sql server 2008 technologies : java , spring 3.0 , mvc architecture. framework : spring 3.0 , hadoop , map reduce. application server : apache tomcat 6.0 7.0 , jboss 4.0 ide tools : eclipse. operating system : ubuntu , windows. reporting tools : crystal , ssrs , yardi reporting frameworks. mailto : abhijitsamage@ymail.com hadoop developer profile educationtype university institute class mca bharati vidyapeeth , university , pune. first bsc shivaji university kolhapur. first experience : 4 year 6 months and ongoing. yardi software india pvt. ltd. pune role : software engineer - hadoop developer project : sentiment analysis using twitter. project description : companys internal r and d project to learn hadoop ecosystem. the objective of the project is to find the sentiments of the users on twitter. companies can analyze the sentiments of the users regarding their products services and use it for betterment of the same. the reason we chose twitter is because the social media is gaining popularity for the customer reviews and it is also creating a good business - customer relationship in the market. the reviews would certainly reflect the service of the company in the market. covers the following processes : collect data : the initial step is to collect all twitter tweets. pre - process data : you need to write mr job to pre - process the data. classification of data : write hive udf for the classification of the data into positive negative opinions. print data : final stage would be to print the desired results with the number of good and bad tweets collected. responsibilities : responsible for understanding the scope of the project and requirement gathering. installing and configuring of hadoop framework , pig , hive an flume. live streaming of tweets from twitter to hdfs using twitter4j api through flume. parsing of raw json data and extract relevant information from it through map reduce ( logic written in using java api as well as pig scripts ) classification of the data into positive negative opinions using hive. hadoop developer profile project description : e commerce product review and customer feedback analysis system yardimarketpalce.com is an internet and ad listing company offering a digital platform of marketplaces for real estate material around multiple consumer needs in over indian and american as well as canadian cities. the scope of the project included designing and deploying a production to product review and customer feedback analysis system using that admin and manager team came to know the review of product and customer feed back. responsibilities : developed multiple mapreduce jobs in java for data cleaning and processing. set up 5 node cluster on commodity type hardware and manage it. worked on java code to optimizing and tuning recommender system to achieve optimal performance. experienced in defining job flows ; managing and reviewing hadoop log files. experienced in running hadoop jobs to process terabytes log format data. supported map reduce programs those are running on the cluster. involved in loading data from unix file system to hdfs. installed and configured hive and trained client to use it according to their need. involved in creating hive tables , loading with data and writing hive queries. environment : map - reduce , hadoop , hdfs , hive , java ( jdk1.7.0_45 ) , flat files , hql and unix shell scripting.',\n",
       " 'summaryhave total 8 years of overall experience in hadoop and java j2ee development and support related activities. 3+ years of experience in handling big data with hadoop cluster. strong experience in processing big data and analyzing the data using mapreduce , hive , pig , spark - core. sparksql , spark - streeming , kafka , aws ( amazon web services , s3( simple storage services ) , dynamodb and redshift. involved in installation , configuring , development of hadoop , big data and hdfs file systems. skillslanguages : java , j2ee , scala and basics of python j2ee common services apis : java servlets , jdbc , jsp , hibernate , xml , struts , spring mvc framework bigdata stack : hdfs , mapreduce , spark ( core , sql , streeming ) , hive , pig , hbase , sqoop , flume , nosql( redshift ) work flow tools : azkaban and oozie storage stack : amazon s3 , dynamodb operating systems : windows , linux( ubuntu , centos ) educationmca ( master of computer applications ) from osmania university , hyderbard in 2007 with 67 % . b.sc ( computer science ) from sri venkateswara university , tirupati in 2004 with 68 % .',\n",
       " 'summaryaround 9.5+of experience overall experience in it and software development and service around 2.5 years of experience in big data technologies with spark , scala and hive experience in using hive , scoop , h base and cloudera manager. responsible for building scalable distributed data solutions using hadoop. experienced in database design , development and support of ms sql server 2008 for production development. transformation of data from one server to other servers using tools like bulk copy program ( bcp ) , dts ssis packages. basic knowledge of microsoft business intelligence tools like ssas and ssrs excellent analytical , communication and interpersonal skills. proficient in technical writing and presentations and a good team player. techno - functional responsibilities include interfacing with users , identifying functional and technical gaps , estimates , designing custom solutions , development , leading developers , producing documentation , and production support. completed oca and mcitp certifications from oracle and microsoft. experiencetitle : us oncology data transformation client : us and european countries role : spark and scala developer company : quintiles ims , bangalore duration : july - 2016 to till date responsibilities load data from text files to hdfs using data brick libraries loading reference data from sql server to hdfs using sqoop. load the data to spark rdd and data frames. write scala code to implement the business rules perform data transformation using spark sql title : dt1 data transformation client : asia pacific countries role : spark and scala developer company : quintiles ims , bangalore duration : jan - 2015 to jun - 2016 responsibilities configure metadata for file using big data composer system. generate the meta data in json files build the logic to de serialize the json using spark libraries load data to spark data frames performing data validation and apply the various business rules save the good data to hive involved in creating the unit , end - to - end and integration test cases using scala test title : apps india sales audit role : sql server 2008 and ssis developer company : ims health india private limited , bangalore duration : jan 2013 to dec - 2014 responsibilities involved in understanding the existing data acquisition systems used in various asia pacific countries this involved processing of supplier files through staging and moving this to data warehouse systems built the stored procedures , tables , indexes , functions for new processes performed end to end testing which were part of data warehouse project worked on sql server tools like enterprise manager and sql query analyzer. used fuzzy logic transformation in ssis to build packages worked with database installation and configuration of databases title : qbase data handling role : sql server 2000 2008 developer company : akzo nobel india limited , bangalore duration : aug 2009 to dec - 2012 responsibilities building tsql scripts and testing the scripts in development and acceptance environment. rollout of scripts in production environment restructuring the stored procedure , designing , coding and developing the same using the t - sql programming to satisfy various business needs. export or import data from other data sources like flat files using import export through dts building tsql scripts and testing the scripts in development environment rollout of scripts in production environment restructuring the stored procedure , designing , coding and developing the same using the t - sql programming to satisfy various business needs. worked on sql server tools like enterprise manager and sql query analyzer. title : qbase product conversions role : sql server 2000 2008 developer company : akzo nobel india limited , bangalore responsibilities building tsql scripts and testing the scripts in development environment rollout of scripts in production environment restructuring the stored procedure , designing , coding and developing the same using the t - sql programming to satisfy various business needs. worked on sql server tools like enterprise manager and sql query analyzer. title : zilla panchayat accounting system role : oracle , pl sql , d2k developer company : c.m.c limited , bangalore duration : aug 2008 to july 2009 responsibilities created tables , functions , pl sql stored procedures and triggers. developed pl sql packages , functions and procedures for the back end processing of the proposed data base design. developed pl sql packages , functions and procedures for the back end processing of the proposed data base design and linking. conducted pl sql training session for co - workers to educate about the latest pl sql features performed the set up the vss source code. title : karnataka treasury department ( ktd ) role : oracle , pl sql , d2k developer company : c.m.c limited , bangalore duration : feb 2008 to july 2008 responsibilities created tables , functions , pl sql stored procedures and triggers. developed pl sql packages , functions and procedures for the back end processing of the proposed data base design. developed pl sql packages , functions and procedures for the back end processing of the proposed data base design and linking. conducted pl sql training session for co - workers to educate about the latest pl sql features skillsbig data technologies : hadoop , hdfs , mapreduce , apache hive , apache spark. databases : ms sql server 2008 2012 , development tools : intellij idea programming : c , c++ , sql , scala testing : scala test , fun suite build : maven , sbt educationbachelors in computer science engineering , from v.t.u university in 2007 , karnataka , india',\n",
       " 'summary2 years of experience in it industry oracle( pl sql ) and including bigdata( hadoop frame works and eco systems ) . hdfs , mapreduce and hadoop ecosystem ( pig and hive ) , and pl sql 1.8 years experience in oracle( pl sql ) developer. created packages , procedures , functions , triggers , cursors using pl sql. currently working on our bigdata infrastructure build out for batch processing as well as real - time processing. capable of processing large sets of structured , semi - structured and unstructured data and supporting systems application architecture. proficient in processing large sets of structured , semi - structured and unstructured data and supporting systems application architecture. a true team builder having natural motivation with strong analytical and problem solving skills. experiencehadoop developer ibm technologies ltd. jul2014 - tilldate descrption : travelclick provides innovative solutions for hotels around the globe that increase revenue , reduce cost , and improve performance. the travelclick collects terabytes of raw data including 10 billion hotel rates. project involved migrating data from sql , sqlserver database and various other sources to hadoop to perform analysis to drive hotel marketing , revenue management , and business strategy. responsibilities : developed big data solutions that enabled the business and technology teams to make data - driven decisions on the best ways to acquire customers and provide them business solutions. involved in installing , configuring and managinghadoop ecosystem components like hive , pig , sqoop and flume. migrated the existing data to hadoop from rdbms ( sql server and oracle ) using sqoop for processing the data. responsible for loading unstructured and semi - structured data into hadoop cluster coming from different sources using flume and managing. environment : hadoop , mapreduce , hive , pig , oozie , sqoop , flume. skillshadoop ecosystem hdfs , map - reduce , hive , pig , sqoop , flume , oozie. database oracle ( sql , pl sql. ) . programming core java and c language. operating systems linux , windows family. educationclass year institution % cgpa b.tech ( ece ) 2014 skd engineering college , j.n.t.u. anantapuramu , a.p. 70 % 12th class 2010 narayana junior college , kurnool 86 % 10th class 2008 a. p. residential school , srisailam , a.p. 72 % oracle project client : finacle banking populating data into 10.x tables role : developer os : windows and linux. duration from aug 2014. description : the tables present in finacle 7.x needs to populated with data from flat files.this document will give detailed insight about how to load data using sql loader. migration upgrade information roles and responsibilities : need to create the tables scripts for the table which are dropped. based on the drop trunc tables the action will be performed. preparation of documentation on application databases objects. analyzed the user requirements. development of code based on the approved technical specifications. created views from multiple tables. used sql*loader to load bulk data from various flat files. development of re - usable objects like general procedures , functions and packages. create the objects like tables , synonyms , sequence.',\n",
       " 'summaryexperienced professional hadoop developer with bachelor of computer science and proficient knowledge in apache hadoop and its ecosystem components like map - reduce , hdfs , hive , pig , sqoop , hbase , flume , spark - core , core java , and mongo db and currently working as a software engineer in jinit technology pvt. ltd , pune. having 2.10 years of experience in it with big data technology. good working experience in using apache hadoop and its ecosystem components like map - reduce , hdfs , hive , sqoop , flume. good knowledge in spark , scala and mongo db. sound knowledge in pig , hbase. good knowledge about udf. responsible for data ingestion , data integration and data processing. good working knowledge on core java concept. very flexible and can work independently as well as in team environment. willing to update my knowledge and learn new skills according to business requirement. experiencecurrently working with jinit technology pvt. ltd. as software engineer from june 2014 to till date. projectname : regulatory marketing environment : spark , scala , hive , sqoop , flume , rest services , java , rdbms. platform : cdh 5.x period : march 2016 to till date. team size : 11 description : the bank wanted to help its customers to avail different products of the bank through analyzing their expenditure behavior. the customers spending ranges from online shopping , medical expenses in hospitals , cash transactions , debit card usage etc. the behavior allows the bank to create an analytical report and based on which the bank used to display the product offers on the customer portal which was built using java. the portal allows the customers to login and see their transactions which they make on a day - to - day basis. this analytics also help the customers plan their budgets through the budget watch and my financial forecast applications embedded into the portal. the portal helped plan the savings and daily pigmy amount for the customers based on their earnings and needs. importing data from legacy system to hadoop using flume and sqoop. writing cli commands using hdfs. writing hive scripts to creating tables and loading data into hive. develop spark application for preprocessing data. performing data analysis using hive. sourcing various attributes to the data processing logic to retrieve the correct results name : business marketing intelligence environment : mapreduce , hive , sqoop , flume , rdbms. platform : cdh 4.x period : sep 2014 to feb 2016 team size : 10 description : the purpose of this project is to store terabytes of information from the web application and extract meaningful information out of it. the solution was based on the open source bigdata s w hadoop .the data will be stored in hadoop file system and processed using map reduce jobs. which in turn includes getting the raw html data from the micro websites , process the html to obtain product and user information , extract various reports out of the visitor tracking information and export the information for further processing. move all crawl data flat files generated from various micro sites to hdfs for further processing write mapreduce scripts to process the data files create hive tables to store the processed data in tabular formats reports creation from hive data skillsbig data ecosystems : hadoop , hdfs , hbase , map reduce , sqoop , hive , pig , spark - core , flume. other language : core - java , scala. tools : eclipse. platforms : windows family , linux unix. databases : mysql , oracle , mongodb educationb.c.s ( bachelor of computer science ) from dr. babasaheb ambedkar marathwada university , aurangabad with 80.67 % in year 2011 - 2014.',\n",
       " 'summaryaround 3 years of overall it experience in big data hadoop and linux environment. knowledge of hadoop architecture and various components such as hdfs , resource manager , node manager and map reduce programming paradigm. extensive experience in setting hadoop cluster. experience in working with hadoop distributed file system ( hdfs ) , map reduce framework , yarn , pig , hive and sqoop. experience in exporting and importing data using sqoop from hdfs to relational database systems and vice - versa. good working knowledge with map reduce and apache pig. involved in writing the pig scripts to reduce the job execution time. excellent communication , interpersonal , analytical skills , and strong ability to perform as part of team. knowledge on flume , hbase and oozie. good exposure on nosql databases like hbase. hands on experience in sqoop for importing and exporting data from rdbms to hdfs , hive. having knowledge on spark and scala. having knowledge on map reduce - 2 , yarn. excellent problem solving skills with a strong technical background and result oriented team player with excellent communication and interpersonal skills. perform the role of a hadoop admin in prod - support , help support 24*7 operations. run in depth analysis of issue to take active role on investigation process. understand the business behind the various projects. work in an agile project environment and be ready to juggle between competing priorities work with the global team stakeholders in the us overlapping hours. experiencecurrently working as a software engineer in capgemini , hyderabad . projectproject name : disney parks and resorts - hadoop landing technology : hadoop , apache pig , hive , sqoop , java , unix , mysql duration : april 2015 to till date role : hadoop developer description : this project is all about the rehousing of their current existing project into hadoop platform. providing data infrastructure for disneys , abcs and espns internet presences is challenging. doing so requires cost effective , performance , scalable and highly available solutions. information requirements from the business add the need for these solutions work together providing consistent acquisition , storage and access to data. burdened with a heavily laden commercial rdbms infrastructure , hadoop provided an opportunity to solve some challenging use cases at disney. the deployment of hadoop helped disney to address growing costs , scalability , and data availability. in addition , it provides our businesses with new data driven business to consumer opportunities. roles and responsibilities : written the apache pig scripts to process the hdfs data. created hive tables to store the processed results in a tabular format. developed the swoop scripts in order to make the interaction between pig and mysql database. writing the script files for processing data and loading to hdfs writing cli commands using hdfs. developed the unix shell scripts for creating the reports from hive data. completely involved in the requirement analysis phase. analyzing the requirement to setup a cluster setup hive with mysql as a remote megastore moved all log text files generated by various products into hdfs location created external hive table on top of parsed data. project name : user story technology : hadoop , apache pig , hive , sqoop , java , unix , mysql duration : dec - 2013 to april 2015 role : hadoop developer roles and responsibilities : imported all the table data of sensex. all the imported data of sensex table is directly stored on hive stage table without partitioning. stage table data has been re - organized in a new table called orc table by date partition file format with snappy compression codec enabled. written pig script to find total sensex ids based on type of sensex. written a map reduce code to find out number of day trading sensex accounts by sensex location. exported the same processed data of day trading sensex accounts to mysql database for reporting purpose. skillslanguages map reduce , pig , sqoop , hive , hbase , core java , spark , scala. frameworks hadoop. java ides eclipse. sql developer databases mysql , oracle , hbase , operating systems windows7 , windows xp , 2000 , 2003 , unix and linux educationbachelor of technology in electronics and communication engineering from jawaharlal nehru technological university , anantapur. intermediate in mathematics - physics - chemistry stream from sri chaitanya juniour college , tirupati , andhra pradesh. ssc from z.p high school , peddapappur , andhra pradesh.',\n",
       " 'summary2 years of experience in hadoop development , hypertext preprocessor , mysql , wordpress. current company : - ubn software solutions from dec , 2014 to till date. previous company bikash infosystem from september to nov , 2014 over 11 months of experience in apache hadoop , hive , pig , hdfs. trained in hadoop from a reputed training institute. technically have strong sound knowledge in hive , pig , sqoop. knowledge on mapreduce. capable of processing large sets of structured , semi - structured and unstructured data. hands - on experience of working with big data and hadoop file system. knowledge of hadoop architecture and hadoop daemons such as name node , secondary name node , job tracker , task tracker. hands on experience in writing mapreduce jobs hive , pig. experience in working on cloudera distribution( clouderavm 4.7 ) strong debugging and problem solving skills. excellent communication , interpersonal and analytical skills. quick learner and adaptive to new and challenging technological environments. ability to prioritize , multi - task , work under pressure , and deliver on time. projectenvironment : hadoop , apache pig , hive , sqoop , mysql role : hadoop developer team size : 6 project description : the purpose of the project is to perform the analysis on the effectiveness and validity of controls and to store log information generated by the source providers as part of the analysis and extract meaningful information out of it. the solution is based on the open source big data software hadoop. the data will be stored in hadoop file system and processed using map reduce jobs , which in turn includes getting the raw data , process the data to obtain controls and redesign change history information , extract various reports out of the controls history and export the information for further processing. roles and responsibilities : using pig , parsing of xml data and storing the data in a pig relation. use apache sqoop scripts to export data from rdbms to hive. storing the relation to a external folder( day wise folder ) which should be added as a partition to hive table weekly done some analysis and store the result to resultant hive table. extra a_agot national and state level certificates for sports ( football ) . participated in mountaineering trip at kulu and manali.',\n",
       " 'experiencedata processing using hadoop technologies present company : tata consultancy services ( tcs ) designation : big data developer technologies : hadoop , spark , cassandra , unix shell scripts , cron jobs , java , flume , sqoop , hive , spark , docker tools : eclipse , putty , winscp , docker , citrix project business intelligence environment : microsoft excel , pentaho kettle details : analyzed the factual data , performed data integration and displayed results by graphical representation vidyut registration event ( java and database connectivity ) environment : net beans java applet , postgresql details : create an event registration system in association with vidyut ( our national tech fest ) that helps students to register for required events , workshops , contest and concerts etc. hotel management ( software engineering project ) environment : rational rose ide tool details : analyzing and designing the hotel management requirements like booking , services from both user interface and back end interface contributions : analyzing part sequence diagram , use case diagram and activity diagram. health issues faced by it professionals ( student social responsibility ) details : visited menora software solution ( london company ) in info park , kochi ( india ) and took survey on health issues faced by it professionals. the survey was exhibited in college for awareness and conducted a talk on \" how to keep a healthy body by dr.lakshmi from khans hospital , kollam ( india ) fisheries and sustainability ( case study ) details : visited neendakara fishing harbor , kollam( india ) and create awareness among fishermen about pollution and hygiene in fish market and also suggest them many alternate ways of re - using the waste skillsjava &#124; unix shell scripting &#124; python &#124; r &#124; scala &#124; c++ &#124; c bigdata ecosystem hadoop &#124; hdfs &#124; mapreduce &#124; hive &#124; sqoop &#124; flume &#124; cassandra &#124; spark operating system windows xp , 7 , 8 , 10 &#124; linux ubuntu , mint , centos , fedora software tools eclipse &#124; netbeans &#124; excel &#124; code blocks &#124; r studio &#124; pentaho &#124; rational rose &#124; putty machine learning &#124; data mining &#124; nlp &#124; business intelligence database mysql &#124; postgresql methodologies agile &#124; cohort &#124; itil aim : etl have to perform in the production live stage which data is coming from live servers , send to the various vendors after processing and analytics on air using spark. data analyzing using machine learning 12 months company : tata consultancy services ( tcs ) designation : data analyst technologies : machine learning , nlp , r , data mining , java , shell scripts tools : eclipse , r studio aim : to automate the document classification problem for unstructured text medical data and need to develop a best models. part time job : i will do part time job in weekends giving training on hadoop &#124; spark &#124; r programming in private institute in chennai , india and online as well for abroad people , who subscribed to the institute. a_a clean up a pilgrim centre ( sabarimala , india ) as a part of the clean - up drive organized by amala bharatha campaign in 2011 and 2012 visited the st. joseph primary school and gave awareness about our environment to the pupils in 2012 volunteered for our national tech fest - vidyut 2012 and food coordinator for vidyut 2013 organized cyber security workshop successfully for national tech fest - vidyut 2014 volunteered for asia amritapuri site regional , acm icpc 2014 active volunteer and member of maitree association ( tcs social service ) for better society educationb - tech ( computer science and engineering ) [ 2015 ] amrita school of engineering , amritapuri - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - cgpa 7.1 10 mailto : veerlasrinivasnag@gmail.com https : in.linkedin.com in srinivas - nag - veerla - a9a6a4a5 high school ( mathematics , physics and chemistry ) [ 2011 ] board of intermediate education , andhra pradesh - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 95.6 % 10th grade [ 2009 ] board of secondary education , andhra pradesh - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 88.66 % international paper publication ( ieee ) title : multi label prediction using association rule generation and simple k - means abstract : lately , modern applications like information retrieval , semantic scene classification , music categorization and functional genomics classification highly require multi label classification. a rule mining algorithm apriori is widely used for rule generation. but apriori is used many times on categorical data , it is seldom used for numerical data. this leads to an idea that with proper data pre - processing , a lot of intangible rules can be derived from such numerical datasets. since the algorithm will check each and every datasets , we used a simple k - means clustering approach for dividing the processing space of apriori and thus rules are generated for each cluster. the accuracy of the algorithm is calculated using hamming loss and is presented in the paper. this hybrid algorithm directly aims to find out hidden patterns in huge numerical datasets and make reliable label prediction easier. student internship providing the back - end user interface design using php and mysql in order to store the data from user to database( registration and login ) , sample web designing to attach the images using html and css in dreamweaver ide tool and designed sample applets in visual .net using visual studio 2010 in kaashiv info tech located at chennai ( india ) in 2013. machine learning &#124; coursera &#124; mit ibm bluemix cloud developer certified &#124; ibm &#124; feb - 2016 big data fundamentals &#124; big data university &#124; 2016 - may - 10 ess 100 introduction to big data &#124; mapr &#124; 2016 - aug - 04 ess 101 apache hadoop essentials &#124; mapr &#124; 2016 - aug - 04 ess 102 mapr converged data platform essentials &#124; mapr &#124; 2016 - aug - 07 da 440 - apache hive essentials &#124; mapr &#124; 2016 - aug - 24 da 410 - apache drill essentials &#124; mapr &#124; 2016 - aug - 29 mapreduce and yarn &#124; big data university &#124; 2016 - sep - 11 dev 301 - developing hadoop applications &#124; mapr &#124; 2016 - sep - 18 mapr certified spark developer ( mcsd ) &#124; mapr &#124; 2016 - dec - 14 attended amazon web services ( aws ) developer training conducted by cloudthat in 2016 attended ibm bluemix cloud application developer training conducted by ibm in 2015 won a prize of worth 10 , 000 rupees ( inr bucks ) in kerala app idea contest 2014 rewarded for participation in android app contest held by vidyut 2014 attended fossmeet in nit calicut ( india ) in 2014 participated in android application development workshop conducted by iisc. bangalore attended online power searching with google course in 2012 attended html workshop in our college in 2011',\n",
       " 'experiencecompany : ibm ( april 2014 to present ) 1 ) project : vodafone money transfer ( vmt ) role : hadoop developer ( jan 2015 to present ) project description : vmt is a telecommunication industry which provides mpesa service to customers. mpesa is aimed at mobile customers who do not have bank account , it allows customers to deposit and withdraw cash via local agents and transfer money to other mobile phone users via sms. ibm ` s role is to enable vmt to analyze big data produced by them under telecom system using hadoop so that client can identify and resolve any bottleneck quickly to retain its customer base and attract new customers. in vmt , large amounts of data is collected and analyzed which is getting generated by customers all the time. data collected could be structured , semi - structured and unstructured. all these data is collected , aggregated and analyzed in hadoop cluster to find any anomaly , specific pattern , customer preferences and to identify services which need to be groomed for improvement of customer experience. to achieve this we ingest data using sqoop , flume and developed map - reduce programs in hive and pig to validate and cleanse the data in hdfs , obtained from heterogeneous data sources and make it suitable for analysis. environment technologies : flume , sqoop , pig , hive , oozie , hdfs , big data , ibm infosphere biginsights for hadoop project role and responsibilities : worked on a 21 nodes hadoop cluster running on ibm infosphere biginsight. worked with highly unstructured and semi structured data of 20 tb in size ( 60 tb with replication factor of 3 ) . used sqoop for importing and exporting the customer profile , table records from sql server into hdfs and hive. load and transform large sets of structured , semi structured using hive. created hive tables as per requirement of internal or external defined with appropriate partitions , bucketing intended for efficiency. performed complex joins on the tables in hive to compare different transactions and services provide by mpesa to their customers. developed pig latin scripts to extract the data logs from the web server output files to load into hdfs and obtain results by performing analysis on that data. developed pig scripts to process the logs data to find the web hits for a particular site. developed pig scripts to find out any warning message , review of campaigns offers provided to customers. projectrole : java application developer ( may - 14 to dec - 14 ) project description : the ibm contracts online ( col ) system is an integrated solution for businesses contracting with ibm. it is a secure web based application that supports the end - to - end contract execution and management process to include electronic delivery , internal approval and routing , negotiation , electronic signature , retrieval and print. the application supports management of the entire contracting life cycle including negotiation , execution , purchase order , active and archive modules. to take part in requirement estimation and implement the deliverable maintaining the quality of technical designs , codes to target minimum or zero defects. ensure that development is performed and delivered as per plan and requirements. participate in the project innovations for the project improvements. act as a central coordination point for various activities like release management and maintenance activities. experience in all phases of agile methodology including sprint planning , planning poker , epic and story creation , development phase , unit testing , uat testing and deployment. skillsbig data eco - system : map reduce , hdfs , hbase , pig , hive , sqoop , oozie , flume , yarn , nifi programming languages : pig , hive , sqoop , j2se , j2ee c c++ frameworks : ibm infosphere biginsights , hortonworks sandbox , amazon ec2 for hadoop. operating systems : windows and unix. database : mysql , pl sql ides : visual studio , net beans and eclipse. tools : ms - powerpoint , ms - office. educationcourse university board year of passing school institute percentage b.tech( cs ) gbtu 2012 mgm college of engineering and technology , noida , up 71.72 % 12th board cbse 2007 little flowers public senior secondary school , delhi 70.40 % 10th board cbse 2005 sardar patel public senior secondary school , delhi 77.80 % completed certification of hortonworks data platform certified developer ( hdpcd ) for hadoop. certification in ibm application developer - big data - biginsights , certificate is having technologies such as mapreduce , pig , hive , hbase , zookeeper , flume , sqoop , oozie. had 6 month industry training from cdac in dac course , which contained training on c , c++ , java ( core + advance ) , asp.net , sql , pl - sql and professional communication.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:03:20.739349Z",
     "start_time": "2018-01-16T15:03:20.656690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TfidfVectorizer()\n",
    "t.fit(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:07:25.133954Z",
     "start_time": "2018-01-16T15:07:25.130996Z"
    }
   },
   "outputs": [],
   "source": [
    "soln = t.transform([\"14 java hbase apache c 14 dasdasd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:07:29.939069Z",
     "start_time": "2018-01-16T15:07:29.928459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7449"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-16T15:07:24.300Z"
    }
   },
   "outputs": [],
   "source": [
    "temp = soln.toarray()[soln.toarray() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-16T15:07:33.685Z"
    }
   },
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:58:31.315407Z",
     "start_time": "2018-01-16T14:58:31.304933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience01',\n",
       " '14',\n",
       " '-',\n",
       " '09',\n",
       " '14',\n",
       " 'technical',\n",
       " 'faculty',\n",
       " 'programming',\n",
       " 'instructor',\n",
       " 'appin',\n",
       " '-',\n",
       " 'technology',\n",
       " 'lab',\n",
       " ',',\n",
       " 'vastrapur',\n",
       " ',',\n",
       " 'ahmedabad',\n",
       " 'provide',\n",
       " 'training',\n",
       " 'in',\n",
       " 'php',\n",
       " ',',\n",
       " 'software',\n",
       " 'testing',\n",
       " ',',\n",
       " 'web',\n",
       " 'development',\n",
       " 'and',\n",
       " 'project',\n",
       " 'planning',\n",
       " 'techniques.',\n",
       " 'address',\n",
       " 'b',\n",
       " '-',\n",
       " '1',\n",
       " '135',\n",
       " ',',\n",
       " 'radhe',\n",
       " 'bungalows',\n",
       " 'nr.',\n",
       " 'khokara',\n",
       " 'circle',\n",
       " 'maninagar(',\n",
       " 'e',\n",
       " ')',\n",
       " ',',\n",
       " 'ahmedabad',\n",
       " 'gujarat',\n",
       " ',',\n",
       " 'india',\n",
       " 'mobile',\n",
       " '+91',\n",
       " '-',\n",
       " '7698875566',\n",
       " 'mail',\n",
       " 'kshtce@gmail.com',\n",
       " 'programming',\n",
       " 'os',\n",
       " 'preference',\n",
       " 'windows',\n",
       " 'linux',\n",
       " 'unix',\n",
       " 'unix',\n",
       " 'macos',\n",
       " 'skills',\n",
       " 'big',\n",
       " 'data',\n",
       " 'cloud',\n",
       " 'computing',\n",
       " ',',\n",
       " 'cluster',\n",
       " 'and',\n",
       " 'grid',\n",
       " 'computing',\n",
       " ',',\n",
       " 'cuda',\n",
       " 'programming',\n",
       " ',',\n",
       " 'php',\n",
       " 'programming',\n",
       " ',',\n",
       " 'operating',\n",
       " 'system',\n",
       " 'like',\n",
       " 'linux',\n",
       " ',',\n",
       " 'windows',\n",
       " ',',\n",
       " 'software',\n",
       " 'testing.',\n",
       " 'a_a',\n",
       " 'last',\n",
       " 'year',\n",
       " 'of',\n",
       " 'masters',\n",
       " 'dissertation',\n",
       " 'done',\n",
       " 'in',\n",
       " 'sac',\n",
       " '-',\n",
       " 'isro',\n",
       " 'attended',\n",
       " 'one',\n",
       " '-',\n",
       " 'day',\n",
       " 'meet',\n",
       " 'up',\n",
       " 'on',\n",
       " 'embedded',\n",
       " 'supercomputing',\n",
       " 'at',\n",
       " 'iit',\n",
       " '-',\n",
       " 'bombay',\n",
       " 'completed',\n",
       " 'ethical',\n",
       " 'hacking',\n",
       " 'certification',\n",
       " 'stage',\n",
       " '1',\n",
       " 'with',\n",
       " 'sunny',\n",
       " 'vaghela',\n",
       " 'at',\n",
       " 'nirma',\n",
       " 'university',\n",
       " 'completed',\n",
       " 'cloud',\n",
       " 'u',\n",
       " 'certification',\n",
       " 'presented',\n",
       " 'many',\n",
       " 'ieee',\n",
       " 'papers',\n",
       " 'like',\n",
       " 'memristor',\n",
       " ',',\n",
       " 'pervasive',\n",
       " 'computing',\n",
       " ',',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " ',',\n",
       " 'gpu',\n",
       " 'programming',\n",
       " ',',\n",
       " 'bigdata',\n",
       " 'etc.',\n",
       " 'organized',\n",
       " 'and',\n",
       " 'co',\n",
       " '-',\n",
       " 'ordinate',\n",
       " 'salcon',\n",
       " '-',\n",
       " '11',\n",
       " 'education2014',\n",
       " '-',\n",
       " '2016',\n",
       " 'master',\n",
       " \"'\",\n",
       " 's',\n",
       " 'degree',\n",
       " 'in',\n",
       " 'computer',\n",
       " 'engineering.',\n",
       " 'gtu',\n",
       " 'pg',\n",
       " 'school',\n",
       " ',',\n",
       " 'gandhinagar',\n",
       " 'high',\n",
       " 'performance',\n",
       " 'computing.',\n",
       " 'cpi',\n",
       " '-',\n",
       " '8.04',\n",
       " 'main',\n",
       " 'subjects',\n",
       " ':',\n",
       " 'cuda',\n",
       " 'programming',\n",
       " ',',\n",
       " 'bigdata',\n",
       " ',',\n",
       " 'cluster',\n",
       " 'and',\n",
       " 'grid',\n",
       " 'computing',\n",
       " ',',\n",
       " 'cloud',\n",
       " 'computing',\n",
       " ',',\n",
       " 'multicore',\n",
       " 'programming',\n",
       " ',',\n",
       " 'parallel',\n",
       " 'programming.',\n",
       " 'title',\n",
       " 'of',\n",
       " 'the',\n",
       " 'thesis',\n",
       " ':',\n",
       " 'gpu',\n",
       " 'based',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'atmospheric',\n",
       " 'correction',\n",
       " 'of',\n",
       " 'remote',\n",
       " 'sensing',\n",
       " 'image',\n",
       " 'relators',\n",
       " ':',\n",
       " 'dr.',\n",
       " 'shailendra',\n",
       " 'srivastava',\n",
       " ',',\n",
       " 'scientist',\n",
       " ',',\n",
       " 'sac',\n",
       " '-',\n",
       " 'isro',\n",
       " ',',\n",
       " 'ahmedabad.',\n",
       " '2009',\n",
       " '-',\n",
       " '2013',\n",
       " 'bachelor',\n",
       " \"'\",\n",
       " 's',\n",
       " 'degree',\n",
       " 'in',\n",
       " 'computer',\n",
       " 'engineering.',\n",
       " 'sal',\n",
       " 'institute',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'engineering',\n",
       " 'research',\n",
       " ',',\n",
       " 'gtu',\n",
       " ',',\n",
       " 'ahmedabad',\n",
       " 'cgpa',\n",
       " '-',\n",
       " '7.92',\n",
       " 'main',\n",
       " 'subjects',\n",
       " ':',\n",
       " 'programming',\n",
       " ',',\n",
       " 'operating',\n",
       " 'system',\n",
       " ',',\n",
       " 'database',\n",
       " ',',\n",
       " 'parallel',\n",
       " 'process',\n",
       " '-',\n",
       " 'ing',\n",
       " ',',\n",
       " 'java',\n",
       " ',',\n",
       " '.net',\n",
       " ',',\n",
       " 'php',\n",
       " 'languages',\n",
       " ',',\n",
       " 'networking',\n",
       " 'etc',\n",
       " '.',\n",
       " 'title',\n",
       " 'of',\n",
       " 'the',\n",
       " 'project(',\n",
       " '7th',\n",
       " 'sem',\n",
       " ')',\n",
       " ':',\n",
       " 'online',\n",
       " 'shopping',\n",
       " 'mall',\n",
       " '(',\n",
       " 'osm',\n",
       " ')',\n",
       " '.',\n",
       " 'title',\n",
       " 'of',\n",
       " 'the',\n",
       " 'project(',\n",
       " '8th',\n",
       " 'sem',\n",
       " ')',\n",
       " ':',\n",
       " 'computer',\n",
       " 'institute',\n",
       " 'management',\n",
       " 'system',\n",
       " '(',\n",
       " 'cims',\n",
       " ')',\n",
       " '.',\n",
       " '2009',\n",
       " 'hsc.',\n",
       " 'diwan',\n",
       " 'ballubhai',\n",
       " 'higher',\n",
       " 'secondary',\n",
       " 'school',\n",
       " ',',\n",
       " 'ahmedabad',\n",
       " 'gujarat',\n",
       " 'secondary',\n",
       " 'and',\n",
       " 'higher',\n",
       " 'secondary',\n",
       " 'board.',\n",
       " 'prcentage',\n",
       " '-',\n",
       " '65',\n",
       " '%',\n",
       " 'main',\n",
       " 'subjects',\n",
       " ':',\n",
       " 'matematics',\n",
       " ',',\n",
       " 'physics',\n",
       " ',',\n",
       " 'chemistry',\n",
       " ',',\n",
       " 'computer.',\n",
       " '2007',\n",
       " 'ssc.',\n",
       " 'diwan',\n",
       " 'ballubhai',\n",
       " 'secondary',\n",
       " 'school',\n",
       " ',',\n",
       " 'ahmedabad',\n",
       " 'gujarat',\n",
       " 'secondary',\n",
       " 'and',\n",
       " 'higher',\n",
       " 'secondary',\n",
       " 'board.',\n",
       " 'prcentage',\n",
       " '-',\n",
       " '87.38',\n",
       " '%',\n",
       " 'main',\n",
       " 'subjects',\n",
       " ':',\n",
       " 'maths',\n",
       " ',',\n",
       " 'science',\n",
       " ',',\n",
       " 'social',\n",
       " 'science',\n",
       " ',',\n",
       " 'gujarati',\n",
       " ',',\n",
       " 'english.',\n",
       " 'sanskrit',\n",
       " ',',\n",
       " 'computer.',\n",
       " '06',\n",
       " '2016',\n",
       " 'project',\n",
       " 'trainee',\n",
       " 'at',\n",
       " 'sac',\n",
       " 'for',\n",
       " 'a',\n",
       " 'year.',\n",
       " 'sac',\n",
       " '-',\n",
       " 'isro',\n",
       " ',',\n",
       " 'ahmedabad',\n",
       " 'gpu',\n",
       " 'based',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'atmospheric',\n",
       " 'correction',\n",
       " 'of',\n",
       " 'remote',\n",
       " 'sensing',\n",
       " 'image.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESUMES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
